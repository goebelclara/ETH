{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Task 1</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "#Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import re\n",
    "import datetime as datetime\n",
    "from datetime import date\n",
    "import numbers\n",
    "\n",
    "#Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#Data modeling\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingRegressor, VotingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, Normalizer, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import chi2\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "#Other\n",
    "import warnings\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing warnings\n",
    "warnings.simplefilter(action = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting random seed\n",
    "seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "xtrain_df = pd.read_csv(\"X_train.csv\", index_col = 0)\n",
    "ytrain_df = pd.read_csv(\"y_train.csv\", index_col = 0)\n",
    "xtest_df = pd.read_csv(\"X_test.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set feature columns\n",
    "feature_columns = xtrain_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set params\n",
    "params = {\n",
    "    \"random_state\": 11,\n",
    "    # Missing values\n",
    "    \"missing_values_imputation_method\": \"mean\", #\"median\",\"mean\",\"knn\",\"mice\"\n",
    "    \"mv_knn_nneighbors\": 5,\n",
    "    \"mv_knn_weights\": \"distance\", #\"uniform\",\"distance\"\n",
    "    \"mv_mice_max_iter\": 5,\n",
    "    \"mv_mice_initial_strategy\": 'median', #\"mean\",\"median\",\"most_frequent\",\"constant\"\n",
    "    \"mv_mice_n_nearest_features\": 5,\n",
    "    # Outliers\n",
    "    \"outlier_removal_method\": \"svm\", #\"iqr\",\"if\",\"md\",\"svm\",\"pca\"\n",
    "    \"o_iqr_threshold\":10,\n",
    "    \"o_if_contamination\":0.1,\n",
    "    \"o_if_n_estimators\":100,\n",
    "    \"o_if_max_samples\": 0.9,\n",
    "    \"o_if_max_features\": 0.9,\n",
    "    \"o_md_chisquarethreshold\":0.95,\n",
    "    \"o_svm_kernel\": \"rbf\", #\"linear\",\"poly\",\"rbf\",\"sigmoid\",\"precomputed\"\n",
    "    \"o_svm_degree\": 5,\n",
    "    \"o_svm_nu\": 0.05,\n",
    "    \"o_svm_gamma\": \"scale\", #\"scale\",\"auto\"\n",
    "    \"o_pca_n_components\": 10,\n",
    "    \"o_pca_percentilereconstructionerror\": 95,\n",
    "    # Feature selection\n",
    "    \"feature_selection_method\": \"lasso\", #\"mi\",\"variancethreshold\",\"lasso\",\"rf\",\"rfe\",\"pca\"\n",
    "    \"fs_mi_threshold\": 0.05,\n",
    "    \"fs_vt_threshold\": 1.0,\n",
    "    \"fs_lasso_alpha\": 0.4,\n",
    "    \"fs_rf_n_estimators\": 50,\n",
    "    \"fs_rf_importance_threshold\": 0.001,\n",
    "    \"fs_rfe_nfeatures\": 300,\n",
    "    \"fs_pca_n_components\": 300,\n",
    "    # Scaling\n",
    "    \"scaling_method\": \"std\", #\"maxabs\",\"std\",\"minmax\",\"robust\"\n",
    "    #Models\n",
    "    \"lasso_params\": {\n",
    "        \"alpha\": [0.]\n",
    "    },\n",
    "    \"xgboost_params\": {\n",
    "        \"n_estimators\":[50,50],\n",
    "        \"learning_rate\": [0.001,0.02,0.05,0.1],\n",
    "        \"eta\": [0.05,0.3,0.5],\n",
    "        \"gamma\": [0.5,0.68,0.85],\n",
    "        \"max_depth\": [4,6,8],\n",
    "        \"subsample\": [0.34,0.59,0.8],\n",
    "        \"colsample_bytree\": [0.68, 0.8,0.9],\n",
    "        \"lambda\": [0.65,0.85],\n",
    "        \"alpha\": [0.4,0.2,0.5]\n",
    "    },\n",
    "    \"randomforest_params\": {\n",
    "        \"n_estimators\": [100],\n",
    "        \"criterion\": [\"squared_error\"], #“squared_error”, “absolute_error”, “friedman_mse”, “poisson”\n",
    "        \"max_depth\": [10],\n",
    "        \"min_samples_split\": [2],\n",
    "        \"max_features\": [1.0]\n",
    "    },\n",
    "    \"lgbm_params\": {\n",
    "        \"n_estimators\": [2000],\n",
    "        \"learning_rate\": [0.02],\n",
    "        \"max_depth\": [6],\n",
    "        \"subsample\": [0.34],\n",
    "        \"colsample_bytree\": [0.68],\n",
    "        \"reg_lambda\": [0.65],\n",
    "        \"reg_alpha\": [0.4]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do: Add hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_constants(X_train,X_test,feature_columns):\n",
    "    constant_column_mask = xtrain_df.nunique() == 1\n",
    "    constants = constant_column_mask[constant_column_mask].index.to_list()\n",
    "    for constant in constants:\n",
    "        X_train = X_train.drop(constant, axis = 1)\n",
    "        X_test = X_test.drop(constant, axis = 1)\n",
    "        feature_columns.remove(constant)\n",
    "    return X_train,X_test,feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, xtest_df, feature_columns = drop_constants(xtrain_df, xtest_df, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_imputation(X_train, X_test, params):\n",
    "    if params['missing_values_imputation_method'] == 'median':\n",
    "        X_train_imp = np.where(np.isnan(np.array(X_train)), np.nanmedian(np.array(X_train), axis = 0), np.array(X_train))\n",
    "        X_test_imp = np.where(np.isnan(np.array(X_test)), np.nanmedian(np.array(X_test), axis = 0), np.array(X_test))\n",
    "    \n",
    "    elif params['missing_values_imputation_method'] == 'mean':\n",
    "        X_train_imp = np.where(np.isnan(np.array(X_train)), np.nanmean(np.array(X_train), axis = 0), np.array(X_train))\n",
    "        X_test_imp = np.where(np.isnan(np.array(X_test)), np.nanmean(np.array(X_test), axis = 0), np.array(X_test))\n",
    "    \n",
    "    elif params['missing_values_imputation_method'] == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=params[\"mv_knn_nneighbors\"], weights=params[\"mv_knn_weights\"])\n",
    "        imputed_values_train = imputer.fit_transform(X_train)\n",
    "        imputed_values_test = imputer.transform(X_test)\n",
    "        X_train_imp = np.where(np.isnan(X_train), imputed_values_train, X_train)\n",
    "        X_test_imp = np.where(np.isnan(X_test), imputed_values_test, X_test)\n",
    "    \n",
    "    elif params['missing_values_imputation_method'] == 'mice':\n",
    "        imputer = IterativeImputer(max_iter=params[\"mv_mice_max_iter\"], initial_strategy=params[\"mv_mice_initial_strategy\"],n_nearest_features=params[\"mv_mice_n_nearest_features\"])\n",
    "        imputed_values_train = imputer.fit_transform(X_train)\n",
    "        imputed_values_test = imputer.transform(X_test)\n",
    "        X_train_imp = np.where(np.isnan(X_train), imputed_values_train, X_train)\n",
    "        X_test_imp = np.where(np.isnan(X_test), imputed_values_test, X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train_imp, index = X_train.index, columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(X_test_imp, index = X_test.index, columns = X_test.columns)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, xtest_df = missing_value_imputation(xtrain_df, xtest_df, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(X_train, y_train, params):\n",
    "    if params['outlier_removal_method'] == 'iqr':\n",
    "        q1 = xtrain_df.quantile(0.25)\n",
    "        q3 = xtrain_df.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        mask = (xtrain_df < (q1 - params[\"o_iqr_threshold\"] * iqr)) | (xtrain_df > (q3 + params[\"o_iqr_threshold\"] * iqr))\n",
    "        outlier_mask = mask.max(axis=1)\n",
    "    \n",
    "    elif params['outlier_removal_method'] == 'if':\n",
    "        model = IForest(contamination=params[\"o_if_contamination\"], n_estimators=params[\"o_if_n_estimators\"], max_samples=params[\"o_if_max_samples\"], max_features=params[\"o_if_max_features\"])\n",
    "        model.fit(xtrain_df)\n",
    "        outlier_mask = model.predict(xtrain_df)\n",
    "        outlier_mask = outlier_mask == 1\n",
    "        \n",
    "    elif params['outlier_removal_method'] == 'md':\n",
    "        X_train_a = np.array(xtrain_df)\n",
    "\n",
    "        mean_vec = np.mean(X_train_a, axis=0)\n",
    "        cov_matrix = np.cov(X_train_a, rowvar=False)\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "\n",
    "        def mahalanobis_distance(x, mean_vec, inv_cov_matrix):\n",
    "            diff = x - mean_vec\n",
    "            return np.sqrt(np.dot(np.dot(diff, inv_cov_matrix), diff.T))\n",
    "\n",
    "        m_distances = np.array([mahalanobis_distance(x, mean_vec, inv_cov_matrix) for x in X_train_a])\n",
    "        \n",
    "        threshold = np.sqrt(chi2.ppf(params[\"o_md_chisquarethreshold\"], df=X_train.shape[1]))\n",
    "        outlier_mask = m_distances > threshold\n",
    "    \n",
    "    elif params['outlier_removal_method'] == 'svm':\n",
    "        X_train_a = np.array(xtrain_df)\n",
    "\n",
    "        ocsvm = OneClassSVM(kernel=params[\"o_svm_kernel\"], nu=params[\"o_svm_nu\"], gamma=params[\"o_svm_gamma\"], degree=params[\"o_svm_degree\"])\n",
    "        ocsvm.fit(X_train_a)\n",
    "        predictions = ocsvm.predict(X_train_a)\n",
    "\n",
    "        outlier_mask = predictions == -1\n",
    "    \n",
    "    elif params['outlier_removal_method'] == 'pca':\n",
    "        X_train_a = np.array(xtrain_df) \n",
    "\n",
    "        pca = PCA(n_components=params[\"o_pca_n_components\"])\n",
    "        X_train_pca = pca.fit_transform(X_train_a)\n",
    "        X_train_reconstructed = pca.inverse_transform(X_train_pca)\n",
    "        reconstruction_error = np.mean((X_train - X_train_reconstructed) ** 2, axis=1)\n",
    "\n",
    "        threshold = np.percentile(reconstruction_error, params[\"o_pca_percentilereconstructionerror\"])\n",
    "        outlier_mask = reconstruction_error > threshold\n",
    "\n",
    "    X_train_cleaned = pd.DataFrame(X_train[~outlier_mask],index=X_train[~outlier_mask].index,columns=X_train.columns)\n",
    "    y_train_cleaned = pd.DataFrame(y_train[~outlier_mask],index=y_train[~outlier_mask].index,columns=y_train.columns)\n",
    "    \n",
    "    n_outliers = sum(outlier_mask)\n",
    "    \n",
    "    return X_train_cleaned,y_train_cleaned,n_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, ytrain_df, n_outliers = outlier_removal(xtrain_df, ytrain_df, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deskewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew(X_train, X_test):\n",
    "    for col in X_train.columns:\n",
    "        try:\n",
    "            pt = PowerTransformer(method=\"yeo-johnson\")\n",
    "            deskewed_col_train = pt.fit_transform(np.array(X_train[col]).reshape(-1, 1))\n",
    "            deskewed_col_test = pt.transform(np.array(X_test[col]).reshape(-1, 1))\n",
    "            X_train[col] = deskewed_col_train\n",
    "            X_test[col] = deskewed_col_test\n",
    "        except: \n",
    "            X_train[col] = X_train[col]\n",
    "            X_test[col] = X_test[col]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, xtest_df = deskew(xtrain_df, xtest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_train, y_train, X_test, params):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "    if params['feature_selection_method'] == 'mi':\n",
    "        mi = mutual_info_regression(X_train, y_train)\n",
    "        mi_df = pd.DataFrame({'Feature': X_train.columns, 'Mutual Information': mi})\n",
    "        mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "        selected_features = mi_df[mi_df['Mutual Information'] > params[\"fs_mi_threshold\"]]['Feature']\n",
    "    \n",
    "    elif params['feature_selection_method'] == 'variancethreshold':\n",
    "        selector = VarianceThreshold(threshold = params[\"fs_vt_threshold\"])\n",
    "        X_train_selected = selector.fit_transform(X_train)\n",
    "        selected_features = X_train.columns[selector.get_support()]\n",
    "\n",
    "    elif params['feature_selection_method'] == 'lasso':\n",
    "        lasso = Lasso(alpha=params[\"fs_lasso_alpha\"])\n",
    "        lasso.fit(X_train_scaled, y_train)\n",
    "        selected_features = X_train.columns[lasso.coef_ != 0]\n",
    "    \n",
    "    elif params['feature_selection_method'] == 'rf':\n",
    "        rf = RandomForestRegressor(n_estimators = params[\"fs_rf_n_estimators\"])\n",
    "        rf.fit(X_train, y_train)\n",
    "        importance = rf.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importance})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "        threshold = params[\"fs_rf_importance_threshold\"]\n",
    "        selected_features = feature_importance_df[feature_importance_df['Importance'] > threshold]['Feature']\n",
    "    \n",
    "    elif params['feature_selection_method'] == 'rfe':\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(estimator=model, n_features_to_select=params[\"fs_rfe_nfeatures\"])\n",
    "        rfe.fit(X_train, y_train)\n",
    "        selected_features = X_train.columns[rfe.support_]\n",
    "    \n",
    "    elif params['feature_selection_method'] == 'pca':\n",
    "        pca = PCA(n_components=params[\"fs_pca_n_components\"])\n",
    "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "        pca_components_train_df = pd.DataFrame(X_train_pca, columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])], index = X_train.index)\n",
    "        pca_components_test_df = pd.DataFrame(X_test_pca, columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])], index = X_test.index)\n",
    "        \n",
    "    if params['feature_selection_method'] == 'pca':\n",
    "        return pca_components_train_df, pca_components_test_df\n",
    "        n_features = params[\"fs_pca_n_components\"]\n",
    "    else:\n",
    "        X_train = X_train[selected_features]\n",
    "        X_test = X_test[selected_features]\n",
    "        n_features = len(selected_features)\n",
    "        \n",
    "        return X_train,X_test,n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, xtest_df, n_features = feature_selection(xtrain_df, ytrain_df, xtest_df, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X_train, X_test, params):\n",
    "    if params['scaling_method'] == 'maxabs':\n",
    "        scaler = MaxAbsScaler()\n",
    "    \n",
    "    elif params['scaling_method'] == 'std':\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    elif params['scaling_method'] == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    elif params['scaling_method'] == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_train_scaled =  pd.DataFrame(data = X_train_scaled, columns = X_train.columns, index = X_train.index)\n",
    "    X_test_scaled =  pd.DataFrame(data = X_test_scaled, columns = X_test.columns, index = X_test.index)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_df, xtest_df = scaling(xtrain_df, xtest_df, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_vali, y_train, y_vali = train_test_split(xtrain_df, ytrain_df, test_size = 0.2, random_state = params[\"random_state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "\n",
    "    return model, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lir_model, lir_r2 = linear_regression(xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    model_params = params[\"lasso_params\"]\n",
    "    \n",
    "    gs = GridSearchCV(linear_model.Lasso(), \n",
    "                      param_grid = model_params,\n",
    "                      cv = 3)\n",
    "    \n",
    "    gs.fit(X_rest, y_rest)\n",
    "    \n",
    "    y_pred = gs.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "        \n",
    "    return gs, r2, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lar_model, lar_r2, lar_params = lasso_regression(xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    model_params = params[\"xgboost_params\"]\n",
    "    \n",
    "    gs = GridSearchCV(xgb.XGBRegressor(), \n",
    "                      param_grid = model_params,\n",
    "                      cv = 2)\n",
    "    \n",
    "    gs.fit(X_rest, y_rest)\n",
    "    \n",
    "    y_pred = gs.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "        \n",
    "    return gs, r2, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model, xgb_r2, xgb_params = xgboost(xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomforest(X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    model_params = params[\"randomforest_params\"]\n",
    "    \n",
    "    gs = GridSearchCV(RandomForestRegressor(), \n",
    "                      param_grid = model_params,\n",
    "                      cv = 2)\n",
    "    \n",
    "    gs.fit(X_rest, y_rest)\n",
    "    \n",
    "    y_pred = gs.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "        \n",
    "    return gs, r2, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model, rf_r2, rf_params = randomforest(xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm(X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    model_params = params[\"lgbm_params\"]\n",
    "    \n",
    "    gs = GridSearchCV(LGBMRegressor(), \n",
    "                      param_grid = model_params,\n",
    "                      cv = 2)\n",
    "    \n",
    "    gs.fit(X_rest, y_rest)\n",
    "    \n",
    "    y_pred = gs.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "        \n",
    "    return gs, r2, gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model, lgbm_r2, lgbm_params = lgbm(xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(lir_model, lar_model, lgbm_model, xgb_model, rf_model, X_rest, y_rest, X_train, y_train, X_vali, y_vali, params):\n",
    "    voting = VotingRegressor(estimators = [\n",
    "        (\"lir_model\", lir_model),\n",
    "        (\"lar_model\", lar_model),\n",
    "        (\"lgbm_model\", lgbm_model),\n",
    "        (\"xgb_model\", xgb_model),\n",
    "        (\"rf_model\", rf_model)])\n",
    "    \n",
    "    voting.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = voting.predict(X_vali)\n",
    "    r2 = r2_score(y_vali, y_pred)\n",
    "        \n",
    "    return voting, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "voting_model, voting_r2 = ensemble(lir_model, lar_model, lgbm_model, xgb_model, rf_model, xtrain_df, ytrain_df, X_train, y_train, X_vali, y_vali, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lir_r2)\n",
    "print(lar_r2)\n",
    "print(xgb_r2)\n",
    "print(rf_r2)\n",
    "print(lgbm_r2)\n",
    "print(voting_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_model.predict(xtest_df)\n",
    "sub = pd.DataFrame(data=y_pred,index=xtest_df.index)\n",
    "sub.columns = [\"y\"]\n",
    "sub.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_model.predict(xtest_df)\n",
    "sub = pd.DataFrame(data=y_pred,index=xtest_df.index)\n",
    "sub.columns = [\"y\"]\n",
    "sub.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
