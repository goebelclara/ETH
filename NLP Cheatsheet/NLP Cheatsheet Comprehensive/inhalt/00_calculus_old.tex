\section{Calculus}
\subsection*{Derivatives}
\emph{Rules} --- 
\begin{itemize}
    \item Sum rule: $\frac{\partial f + g}{\partial x} = \frac{\partial f}{\partial x} + \frac{\partial g}{\partial x}$
    \item Product rule: $\frac{\partial f \times g}{\partial x} = f \times \frac{\partial g}{\partial x} + g \times \frac{\partial f}{\partial x}$
    \item Chain rule: $\frac{\partial f(g)}{\partial x} = \frac{\partial f}{\partial g} \times \frac{g}{\partial x}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Common derivatives} --- 
\begin{multicols}{2}
\begin{itemize}
    \item $\frac{\partial x^n}{\partial x} = nx^{n-1}$
    \item $\frac{\partial e^{kx}}{\partial x} = k \times e^{kx}$
    \item $\frac{\partial log(x)}{\partial x} = \frac{1}{x}$
    \item $\frac{\partial \sqrt{x}}{\partial x} = \frac{1}{2\sqrt{x}}$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Partial and directional derivative} --- 
\begin{itemize}
    \item For a function that depends on $n$ variables $\{x_i\}_{i=2}^n$, partial derivative is slope of tangent line along direction of one specific variable $x_i$
    \item Directional derivative is slope of tangent line along direction of selected unit vector $\boldsymbol{u}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Gradient} --- 
\begin{itemize}
    \item Given scalar-valued function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$, returns vector containing first-order partial derivatives:\\
    $\nabla_{\boldsymbol{x}} f: [\frac{\partial f}{\partial x_1} ... \frac{\partial f}{\partial x_n}]^\intercal$
    \item Gradient points in direction of greatest upward slope of f \item Magnitude of gradient equals rate of change when moving into direction of greatest upward slope 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Hessian} --- 
\begin{itemize}
    \item Given scalar-valued function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$, returns matrix containing second-order partial derivatives:\\
    $\mathcal{H} = \nabla_{\boldsymbol{x}}^2 f: \begin{bmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & ... & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    ... & ... & ... \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & ... & \frac{\partial^2 f}{\partial x_n^2}
    \end{bmatrix}$
    \item $\mathcal{H}$ is symmetric 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Jacobian} --- 
\begin{itemize}
    \item Given vector-valued function $\boldsymbol{f}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ with $\boldsymbol{f} = [f_1(\boldsymbol{x}), ..., f_m(\boldsymbol{x})]^\intercal$, returns matrix containing first-order partial derivatives:\\
    $\nabla_{\boldsymbol{x}} \boldsymbol{f}: \begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & ... & \frac{\partial f_1}{\partial x_n} \\
    ... & ... & ... \\
    \frac{\partial f_m}{\partial x_1} & ... & \frac{\partial f_m}{\partial x_n}
    \end{bmatrix}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Matrix calculus rules} --- 
\begin{multicols}{2}
\begin{itemize}
    \item $\frac{\partial \boldsymbol{a}^\intercal\boldsymbol{x}}{\partial \boldsymbol{x}} = \boldsymbol{a}$
    \item $\frac{\partial \boldsymbol{x}^\intercal\boldsymbol{A}\boldsymbol{x}}{\partial \boldsymbol{x}} = (\boldsymbol{A} + \boldsymbol{A}^\intercal)\boldsymbol{x}$
    \item $\frac{\partial \boldsymbol{a}^\intercal\boldsymbol{A}\boldsymbol{b}}{\partial \boldsymbol{A}} = \boldsymbol{a}\boldsymbol{b}^\intercal$
    \item For symmetric $\boldsymbol{A}$: $\frac{\partial \boldsymbol{x}^\intercal\boldsymbol{A}\boldsymbol{x}}{\partial \boldsymbol{x}} = 2\boldsymbol{A}\boldsymbol{x}$
\end{itemize}
\end{multicols}

{\color{black}\hrule height 0.001mm}

\subsection*{Extrema}
\emph{Conditions for local minima and maxima} --- 
\begin{itemize}
    \item Point is a stationary point, i.e. first-order derivative = 0
    \item If Hessian is pd, it's a local minimum, if Hessian is nd, it's a local maximum, if Hessian is indefinite, it's a saddle point
    \item Local minima and maxima are the unique global minima and maxima in strictly convex functions resp. one of possibly infinitely many global minima and maxima in convex functions
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Nature of optimum} --- 
What does Hessian and function look like?
\begin{itemize}
    \item If Hessian is pd and loss function is strictly convex, stationary point is a global minimum, and there is a unique solution
    \item If Hessian is psd and loss function is convex, stationary point is a global minimum, and there may be a geometrically unique or infinitely many solutions
    \item If Hessian is p(s)d but loss function is not convex, stationary point may be a local minimum and there may be a geometrically unique or infinitely many solutions
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization approach} --- 
Is function differentiable, continuous, and are relevant terms invertible?
\begin{itemize}
    \item If yes, analytically solvable
    \item If no, numerically solvable (e.g. via gradient descent)
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Constrained optimization} --- 
\begin{itemize}
    \item Lagrangian function: $\mathcal{L}(\boldsymbol{x},\lambda) = f(\boldsymbol{x}) + \lambda g(\boldsymbol{x})$, where $g(\boldsymbol{x})$ is an $(m-1)$ dimensional constraint surface and $\lambda$ is the Lagrange multiplier
    \item$\nabla_{\boldsymbol{x}} \mathcal{L} = \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) + \lambda \nabla_{\boldsymbol{x}} g(\boldsymbol{x})$
    \item$\nabla_{\lambda} \mathcal{L} = g(\boldsymbol{x})$
    \item Solution is feasible if it fulfills constraints and optimal, if no other feasible solution produces a lower error
    \item Minimizing over Lagrangian $\mathcal{L}(\boldsymbol{x},\lambda) = f(\boldsymbol{x}) + \lambda g(\boldsymbol{x})$ corresponds to minimizing log-loss resp. negative likelihood:
    \begin{itemize}
        \item $\hat{\boldsymbol{x}} = argmax_\boldsymbol{x} p(\boldsymbol{D} |\boldsymbol{x}) \rho(\boldsymbol{x}) $ 
        \item $= argmin_\boldsymbol{x} (- log p(\boldsymbol{D} |\boldsymbol{x}) + k(\boldsymbol{x}))$
        \item where $k(\boldsymbol{x}) = -log \rho(\boldsymbol{x})$
    \end{itemize}
\end{itemize}\\
For multiple constraints: Minimize $f(\boldsymbol{x})$ subject to $m$ inequality constraints $\{g^{(i)}(\boldsymbol{x}) \leq 0\}_{i=1}^m$ and $p$ equality constraints $\{h^{(j)}(\boldsymbol{x}) = 0\}_{j=1}^p$ 
\begin{itemize}
    \item Then, Lagrangian is given by: $\mathcal{L}(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{\mu}) = f(\boldsymbol{x}) + \sum_{i=1}^m \mu^{(i)} g^{(i)}(\boldsymbol{x}) + \sum_{j=1}^p \lambda^{(j)} h^{(j)}(\boldsymbol{x})$
    \item Then, general solution $\boldsymbol{x^*},\boldsymbol{\lambda^*},\boldsymbol{\mu^*}$ is given by: $\nabla_{\boldsymbol{x}} \mathcal{L} = 0$ subject to:
    \begin{itemize}
        \item $\{g^{(i)}(\boldsymbol{x}) \leq 0\}_{i=1}^m$ and $\{h^{(j)}(\boldsymbol{x}) = 0\}_{j=1}^p$ 
        \item $\{\mu^{(i)} \geq 0\}_{i=1}^m$
        \item $\{\mu^{(i)}g^{(i)}(\boldsymbol{x}) = 0\}_{i=1}^m$
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Integrals}
\emph{Indefinite integral} --- 
\begin{itemize}
    \item $F(x) = \int f(x) dx$
    \item $F'(x) = f(x)$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Definite integral} --- 
\begin{itemize}
    \item $F(b) - F(a) = \int_a^b f(x) dx$
    \item $F'(x) = f(x)$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Common integrals} --- 
\begin{itemize}
    \item $f(x) = x^n \rightarrow F(x) = \frac{x^{n+1}}{n+1}$ for $n \neq 1$
    \item $f(x) = \frac{1}{x} \rightarrow F(x) = log(x)$
    \item $f(x) = e^x \rightarrow F(x) = e^x$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Other}
\emph{Common exp and log rules} ---
\begin{multicols}{2}
\begin{itemize}
    \item $a^m \cdot a^n = a^{m+n}$
    \item $\frac{a^m}{a^n} = a^{m-n}$
    \item $(ab)^n = a^n b^n$
    \item $\left(\frac{a}{b}\right)^n = \frac{a^n}{b^n}$
    \item $a^{-n} = \frac{1}{a^n}$
    \item $a^0 = 1$
    \item $a^1 = a$
    \item $\log(xy) = \log x + \log y$
    \item $\log\left(\frac{x}{y}\right) = \log x - \log y$
    \item $\log(x^n) = n \log x$
    \item $\log 1 = 0$
    \item $\log (x<1) < 0$
    \item $\log (x>1) > 0$
    \item $e^{log(x)} = log(e^x) = x$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}
\emph{Geometric series} --- 
\begin{itemize}
    \item Finite: $S_n = \sum_{i=1}^n a_i r^{i-1} = a_1 (\frac{1-r^n}{1-r})$
    \item Infinite: $S = \sum_{i=0}^\infty a_i r^i = \frac{a_1}{1-r}$ for $r < 1$
\end{itemize}