\section{Logistic Regression}
\subsection*{Formulation}
\begin{itemize}
    \item Probability of each class is estimated via \emph{sigmoid function}: $\sigma(z) = \frac{1}{1+e^{-z}} = \frac{e^{z}}{1+e^{z}}$
    \item $P(y=1|\boldsymbol{x}) = \frac{1}{1+e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{1+e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}, \quad P(y=0|\boldsymbol{x}) = \frac{1}{1+e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}} = \frac{e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}}{1+e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}}$
    \item \emph{Odds}: $\frac{P(y=1|\boldsymbol{x})}{P(y=0|\boldsymbol{x})} = e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}$
    \item \emph{Log odds}: $ln(\frac{P(y=1|\boldsymbol{x})}{P(y=0|\boldsymbol{x})}) = \boldsymbol{\beta} \cdot \boldsymbol{x} = ln(\frac{P(y=1)}{P(y=0)}) + ln(\frac{P(\boldsymbol{x}|y=1)}{P(\boldsymbol{x}|y=0)})$
    \item Geometrically, $z = \boldsymbol{\beta} \cdot \boldsymbol{x}$ defines a linear separating hyperplane: When $z > 0$ resp. log-odds $> 0$, then odds $> 1$ resp. $P(y=1|\boldsymbol{x}) > P(y=0|\boldsymbol{x})$, then predict $y = 1$, otherwise $y = 0$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Likelihood: $L(\boldsymbol{\beta}) = \prod_{i=1}^n P(y^{(i)} | \boldsymbol{x}^{(i)}; \boldsymbol{\beta}) = \prod_{i=1}^n \sigma(z^{(i)})^{y^{(i)}} (1 - \sigma(z^{(i)}))^{1 - y^{(i)}}$
    \item Log likelihood: 
    $\log L(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right] = \sum_{i=1}^n \left[ y^{(i)} \log \frac{1}{1 + e^{-z^{(i)}}} + (1 - y^{(i)}) \log \frac{e^{-z^{(i)}}}{1 + e^{-z^{(i)}}} \right] = \sum_{i=1}^n \left[ y^{(i)} z^{(i)} - \log (1 + e^{z^{(i)}}) \right]$
    \item Log loss: $ -\log L(\boldsymbol{\beta})$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\frac{\partial -\log L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = -\sum_{i=1}^n \frac{\partial}{\partial \boldsymbol{\beta}} [ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \\ \log (1 - \sigma(z^{(i)})) ] = \sum_{i=1}^n [ \sigma(z^{(i)}) - y^{(i)} ] \boldsymbol{x}^{(i)}$
    \\
    Proof:
    \begin{itemize}
        \item Derivative of the sigmoid: $\frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} = \sigma(z^{(i)}) (1 - \sigma(z^{(i)}))$
        \item Derivative of the first term: $
        \frac{\partial}{\partial \boldsymbol{\beta}} \left[ y^{(i)} \log \sigma(z^{(i)}) \right] = y^{(i)} \frac{1}{\sigma(z^{(i)})} \frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial \boldsymbol{\beta}} = y^{(i)} \frac{1}{\sigma(z^{(i)})} \sigma(z^{(i)}) (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = y^{(i)}  (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = y^{(i)}\boldsymbol{x}^{(i)}  - y^{(i)}\sigma(z^{(i)}) \boldsymbol{x}^{(i)}$
        \item Derivative of the second term: $\frac{\partial}{\partial \boldsymbol{\beta}} \left[ (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right] = (1 - y^{(i)}) \frac{1}{1 - \sigma(z^{(i)})} (-1) \frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial \boldsymbol{\beta}} = (1 - y^{(i)}) \frac{1}{1 - \sigma(z^{(i)})} (-1) \sigma(z^{(i)}) (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = -(1 - y^{(i)}) \sigma(z^{(i)}) \boldsymbol{x}^{(i)}= y^{(i)} \sigma(z^{(i)}) \boldsymbol{x}^{(i)} - \sigma(z^{(i)}) \boldsymbol{x}^{(i)}$
    \end{itemize}
    \item If we set gradient to 0, we have \emph{expectation matching}: $\sum_{i=1}^n \sigma(z^{(i)}) \boldsymbol{x}^{(i)} = \sum_{i=1}^n y^{(i)} \boldsymbol{x}^{(i)}$ 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
Log loss is convex – Proof:
\begin{itemize}
    \item Sum of convex functions is convex
    \item Thus, we need to prove convexity of $ \ln(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}}) $ and $ -y^{(i)} \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} $
    \item For second term:
    \begin{itemize}
        \item $\mathcal{H}(\boldsymbol{\beta}) = \nabla^2_{\boldsymbol{\beta}} \left(-y^{(i)} \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}\right) = 0$
        \item $ \mathcal{H} \succcurlyeq 0 $
    \end{itemize}
    \item For first term:
    \begin{itemize}
        \item $ \mathbb{H}(\boldsymbol{\beta}) = \nabla^2_{\boldsymbol{\beta}} \ln(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}}) $
        \item $ = v(x) \times u'(x) - v'(x) \times u(x) = \frac{1}{1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}} \times e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \times \boldsymbol{x} \boldsymbol{x}^\intercal - \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \times \boldsymbol{x}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \times \boldsymbol{x}^\intercal \times e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}$
        \item $= \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^\intercal (1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}) - e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^\intercal e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2}$
        \item $ = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^T}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} $
        \item $ \mathcal{H} \succcurlyeq 0 $\\
        Proof:
        $ \boldsymbol{a}^T \mathcal{H} \boldsymbol{a} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \boldsymbol{a}^T \boldsymbol{x} \boldsymbol{x}^T \boldsymbol{a} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \lVert \boldsymbol{a}^T \boldsymbol{x} \rVert^2 \geq 0$
    \end{itemize}
\end{itemize}
Perfectly separable data requires regularization – Proof:
\begin{itemize}
    \item Let weights for each class $k$ be scaled by $c$ as $c \boldsymbol{\tilde{\beta}}_k$
    \item Gradient of log-loss with respect to $c$ is always negative, causing gradient descent to grow $c$ without bound
    \begin{itemize}
        \item $\textrm{Log loss} = \sum_{i=1}^n \ln\left(1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}\right) - y^{(i)} c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}$
        \item $\nabla_c \textrm{ log loss} = \sum_{i=1}^n \frac{1}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} \times e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}} \times \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} - y^{(i)} \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}$
        \item $= \sum_{i=1}^n \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} \left(\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)}\right)$
        \item Given perfect separation:
        \begin{itemize}
            \item If $y^{(i)} = 1$, $\boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} > 0$, $\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)} < 0$
            \item If $y^{(i)} = 0$, $\boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} < 0$, $\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)} > 0$
        \end{itemize}
        \item Thus, for all $i$, $\nabla_c \textrm{ log loss} < 0$
    \end{itemize}
\end{itemize}