\section{Probability and Statistics}
\subsection*{Probability Distributions}
\emph{Normal distribution} --- 
$\mathcal{X} \sim \mathcal{N}(\mu, \sigma^2)$\\
For univariate, PDF: $\frac{1}{\sqrt{2\pi\sigma}} exp(\frac{-(x-\mu)^2}{2\sigma^2})$\\
For multivariate, PDF: $\frac{1}{{2\pi\sigma}^{n/2}} \frac{1}{|\boldsymbol{\Sigma}|^{1/2}} exp(-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}))$

{\color{lightgray}\hrule height 0.001mm}

\emph{Standard normal distribution} --- Normal distribution, standardized via z-score $z = \frac{x-\mu}{\sigma}$, which results in $\mu = 0$ and $\sigma = 1$

{\color{lightgray}\hrule height 0.001mm}

\emph{Bernoulli distribution} --- Trial with success (probability $p$) or failure (probability $1-p$)
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Bernoulli}(p)$
    \item PDF: $p(x) p^x (1-p)^x$
    \item Mean: $\mathbb{E}(x) = p$
    \item Variance: $\mathbb{V}(x) = p(1-p)$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Binomial distribution} --- $n$ independent Bernoulli trials with $k$ successes
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Bin}(n,p)$
    \item PDF: $\binom{n}{k} p^k (1-p)^{n-k}$
    \item Mean: $\mathbb{E}(x) = np$
    \item Variance: $\mathbb{V}(x) = np(1-p)$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Poisson distribution} ---
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Pois}(\lambda)$
    \item PDF: $e^{-\lambda} \frac{\lambda^x}{x!}$
    \item Mean: $\mathbb{E}(x) = \lambda$
    \item Variance: $\mathbb{V}(x) = \lambda$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Beta distribution} ---
\begin{itemize}
    \item $X$ takes values $\in [0,1]$
    \item Represents the probability of a Bernoulli process after observing $\alpha-1$ successes and $\beta-1$ failures
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Beta}(\alpha,\beta)$ where $\alpha,\beta > 0$
    \item PDF: $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)+\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}$
    where $\Gamma(\alpha) = \int_0^\infty u^{\alpha-1} e^{-u} du$
    \item Mean: $\mathbb{E}(x) = \frac{\alpha}{\alpha+\beta}$
    \item Variance: $\mathbb{V}(x) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Exponential families} ---  Family of probability distributions (incl. Gaussian, Poisson, Bernoulli, Categorical, Gamma, Beta) of the form: $ p(x \mid \boldsymbol{\theta}) = \frac{1}{Z(\boldsymbol{\theta})} \boldsymbol{h}(x) \exp\left(\boldsymbol{\theta} \cdot \boldsymbol{\Phi}(x)\right) $ where
\begin{itemize}
    \item $Z(\boldsymbol{\theta})$ is a \emph{partition function} which normalizes
    \item $\boldsymbol{h}(x)$ is the \emph{support}
    \item $\boldsymbol{\theta}$ are the \emph{parameters}
    \item $\boldsymbol{\Phi}(x)$ are the \emph{sufficient statistics}
\end{itemize}
Properties:
\begin{itemize}
    \item Finite sufficient statistics
    \item \emph{Conjugate priors}:
    \begin{itemize}
        \item Prior distribution $p$ for a likelihood distribution $q$ such that the posterior distribution induced by $p$ and $q$ is of the same form as $p$
        \item This helps to conduct Bayesian machine learning: Posterior parameters can be updated using prior parameters and do not have to be re-derived
        \item E.g.: Bernoulli distribution with Beta prior:
        \begin{itemize}
            \item Assume prior $P(\theta) \sim \textrm{Beta}(\alpha, \beta)$
            \item Likelihood $P(X \mid \theta) = \theta^k (1 - \theta)^{n-k}$, where $n = \textrm{trials}$ and $k = \textrm{successes}$
            \item Posterior $P(\theta \mid X) \propto P(X \mid \theta) P(\theta) \propto \theta^k (1 - \theta)^{n-k} \theta^{\alpha-1} (1 - \theta)^{\beta-1} \propto \theta^{k + \alpha - 1} (1 - \theta)^{n - k + \beta - 1}$
            \item Due to conjugate prior, posterior $\sim \textrm{Beta}(\alpha', \beta')$, with $\alpha' = k + \alpha$ and $\beta' = n - k + \beta$
        \end{itemize}
    \end{itemize}
\end{itemize}
Derivation from maximum entropy:
\begin{itemize}
    \item \emph{Maximum entropy principle}: When estimating a probability distribution over some constraints (e.g. known $\mathbb{E}$), we should choose the distribution that has the highest entropy
    \item \emph{Entropy} in discrete case:
    $
    H(x) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)
    $
    \item Assume we know $\mathbb{E}(\boldsymbol{\Phi}(x)) = F$
    \item Using the maximum entropy principle, we want to find $J(p) = -\sum_{x \in \mathcal{X}} p(x) \log p(x)$ subject to
    \begin{itemize}
        \item Non-negativity constraint: $p(x) \geq 0 \quad \forall x$
        \item Sum-to-one constraint: $\sum_{x \in \mathcal{X}} p(x) = 1$
        \item User-defined constraint: $\mathbb{E}(\boldsymbol{\Phi}(x)) = \sum_{x \in \mathcal{X}} p(x) \boldsymbol{\Phi}(x) = F$
    \end{itemize}
    \item This is a constrained optimization problem with the objective function: $
    J(p, \lambda) = -\sum_{x \in \mathcal{X}} p(x) \log p(x) - \lambda_0 \left(1 - \sum_{x \in \mathcal{X}} p(x)\right) + \sum_k \lambda_k \left(F_k - \sum_{x \in \mathcal{X}} p(x) \Phi_k(x)\right)
    $
    \item Then, the gradient is given by:
    $
    \frac{\partial J(p, \lambda)}{\partial p(x)} = -1 - \log(p(x)) - \lambda_0 - \sum_k \lambda_k \Phi_k(x) = 0
    $
    \item $\Rightarrow \log(p(x)) = -1 - \lambda_0 - \sum_k \lambda_k \Phi_k(x) = \log(\frac{1}{Z} \exp\left(-\sum_k \lambda_k \Phi_k(x)\right))$ where $Z = \exp(1 + \lambda_0)$
    \item $\Rightarrow p(x) = \frac{1}{Z} \exp\left(-\sum_k \lambda_k \Phi_k(x)\right)$
    \item $\Rightarrow
    \sum_{x \in \mathcal{X}} p(x) = \sum_{x \in \mathcal{X}} \frac{1}{Z} \exp\left(-\sum_k \lambda_k \Phi_k(x)\right) = 1
    $ by sum-to-one constraint
    \item $\Rightarrow
    Z = \sum_{x \in \mathcal{X}} \exp\left(-\sum_k \lambda_k \Phi_k(x)\right)$
    \item If we plug this $Z$ back into $p(x)$, we get:
    $
    p(x) = \frac{\exp\left(-\sum_k \lambda_k \Phi_k(x)\right)}{\sum_{x \in \mathcal{X}} \exp\left(-\sum_k \lambda_k \Phi_k(x)\right)} = \frac{1}{\sum_{x \in \mathcal{X}} \exp\left(-\boldsymbol{\lambda}^\intercal \boldsymbol{\Phi}(x)\right)} \exp\left(-\boldsymbol{\lambda}^\intercal \boldsymbol{\Phi}(x)\right)
    $
    \item Then, we see that this is in the exponential family
\end{itemize}
Proof that Gaussian is an exponential distribution:
\begin{itemize}
    \item Gaussian PDF:
    $
    f(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} $
    \item $= \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2 - 2 \mu x + \mu^2}{2 \sigma^2}}$
    \item $=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2} + \frac{2 \mu x}{2 \sigma^2} - \frac{\mu^2}{2 \sigma^2}}$
    \item $= \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} e^{\frac{\mu x}{\sigma^2}} e^{-\frac{\mu^2}{2 \sigma^2}} 
    $
    \item $= \frac{e^{-\frac{\mu^2}{2 \sigma^2}}}{\sqrt{2 \pi \sigma^2}} \times 1 \times e^{\frac{\mu x}{\sigma^2}} e^{-\frac{x^2}{2 \sigma^2}}$
    \item $= \frac{e^{-\frac{\mu^2}{2 \sigma^2}}}{\sqrt{2 \pi \sigma^2}} \times 1 \times e^{\frac{\mu}{\sigma^2}x-\frac{1}{2 \sigma^2} x^2}$
    \item $= \frac{e^{-\frac{\mu^2}{2 \sigma^2}}}{\sqrt{2 \pi \sigma^2}} \times 1 \times e^{[\frac{\mu}{\sigma^2},\frac{-1}{2\sigma^2}] \cdot [x,x^2]}$
    \item Thus, parameters of exponential family given by:
    \begin{itemize}
        \item $\frac{1}{Z(\boldsymbol{\theta})} = \frac{e^{-\frac{\mu^2}{2 \sigma^2}}}{\sqrt{2 \pi \sigma^2}}$
        $ \Rightarrow
        Z(\boldsymbol{\theta}) = \sqrt{2 \pi \sigma^2} \times \frac{1}{e^{-\frac{\mu^2}{2 \sigma^2}}} = \sqrt{2 \pi \sigma^2} e^{\frac{\mu^2}{2 \sigma^2}}
        $
        \item $\boldsymbol{h}(x) = 1$
        \item $\boldsymbol{\theta} = \begin{bmatrix}
        \mu / \sigma^2 \\
        -1 / 2 \sigma^2
        \end{bmatrix}$
        \item $
        \boldsymbol{\Phi}(x) = \begin{bmatrix}
        x \\
        x^2
        \end{bmatrix}
        $
    \end{itemize}
\end{itemize}
Proof that Poisson is an exponential distribution:
\begin{itemize}
    \item Poisson PDF:
    $
    f(x \mid \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}
    $
    \item $
    = \frac{1}{x!}e^{-\lambda} \lambda^x
    $
    \item $
    = \frac{1}{e^\lambda}\frac{1}{x!} e^{log(\lambda^x)}
    $
    \item Thus, parameters of exponential family given by:
    \begin{itemize}
        \item $\frac{1}{Z(\boldsymbol{\theta})} = \frac{1}{e^\lambda}$
        $ \Rightarrow
        Z(\boldsymbol{\theta}) = e^\lambda
        $
        \item $\boldsymbol{h}(x) = \frac{1}{x!}$
        \item $\boldsymbol{\theta} = \begin{bmatrix}
        log(\lambda)
        \end{bmatrix}$
        \item $
        \boldsymbol{\Phi}(x) = \begin{bmatrix}
        x
        \end{bmatrix}
        $
    \end{itemize}
\end{itemize}
Proof that Beta is an exponential distribution:
\begin{itemize}
    \item Beta PDF: $
    f(x \mid \alpha, \beta) = \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}
    $
    \item 
    $
    = \frac{1}{B(\alpha, \beta)} \times 1 \times  e^{(\alpha - 1) \ln(x) + (\beta - 1) \ln(1 - x)}
    $
    \item 
    $
    = \frac{1}{B(\alpha, \beta)} \times 1 \times  e^{[(\alpha - 1),(\beta - 1)] \times [\ln(x), \ln(1 - x)]}
    $
    \item Thus, parameters of exponential family given by:
    \begin{itemize}
        \item $\frac{1}{Z(\alpha, \beta)} = \frac{1}{B(\alpha, \beta)}$
        $ \Rightarrow
        Z(\alpha, \beta) = B(\alpha, \beta)
        $
        $ \Rightarrow
        Z(\boldsymbol{\theta}) = B(\theta_1 + 1, \theta_2 + 1)
        $
        \item $\boldsymbol{h}(x) = 1$
        \item $\boldsymbol{\theta} = \begin{bmatrix}
        (\alpha - 1)\\
        (\beta - 1)
        \end{bmatrix}$
        \item $
        \boldsymbol{\Phi}(x) = \begin{bmatrix}
        \ln(x)\\
        \ln(1-x)
        \end{bmatrix}
        $
    \end{itemize}
\end{itemize}
Proof that Chi Square is an exponential distribution:
\begin{itemize}
    \item Chi Square PDF: $
    f(x \mid k) = \frac{x^{k/2 - 1} e^{-x/2}}{2^{k/2} \Gamma(k/2)}
    $
    \item 
    $
    = \frac{1}{2^{k/2}\Gamma(k/2)} e^{- x/2} e^{(k/2 - 1) \ln(x)}
    $
    \item Thus, parameters of exponential family given by:
    \begin{itemize}
        \item $\frac{1}{Z(k)} = \frac{1}{2^{k/2}\Gamma(k/2)}$
        $ \Rightarrow
        Z(k) = 2^{k/2}\Gamma(k/2)
        $
        $ \Rightarrow
        Z(\boldsymbol{\theta}) = 2^{\boldsymbol{\theta} +1}\Gamma(\boldsymbol{\theta} + 1)
        $
        \item $\boldsymbol{h}(x) = e^{- x/2}$
        \item $\boldsymbol{\theta} = \begin{bmatrix}
        k/2 - 1
        \end{bmatrix}$
        \item $
        \boldsymbol{\Phi}(x) = \begin{bmatrix}
        \ln(x)
        \end{bmatrix}
        $
    \end{itemize}
\end{itemize}
Proof that Binomial is an exponential distribution:
\begin{itemize}
    \item Binomial PDF: $
    f(x \mid n, \pi) = \binom{n}{x} \pi^x (1 - \pi)^{n - x}
    $
    \item 
    $
    = \binom{n}{x} e^{x \ln(\pi)}  e^{(n - x) \ln(1 - \pi)}
    $
    \item 
    $
    = \binom{n}{x} e^{x \ln(\pi) - x\ln(1 - \pi) + n\ln(1 - \pi)}
    $
    \item 
    $
    = \binom{n}{x} e^{x \ln(\frac{\pi}{1-\pi})} e^{n\ln(1 - \pi)}
    $
    \item 
    $
    = e^{n\ln(1 - \pi)} \binom{n}{x} e^{\ln(\frac{\pi}{1-\pi}) x}
    $
    \item Thus, parameters of exponential family given by:
    \begin{itemize}
        \item $\frac{1}{Z(\pi)} = e^{n\ln(1 - \pi)}$
        $ \Rightarrow
        Z(\pi) = \frac{1}{e^{n\ln(1 - \pi)}}
        $
        $ \Rightarrow
        Z(\boldsymbol{\theta}) = e^{n\ln(1 + e^{\boldsymbol{\theta}})}
        $
        \item $\boldsymbol{h}(x) = \binom{n}{x}$
        \item $\boldsymbol{\theta} = \begin{bmatrix}
        \ln(\frac{\pi}{1-\pi})
        \end{bmatrix}$
        \item $
        \boldsymbol{\Phi}(x) = \begin{bmatrix}
        x
        \end{bmatrix}
        $
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Hypothesis Testing}
\emph{Terminology} ---
\begin{itemize}
    \item \emph{Hypothesis}:
    \begin{itemize}
        \item $H_0$: Accepted null hypothesis, e.g. $p=p_0$, $p_1-p_2=p_{0,1}-p_{0,2}=0$
        \item $H_A$: Alternative hypothesis, e.g. $p \neq p_0$, $p_1-p_2 \neq p_{0,1}-p_{0,2} \neq 0$
    \end{itemize}
    \item Errors:
    \begin{itemize}
        \item \emph{True positive}: Chose $H_0$, and $H_0$ obtains
        \item \emph{False negative, type I error}: Chose $H_A$, but $H_0$ obtains
        \item \emph{True negative}: Chose $H_A$, and $H_A$ obtains
        \item \emph{False positive, type II error}: Chose $H_0$, but $H_A$ obtains
    \end{itemize}
    \item \emph{Significance level} $\alpha$: 
    \begin{itemize}
        \item $\alpha \geq p(\textrm{type I error}) = p(\bar{x} \geq c \mid H_0)$ with equality for continuous variables 
        \item If $\alpha$ is small, the probability that we are erroneously rejecting $H_0$ is very small
        \item Set by us, typically at $5\%$
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $\alpha = p(\bar{x} \geq c \mid H_0) = p(\sqrt{n}\bar{x} \geq \sqrt{n}c \mid H_0) = p(z_n \geq \sqrt{n}c \mid H_0) = 1-\Phi(\sqrt{n}c)$ 
        where 
        \begin{itemize}
            \item $\Phi$ is the CDF of the normal distribution 
            \item $z_n \mid H_0 = \frac{\bar{x}-0}{1/\sqrt{n}} = \sqrt{n}\bar{x}$
        \end{itemize}
    \end{itemize}
    \item \emph{Critical value} $z$: 
    \begin{itemize}
        \item For two-sided: $z_{\alpha/2}$, $z_{1 - \alpha/2}$
        \item For one-sided upper tail: $z_{1 - \alpha}$
        \item For one-sided lower tail: $z_{\alpha}$
        \item Associated z-score with $\alpha$
        \item Corresponds to critical value $c$ prior to z-score transformation
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $\alpha = 1-\Phi(\sqrt{n}c) \Rightarrow c = \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha)$ where $\Phi$ is the CDF of the normal distribution
    \end{itemize}
    \item \emph{P-value} $p$: 
    \begin{itemize}
        \item For two-sided: $p = P(|z| \geq z_n)$
        \item For one-sided upper tail: $p = P(z \geq z_n)$
        \item For one-sided lower tail: $p = P(z \leq z_n)$
        \item Probability, given $H_0$ that we observe a value as or more extreme as the observed value $z_n$ 
        \item Smallest significance level resp. largest confidence level, at which we can reject $H_0$ given the sample observed
        \item If p-value is less than significance level resp. if observed value is more extreme than critical value, reject $H_0$, because the probability that we are erroneously doing so is very small
    \end{itemize}
    \item \emph{Confidence level}: $1-\alpha$, probability, given $H_0$, that we retain $H_0$
    \item \emph{Beta}: $\beta = p(\textrm{type II error})$
    \item \emph{Power}: 
    \begin{itemize}
        \item $1-\beta = p(\textrm{type II error}) = p(\bar{x} \geq c \mid H_1)$
        \item Probability, given $H_A$, that we reject $H_0$
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $1-\beta = p(\bar{x} \geq c \mid H_1) = p(\sqrt{n}(\bar{x}-1) \geq \sqrt{n}(c-1) \mid H_1) = p(z_n \geq \sqrt{n}(c-1) \mid H_1) = p(z_n \geq \sqrt{n}(c-1) \mid H_0) = 1-\Phi(\sqrt{n}(c-1))$ where 
        \begin{itemize}
            \item $\Phi$ is the CDF of the normal distribution 
            \item $z_n \mid H_1 = \frac{\bar{x}-1}{1/\sqrt{n}} = \sqrt{n}(\bar{x}-1)$
            \item We can switch from $\mid H_1$ to $\mid H_2$ because the two distributions follow the same form, just shifted
        \end{itemize}
    \end{itemize}
    \item Test types:
    \begin{itemize}
        \item \emph{Two-sided}: $H_0: p = p_0, H_A: p \neq p_0$
        \item \emph{One-sided upper tail}: $H_0: p \leq p_0, H_A: p > p_0$
        \item \emph{One-sided lower tail}: $H_0: p \geq p_0, H_A: p < p_0$
    \end{itemize}
    \item Calculating \emph{test statistic}:
    \begin{itemize}
        \item $z_n \mid H_0 = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Multiple comparisons problem} ---
Accumulation of false positive rate ($\alpha$) for $K$ tests, due to independence of tests:
$P(|\textrm{false rejections of }H_0| > 0) = 1-P(|\textrm{false rejections of }H_0| = 0) = 1-(1-\alpha)^K$

{\color{lightgray}\hrule height 0.001mm}

\emph{Corrections for multiple comparisons problem} ---
\emph{Bonferroni correction}: New significance level set to $\alpha* = \alpha / K$