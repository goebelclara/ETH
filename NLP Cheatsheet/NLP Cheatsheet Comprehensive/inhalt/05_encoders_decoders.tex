\section{Attention}
\subsection*{Description}
\begin{itemize}
    \item Helps specify which inputs we need to pay attention to when producing a given output
    \item Can be used:
    \begin{itemize}
        \item As \emph{cross-attention}: Between encoder and decoder
        \item As \emph{self-attention}: Within encoder or decoder
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Adjusted method}
Steps:
\begin{itemize}
    \item Generate three sets of re-weighted embeddings: 
    \begin{itemize}
        \item $\boldsymbol{Q} = \boldsymbol{E} \boldsymbol{W}^q$ resp. $\boldsymbol{q}_i = \boldsymbol{e}_i \boldsymbol{W}^q$
        \begin{multicols}{2}
        \begin{itemize}
            \item $\boldsymbol{E}$ $(m \times h)$
            \item $\boldsymbol{W}_q$ $(h \times d_k)$
            \item $\boldsymbol{Q}$ $(m \times d_k)$
            \item $\boldsymbol{q}_i$ is row vector $(1 \times d_k)$
            \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
        \end{itemize}
        \end{multicols}
        \item $\boldsymbol{K} = \boldsymbol{E} \boldsymbol{W}^k$ resp. $\boldsymbol{k}_i = \boldsymbol{e}_i \boldsymbol{W}^k$
        \begin{multicols}{2}
        \begin{itemize}
            \item $\boldsymbol{E}$ $(n \times h)$
            \item $\boldsymbol{W}_k$ $(h \times d_k)$
            \item $\boldsymbol{K}$ $(n \times d_k)$
            \item $\boldsymbol{k}_i$ is row vector $(1 \times d_k)$
            \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
        \end{itemize}
        \end{multicols}
        \item $\boldsymbol{V} = \boldsymbol{E} \boldsymbol{W}^v$ resp. $\boldsymbol{v}_i = \boldsymbol{e}_i \boldsymbol{W}^v$ where 
        \begin{multicols}{2}
        \begin{itemize}
            \item $\boldsymbol{E}$ $(n \times h)$
            \item $\boldsymbol{W}_v$ $(h \times d_v)$
            \item $\boldsymbol{V}$ $(n \times d_v)$
            \item $\boldsymbol{v}_i$ is row vector $(1 \times d_v)$
            \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
        \end{itemize}
        \end{multicols}
    \end{itemize}
    \item Compute \emph{similarity matrix}: $\boldsymbol{A} = \sigma(\frac{\boldsymbol{Q}\boldsymbol{K}^\intercal}{\sqrt{d_k}})$ in $(m \times n)$ resp. $\boldsymbol{\alpha}_{t} = \sigma(\frac{\boldsymbol{q}_t\boldsymbol{K}^\intercal}{\sqrt{d_k}})$ resp. $\alpha_{ti} = \frac{exp( \boldsymbol{q}_t \cdot \boldsymbol{k}_i )}{\sum_{i'} exp( \boldsymbol{q}_t \cdot \boldsymbol{k}_{i'} )}$
    \item Compute \emph{attention-weighted embedding matrix}: $\boldsymbol{Z} = \boldsymbol{A}\boldsymbol{V}$ in $(m \times d_v)$ resp. $\boldsymbol{z}_t = \boldsymbol{\alpha}_t\boldsymbol{V} = \sum_i \alpha_{ti} \boldsymbol{v}_i$ 
\end{itemize}
\begin{itemize}
    \item In cross-attention:
    \begin{itemize}
        \item $\boldsymbol{Q}$ is decoder input with $m$
        \item $\boldsymbol{V},\boldsymbol{K}$ are encoder outputs with $n$
    \end{itemize}
    \item In self-attention:
    \begin{itemize}
        \item $\boldsymbol{Q}, \boldsymbol{V},\boldsymbol{K}$ are all either encoder or decoder inputs with $n$ or $m$
        \item In \emph{masked self-attention}, states with time $\geq m$ in decoder are masked
    \end{itemize}
    \item In \emph{multi-head attention}:
    \begin{itemize}
        \item Creates multiple sets (\emph{heads}) of $\boldsymbol{Q,K,V}$ and calculates attention correspondingly
        \item Concatenates generated matrices $\boldsymbol{Z}$
        \item $\text{Multi Head Attention} \ \boldsymbol{Z} = \text{Concat}(\boldsymbol{Z}_{\text{head}_1}, \ldots, \boldsymbol{Z}_{\text{head}_h}) \boldsymbol{W}_O + \boldsymbol{b}_O$
        where 
        \begin{itemize}
            \item $\text{Concat}(...)$ in $(m \times (n \times n_{heads}))$
            \item $\boldsymbol{W}_O$ in $((n_{heads} \times n) \times d_v)$
            \item $\boldsymbol{b}_O$ in $1 \times d_v)$
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Further proofs}
Self-attention without positional encodings is permutation equivariant
\begin{itemize}
    \item \emph{Permutation equivariance}: $\text{Attention} \ \boldsymbol{\Pi}\boldsymbol{Z} = \boldsymbol{\Pi} \ \text{Attention} \ \boldsymbol{Z}$
    \item The self-attention is given by:
    $
    \boldsymbol{A} = \boldsymbol{Z}\boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}^\intercal
    $
    \item After permutation, self-attention is given by: 
    $
    \boldsymbol{A}' = (\boldsymbol{\Pi} \boldsymbol{Z})\boldsymbol{W}_q \boldsymbol{W}_k^\intercal (\boldsymbol{\Pi} \boldsymbol{Z})^\intercal = \boldsymbol{\Pi} \boldsymbol{Z}\boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}^\intercal \boldsymbol{\Pi}^\intercal = \boldsymbol{\Pi} (\boldsymbol{Z}\boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}^\intercal) \boldsymbol{\Pi}^\intercal = \boldsymbol{\Pi} \boldsymbol{A} \boldsymbol{\Pi}^\intercal
    $
    \item Applying softmax:
    $
    \text{softmax}(\boldsymbol{A}') = \text{softmax}(\boldsymbol{\Pi} \boldsymbol{A} \boldsymbol{\Pi}^\intercal) = \boldsymbol{\Pi} \ \text{softmax}(\boldsymbol{A})\ \boldsymbol{\Pi}^\intercal 
    $ since permutation matrix simply swaps rows and columns. The softmax operates on a matrix row-wise, i.e. the normalization for each row only depends on entries in that row. For this reason, it does not matter whether the permutation happens before or after applying the softmax
    \item Final output: $
    \boldsymbol{Z}' = \text{softmax}(\boldsymbol{A}') (\boldsymbol{\Pi} \boldsymbol{Z}) W_v = \boldsymbol{\Pi} \ \text{softmax}(\boldsymbol{A}) \ \boldsymbol{\Pi}^\intercal(\boldsymbol{\Pi} \boldsymbol{Z}) W_v = \boldsymbol{\Pi} \ \text{softmax}(\boldsymbol{A})(\boldsymbol{\Pi}^{-1}\boldsymbol{\Pi}) \boldsymbol{Z} W_v = \boldsymbol{\Pi} \ \text{softmax}(\boldsymbol{A})\boldsymbol{Z} W_v
    $ because $\boldsymbol{\Pi}$, as a permutation matrix, has exactly one $1$ in each row and each column and $0$ everywhere else. It is an orthogonal matrix, thus $\boldsymbol{\Pi}^\intercal = \boldsymbol{\Pi}^{-1}$ and $\boldsymbol{\Pi} \ \boldsymbol{\Pi}^{-1} = I$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

Self-attention with learned $\boldsymbol{Q}$ and without positional encodings is permutation invariant
\begin{itemize}
    \item \emph{Permutation invariance}: $\text{Attention} \ \boldsymbol{\Pi}\boldsymbol{Z} = \text{Attention} \ \boldsymbol{Z}$
    \item See proof above, but do not decompose $\boldsymbol{Q}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

Self-attention with positional encodings is not permutation equi- or invariant

\section{Positional Embeddings}
\begin{itemize}
    \item Can be absolute or relative
    \item Attention with \emph{absolute positional encodings}: $
    \boldsymbol{A}_{q,k}^{\text{absolute}} = \left( \boldsymbol{Z}_{q} + \boldsymbol{P}_{q} \right) \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \left( \boldsymbol{Z}_{k} + \boldsymbol{P}_{k} \right)^\intercal
    = \boldsymbol{Z}_{q} \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}_{k}^\intercal 
    + \boldsymbol{Z}_{q} \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{P}_{k}^\intercal 
    + \boldsymbol{P}_{q} \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}_{k}^\intercal 
    + \boldsymbol{P}_{q} \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{P}_{k}^\intercal
    $
    \item Attention with \emph{relative positional encodings}, where relative difference $\boldsymbol{\delta} = \boldsymbol{q}-\boldsymbol{k}$: 
    $
    \boldsymbol{A}_{q,k}^{\text{relative}} := \boldsymbol{Z}_{q} \boldsymbol{W}_q \boldsymbol{W}_k^\intercal \boldsymbol{Z}_{k}^\intercal 
    + \boldsymbol{Z}_{q} \boldsymbol{W}_q \widetilde{\boldsymbol{W}}_k \boldsymbol{r}_\delta 
    + \boldsymbol{u}^\intercal \boldsymbol{W}_k \boldsymbol{Z}_{k} 
    + \boldsymbol{v}^\intercal \widetilde{\boldsymbol{W}}_k \boldsymbol{r}_\delta
    $ where \emph{Gaussian encodings} are given by parameters
    \begin{itemize}
        \item $
        \boldsymbol{W}_q = \boldsymbol{W}_k = 0
        $
        \item $
        \widetilde{\boldsymbol{W}}_k = \boldsymbol{I}
        $
        \item $
        \boldsymbol{r}_\delta = 
        \begin{pmatrix}
        \|\delta\|^2 \\
        \delta_1 \\
        \delta_2
        \end{pmatrix}
        $
        \item $
        \boldsymbol{v} = -\alpha 
        \begin{pmatrix}
        1 \\
        -2\Delta_1 \\
        -2\Delta_2
        \end{pmatrix}
        $
        \item $\boldsymbol{v}$ and $\boldsymbol{r}_\delta$ are in $(1 \times d_p)$
        \item If these parameters are plugged into formula for attention with relative positional encodings, we recover formula for attention with absolute positional encodings
    \end{itemize}
    \item Relative encodings speed up the calculation of the attention vs. absolute encodings (since $d_p$ is very small), but applying softmax and calculating $\boldsymbol{Z}$ for relative encodings has the same complexity as for absolute encodings, thus diminishing the benefit
\end{itemize}

\section{Encoder Decoder RNNs}
\subsection*{Formulation}
\emph{Formulation} --- 
Model architecture:
\begin{itemize}
    \item Inputs fed into encoder in reverse order
    \item \emph{Encoder}: 
    \begin{itemize}
        \item Sequence-to-vector
        \item Hidden states $\boldsymbol{h}_n^{(e)} = f ( \boldsymbol{W}_1^{(e)} \boldsymbol{h}_{n-1}^{(e)} + \boldsymbol{W}_2 \boldsymbol{w}_{n} )$ where 
        \begin{itemize}
            \item $f$ is activation function
            \item $\boldsymbol{w}_{n}$ is input token embedding at time step $n$ in input sequence
            \item $\boldsymbol{h}_{n-1}^{(e)}$ is encoder hidden state from previous time step
        \end{itemize}
    \end{itemize}
    \item Outputs from encoder to decoder are weighted by attention weights:
    \begin{itemize}
        \item Context vector $\boldsymbol{z}_m = \sum_{n=1}^N \alpha_{m,n} \boldsymbol{h}_n^{(e)}$ where 
        \begin{itemize}
            \item $\boldsymbol{h}_n^{(e)}$ is the encoder hidden state (= $V$)
            \item $\alpha_{m,n}$ is attention weight at decoder time step $m$ for encoder hidden state at time step $n$, given by $ \textrm{softmax}(\boldsymbol{h}_{m-1}^{(d)} \times [\boldsymbol{h}_1^{(e)}, ..., \boldsymbol{h}_N^{(e)}])$ where $\boldsymbol{h}_{m-1}^{(d)}$ is previous decoder hidden state (= $Q$) and $[\boldsymbol{h}_1^{(e)}, ..., \boldsymbol{h}_N^{(e)}]^\intercal$ are the final encoder hidden states at each time step (= $K$)
        \end{itemize}
    \end{itemize}
    \item Alongside context vectors, target sequence inputs are fed into decoder with one time step lag during training
    \item \emph{Decoder}: 
    \begin{itemize}
        \item Vector-to-sequence
        \item Hidden states $\boldsymbol{h}_m^{(d)} = f ( \boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)} + \boldsymbol{W}_2^{(d)} \boldsymbol{w'}_{m-1} + \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m )$ where
        \begin{itemize}
            \item $f$ is activation function
            \item $\boldsymbol{w'}_{m-1}$ is target token embedding at time step $m-1$ in target sequence
            \item $\boldsymbol{h}_{m-1}^{(d)}$ is decoder hidden state from previous time step with $\boldsymbol{h}_0^{(d)} = \boldsymbol{h}_N^{(e)}$, i.e. last encoder output is first decoder input
            \item $\boldsymbol{z}_m$ is cross-attention
        \end{itemize} 
    \end{itemize}
\end{itemize}
Runtime analysis:
\begin{itemize}
    \item Let $\boldsymbol{W}^{(e)} \boldsymbol{h}$ be in $((N \times d) \times (d \times d))$ resp. $\boldsymbol{W}^{(d)} \boldsymbol{h}$ in $((M \times d) \times (d \times d))$
    \item Let number of encoder resp. decoder layers be $l_e, l_d$
    \item We perform $\boldsymbol{z}_m = \sum_{n=1}^N \alpha_{m,n} \boldsymbol{h}_n^{(e)}$ for $M$ decoder time steps, summing over $N$ encoder outputs $\boldsymbol{h}_n^{(e)}$ of dimensionality $d$
    \item Encoder: $O(l_e N d^2)$ from hidden states
    \item Decoder: $O(l_d M d^2 + l_d  d N M)$
    \begin{itemize}
        \item $O(l_d M d^2)$ from hidden states
        \item $O(l_d d N M)$ from cross-attention
    \end{itemize}
\end{itemize}
Challenge: Sequential, cannot be parallelized\\
Solution: Transformers

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\theta = \boldsymbol{W}_1^{(e)}, \boldsymbol{W}_2^{(e)}, \boldsymbol{W}_1^{(d)}, \boldsymbol{W}_2^{(d)}, \boldsymbol{W}_3^{(d)}$ 

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Maximize log likelihood
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Perform forward pass 
    \item Perform backpropagation
    \item Gradient with regard to encoder output:
    \begin{itemize}
        \item $\nabla_{\boldsymbol{h}_1^{(e)}} L = \frac{\partial L}{\partial \boldsymbol{h}_m^{(d)}} ) \frac{\partial \boldsymbol{h}_m^{(d)}}{\partial \boldsymbol{h}_{n}^{(e)}}$
        \item $\frac{\partial \boldsymbol{h}_m^{(d)}}{\partial \boldsymbol{h}_{n}^{(e)}} = \frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} \boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)}
        \times \frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m
        \times \frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} f ( \boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)} + \boldsymbol{W}_2^{(d)} \boldsymbol{w'}_{m-1} + \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m )$
        \item We can further decompose $\frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m$:
        \begin{itemize}
            \item $
            =\frac{\partial}{\partial \boldsymbol{h}_n^{(e)}} \alpha_{m,n} \boldsymbol{h}_n^{(e)} + \frac{\partial}{\partial \boldsymbol{h}_n^{(e)}} \sum_{i \neq n}^N \alpha_{m,i} \boldsymbol{h}_i^{(e)}
            $\\
            $
            = \alpha_{m,n} + \frac{\partial}{\partial \boldsymbol{h}_n^{(e)}} \alpha_{m,n}\boldsymbol{h}_n^{(e)} + \frac{\partial}{\partial \boldsymbol{h}_n^{(e)}} \sum_{i \neq n}^N \alpha_{m,i} \boldsymbol{h}_i^{(e)}
            $ due to product rule for $\alpha_{m,n} \boldsymbol{h}_n^{(e)}$\\
            $
            = \Phi_{m,n} + \Phi'_{m,n} \boldsymbol{h}_n^{(e)} + \sum_{i \neq n}^N \left[\Phi_{m,i} \frac{\partial \boldsymbol{h}_i^{(e)}}{\partial \boldsymbol{h}_n^{(e)}} + \Phi'_{m,i} \boldsymbol{h}_i^{(e)} \right]
            $ due to product rule for $\alpha_{m,i} \boldsymbol{h}_i^{(e)}$ and by replacing $\alpha_{m,n}$ with $\Phi_{m,n}$
        \end{itemize}
        \item Runtime analysis:
        \begin{itemize}
            \item $\frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} \boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)}$ is in $O(m \times N + N-n)$:\\
            1) Taking derivatives of $m$ decoder steps, due to attention $\boldsymbol{z}_m$ which is applied over $N$ encoder outputs, takes $O(m \times N)$\\
            2) Taking derivatives of $N-n$ encoder steps takes $O(N-n)$
            \item $\frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m$ is in $O(N)$:\\
            1) $\Phi_{m,n}$ and $\Phi'_{m,n}$ are in $O(1)$, since they don't contain $\boldsymbol{h}_{n}^{(e)}$\\
            2) $\sum_{i \neq n}^N \Phi_{m,i} \frac{\partial \boldsymbol{h}_i^{(e)}}{\partial \boldsymbol{h}_n^{(e)}}$ is in $O(N-n)$ if we reuse terms in chain rule by factorizing, since the derivative is only non-null for $N-n$ encoder steps \\
            3) $\sum_{i \neq n}^N  \Phi'_{m,i} \boldsymbol{h}_i^{(e)} $ is in $O(N)$, since here we're summing over all $N$ encoder steps
            \item $\frac{\partial}{\partial \boldsymbol{h}_{n}^{(e)}} f(...)$ is in $O(N)$:\\
            1) $\boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)}$ and $\boldsymbol{W}_2^{(d)} \boldsymbol{w'}_{m-1}$ are in $O(1)$, since they don't contain $\boldsymbol{h}_{n}^{(e)}$\\ 
            2) $\boldsymbol{W}_1^{(d)} \boldsymbol{z}_m$ is in $O(N)$, since it represents the attention applied over $N$ encoder outputs
        \end{itemize}
    \end{itemize}
    \item Perform gradient descent 
\end{itemize}

\section{Encoder Decoder Transformers}
\subsection*{Formulation}
\emph{Formulation} --- 
Model architecture:
\begin{itemize}
    \item Inputs fed into encoder in reverse order
    \item \emph{Encoder}: 
    \begin{itemize}
        \item Sequence-to-vector
        \item Takes in input sequence token embeddings (semantic vector, $\boldsymbol{X}$ in $(N \times d_{model})$) and positional embeddings (sinusoidal pointer vector for word position, given that model is not sequential, $\boldsymbol{P}$ in $(N \times d_{model})$) and adds them:
        $\boldsymbol{H}_0^{(e)} = \boldsymbol{X} + \boldsymbol{P}$
        \item Multi-head self-attention, applied to all tokens jointly, where $\boldsymbol{Q,K,V}$ are tokens in input sequence:
        \begin{itemize}
            \item $\boldsymbol{Q} = \boldsymbol{H}_{(l-1)}^{(e)} \boldsymbol{W}_q$
            \item $\boldsymbol{K} = \boldsymbol{H}_{(l-1)}^{(e)} \boldsymbol{W}_k$
            \item $\boldsymbol{V} = \boldsymbol{H}_{(l-1)}^{(e)} \boldsymbol{W}_v$
            \item $\text{Attention} \ \boldsymbol{Z} = \text{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^\top}{\sqrt{d_k}}\right) \boldsymbol{V}$
            \item $\text{Multi Head Attention} \ \boldsymbol{Z} = \text{Concat}(\boldsymbol{Z}_{\text{head}_1}, \ldots, \boldsymbol{Z}_{\text{head}_h}) \boldsymbol{W}_O + \boldsymbol{b}_O$
        \end{itemize}
        \item Addition and normalization: Skip connections (from token + positional embeddings) added back and normalized: $\boldsymbol{H}_l^{(e)} = \text{Layer Norm}(\text{Multi Head Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) + \boldsymbol{H}_{(l-1)}^{(e)})$
        \item Feed-forward network, parallel for each token: $\text{FFN}(\boldsymbol{H}_l^{(e)}) = \textrm{ReLU}(\boldsymbol{H}_l^{(e)} \boldsymbol{W}_1 + \boldsymbol{b}_1)\boldsymbol{W}_2 + \boldsymbol{b}_2$ where
        \begin{itemize}
            \item $\boldsymbol{W}_1 \in \mathbb{R}^{(d_v \times r)}$
            \item $\boldsymbol{b}_1 \in \mathbb{R}^{(1 \times r)}$
            \item $\boldsymbol{W}_2 \in \mathbb{R}^{(r \times d_v)}$
            \item $\boldsymbol{b}_2 \in \mathbb{R}^{(1 \times d_v)}$
        \end{itemize}
        \item Addition and normalization: Skip connections (from first addition and normalization) added back and normalized: $\boldsymbol{H}_l^{(e)} = \text{Layer Norm}(\text{FFN}(\boldsymbol{H}_l^{(e)}) + \boldsymbol{H}_l^{(e)})$
        \item Generates hidden states $\boldsymbol{h}_n^{(e)}$
    \end{itemize}
    \item \emph{Decoder}: 
    \begin{itemize}
        \item Vector-to-sequence
        \item Target sequence inputs are fed into decoder with one time step lag (masked self-attention): 
        \begin{itemize}
            \item Takes in target sequence token embeddings (semantic vector, $\boldsymbol{Y}$ in $(M \times d_{model})$) and positional embeddings (sinusoidal pointer vector for word position, given that model is not sequential, $\boldsymbol{P}$ in $(M \times d_{model})$) and adds them:
            $\boldsymbol{H}_0^{(d)} = \boldsymbol{Y} + \boldsymbol{P}$
            \item Masked self-attention, applied to all tokens jointly, where $\boldsymbol{Q,K,V}$ are tokens in target sequence:
            $\boldsymbol{Q} = \boldsymbol{H}_{(l-1)}^{(d)} \boldsymbol{W}_q$\\
            $\boldsymbol{K} = \boldsymbol{H}_{(l-1)}^{(d)} \boldsymbol{W}_k$\\
            $\boldsymbol{V} = \boldsymbol{H}_{(l-1)}^{(d)} \boldsymbol{W}_v$\\
            $\text{Masked Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^\top}{\sqrt{d_k}} + \textrm{mask}\right) \boldsymbol{V}$ where mask covers tokens in positions $m \geq t$
            \item Addition and normalization: Skip connections (from token + positional embeddings) added back and normalized: $\boldsymbol{H}_l^{(d)} = \text{Layer Norm}(\text{Masked Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) + \boldsymbol{H}_{(l-1)}^{(d)})$
        \end{itemize}
        \item Encoder outputs are fed into decoder with cross-attention: 
        \begin{itemize}
            \item Cross-attention:
            $\boldsymbol{Q} = \boldsymbol{H}_l^{(d)} \boldsymbol{W}_q$\\
            $\boldsymbol{K} = \boldsymbol{H}_{(N)}^{(e)} \boldsymbol{W}_k$\\
            $\boldsymbol{V} = \boldsymbol{H}_{(N)}^{(e)} \boldsymbol{W}_v$\\
            $\text{Cross Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) = \text{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^\top}{\sqrt{d_k}}\right) \boldsymbol{V}$
            \item Addition and normalization: Skip connections (from first addition and normalization) added back and normalized: $\boldsymbol{H}_l^{(d)} = \text{Layer Norm}(\text{Cross Attention}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V}) + \boldsymbol{H}_l^{(d)})$
        \end{itemize}
        \item Feed-forward network, parallel for each token: $\text{FFN}(\boldsymbol{H}_l^{(d)}) = \textrm{ReLU}(\boldsymbol{H}_l^{(d)} \boldsymbol{W}_1 + \boldsymbol{b}_1)\boldsymbol{W}_2 + \boldsymbol{b}_2$ where
        \begin{itemize}
            \item $\boldsymbol{W}_1 \in \mathbb{R}^{(d_v \times r)}$
            \item $\boldsymbol{b}_1 \in \mathbb{R}^{(1 \times r)}$
            \item $\boldsymbol{W}_2 \in \mathbb{R}^{(r \times d_v)}$
            \item $\boldsymbol{b}_2 \in \mathbb{R}^{(1 \times d_v)}$
        \end{itemize}
        \item Addition and normalization: Skip connections (from second addition and normalization) added back and normalized: $\boldsymbol{H}_l^{(d)} = \text{Layer Norm}(\text{FFN}(\boldsymbol{H}_l^{(d)}) + \boldsymbol{H}_l^{(d)})$
        \item Generates hidden states $\boldsymbol{h}_m^{(d)}$
    \end{itemize}
    \item Linear layer applied to $\boldsymbol{h}_M^{(d)}$
    \item Softmax layer applied to select token with highest probability:
    \begin{itemize}
        \item Neural networks make no independence assumption, i.e. output $y_t$ is conditioned on entire history (non-Markovian structure: $\boldsymbol{x},\boldsymbol{y}_{<t}$) rather than window of size $n$ (Markovian structure: $\boldsymbol{x},\langle y_t, ..., y_{t-1} \rangle$)
        \item This results in runtime of $O(|\Sigma|^n)$ rather than $O(|\Sigma| \times n)$
        \item Since it is intractable to search for best sequence overall (explore), we turn to deterministic or stochastic variants (exploit): 
        \begin{itemize}
            \item \emph{Greedy decoding}: 
            \begin{itemize}
                \item Select highest-probability token at each step
                \item Does not require normalization at each step
                \item E.g. if we have trigrams and vocabulary $\{a,b\}$. Let first scores be given by $\textrm{BOS}\textrm{BOS}a = 6, \textrm{BOS}\textrm{BOS}b = 4$, we pick $\textrm{BOS}\textrm{BOS}a$. We then check scores for $\textrm{BOS}aa = 7, \textrm{BOS}ab = 3$ and pick $\textrm{BOS}aa$. We then check scores for $aaa = 2, aab = 8$ and so on. 
            \end{itemize}
            \item \emph{Beam search}: 
            \begin{itemize}
                \item Keep $k$-highest-probability tokens in memory (beam size) at each step
                \item Requires normalization at each step
                \item E.g. if we have $k=2$, trigrams and vocabulary $\{a,b\}$. Let first probabilities be given by $\textrm{BOS}\textrm{BOS}a = 0.6, \textrm{BOS}\textrm{BOS}b = 0.4$, we keep both in memory.\\
                We then check local probabilities for $\textrm{BOS}aa = 0.7, \textrm{BOS}ab = 0.3, \textrm{BOS}ba = 0.1, \textrm{BOS}bb = 0.9$ and calculate total probabilities $\textrm{BOS}aa = 0.7 * 0.6, \textrm{BOS}ab = 0.3 * 0.6, \textrm{BOS}ba = 0.9 * 0.4, \textrm{BOS}bb = 0.1 * 0.4$. Thereof, we keep highest $2$ options, i.e. $\textrm{BOS}aa, \textrm{BOS}ba$.\\
                We then check local probabilities for $aaa = 0.7, aab = 0.3, baa = 0.1, bab = 0.9$ and so on. 
            \end{itemize}
            \item \emph{Nucleus sampling}: Sample tokens from items that cover $p\%$ of PMF
        \end{itemize}
    \end{itemize}
\end{itemize}
Runtime analysis: Cross-attention:
\begin{itemize}
    \item Computing $\boldsymbol{Q}$ with $O(m \times h \times d_k)$, $\boldsymbol{K}$ with $O(n \times h \times d_k)$, $\boldsymbol{V}$ with $O(n \times h \times d_v)$
    \item Assume $m = n$ and $d_k = d_v = d$
     \item Computing $\boldsymbol{A}$ with $O(m \times d_k \times n)$, computing $\boldsymbol{Z}$ with $O(m \times d_k \times n \times d_v)$
     \item Assume $m = 1$ (for one specific query) and $d_v = d$
     \item Total runtime for single layer: $O(n \times h \times d + h \times n \times d)$
\end{itemize}
Advantages vs. RNNs:
\begin{itemize}
    \item Relies on attention to obtain a fixed-size representation of a sequence
    \item Allows to learn longer-range dependencies 
    \item Allows for parallelization 
\end{itemize}

\section{Connection CNN and Multi Head Self Attention}
Theorem: A multi-head self-attention layer operating on $K^2$ heads of dimension $n$ and output dimension $d_v$, employing a relative positional encoding of dimension $d_p \geq 3$, can express any convolutional layer of kernel size $K \times K$ and $d_v$ output channels\\
Theorem part 1:
\begin{itemize}
    \item Given a multi-head self-attention layer with $n_{heads} = K^2$ and $n \geq d_v$
    \item Given a convolutional layer with a $K \times K$ kernel and $d_v$ output channels
    \item Let $f : [n_{heads}] \to \Delta_K$ be a bijective map between heads and shifts
    \item Assume $\text{softmax} \left( \boldsymbol{A} \right) =
    \begin{cases} 
    1 & \text{if } f(h) = \boldsymbol{q} - \boldsymbol{k} = \boldsymbol{\delta} \\
    0 & \text{otherwise}
    \end{cases}$
    \item Then, for any convolutional layer, there exists a corresponding weight per head $\boldsymbol{W}_v$ such that the multi-head self-attention equals the convolution
    \item Proof:
    \begin{itemize}
        \item Contribution of each head in multi-head self-attention is given by: $\boldsymbol{W} = \boldsymbol{W}_v \boldsymbol{W}_{\text{out}}^{(h)}$ where $\boldsymbol{W}_{\text{out}}^{(h)}$ is the portion of $\boldsymbol{W}_{\text{out}}$ associated with head $h$
        \item This means, we can rewrite $\text{Multi Head Attention} \ \boldsymbol{Z} = \sum_{h \in n_{heads}} \text{softmax}(\boldsymbol{A}^{(h)}) \boldsymbol{Z} \boldsymbol{W}^{(h)} + \boldsymbol{b}_O$
        \item This matches $\text{Convolution} \ \boldsymbol{Z} = \sum_{(u, v) \in \Delta_K} \boldsymbol{X}_{i', j'} \boldsymbol{W}_{u, v} + \boldsymbol{b}$
    \end{itemize}
\end{itemize}
Theorem part 2:
\begin{itemize}
    \item It is possible to construct a relative encoding scheme $\boldsymbol{r}_\delta$ using parameters $\boldsymbol{W}_q$, $\boldsymbol{W}_k$, $\widetilde{\boldsymbol{W}}_k$, and $\boldsymbol{u}$ so that, for every shift $\in \Delta_K$, there exists a vector $\boldsymbol{v}$ that yields the mapping $f : [n_{heads}] \to \Delta_K$
    \item Assume $\boldsymbol{A} = -\alpha \left( \|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2 + c \right)$
    \item Behavior for $\boldsymbol{\delta} = \boldsymbol{\Delta}$ resp. $\boldsymbol{\delta} \neq \boldsymbol{\Delta}$:
    \begin{itemize}
        \item Softmax is given by:
        $\text{softmax}(\boldsymbol{A}) = \frac{\exp\left(-\alpha \left( \|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2 + c \right)\right)}{\sum_{k'} \exp\left(-\alpha \left( \|\boldsymbol{\delta}' - \boldsymbol{\Delta}\|^2 + c \right)\right)}$
        \item In numerator:
        \begin{itemize}
            \item If $\boldsymbol{\delta} = \boldsymbol{\Delta}$, $\exp(\boldsymbol{A}) = \exp(-\alpha c)$
            \item If $\boldsymbol{\delta} \neq \boldsymbol{\Delta}$, $\exp(\boldsymbol{A}) \to 0$ as $\alpha \to \infty$, since entire term inside exponent grows very negative
        \end{itemize}
        \item In denominator: $\exp(\boldsymbol{A}) \to \exp(-\alpha c)$ as $\alpha \to \infty$, since only the term corresponding to $\boldsymbol{\delta} = \boldsymbol{\Delta}$ contributes significantly
        \item Then,
        \begin{itemize}
            \item If $\boldsymbol{\delta} = \boldsymbol{\Delta}$, softmax $\to 1$
            \item If $\boldsymbol{\delta} \neq \boldsymbol{\Delta}$, softmax $\to 0$
        \end{itemize}
        \item This proves assumption in part 1 of theorem
    \end{itemize}
    \item Constant $c$ is given by $c = \max_{\boldsymbol{\delta} \neq \boldsymbol{\Delta}} \|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2$:
    \begin{itemize}
        \item $\boldsymbol{A} = -\alpha \left( \|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2 + c \right)$
        \item To ensure proper softmax behavior $-\alpha c$ must dominate over $-\alpha \|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2$
        \item Then, we require $\|\boldsymbol{\delta} - \boldsymbol{\Delta}\|^2 + c \gg 0$ for $\boldsymbol{\delta} \neq \boldsymbol{\Delta}$
    \end{itemize}
\end{itemize}