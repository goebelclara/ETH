\section{Natural Language Processing (NLP) Basics}
\subsection*{Formulation}
\emph{Training data}  --- 
\begin{itemize}
    \item $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}$
    \item E.g.: 
    $\begin{bmatrix}
    \textrm{score for doc 1} \\
    ...\\
    \textrm{score for doc n}
    \end{bmatrix}
    $ 
    $=
    \begin{bmatrix}
    \textrm{feature 1 for doc 1} ... \textrm{feature m for doc 1} \\
    ...\\
    \textrm{feature 1 for doc n} ... \textrm{feature m for doc n}
    \end{bmatrix}
    $
    $
    \times 
    \begin{bmatrix}
    \textrm{coefficient for feature 1} \\
    ...\\
    \textrm{coefficient for feature m}
    \end{bmatrix}
    $
    \item Where $\textrm{feature j for doc i} = \boldsymbol{w}^{(i)} \cdot \boldsymbol{a}^{(i)} = 
    \begin{bmatrix}
    \textrm{weight of word 1 in doc i...weight of word n in doc i}
    \end{bmatrix}
    \times 
    \begin{bmatrix}
    \textrm{attribute j of vocab word 1} \\
    ...\\
    \textrm{attribute j of vocab word n}
    \end{bmatrix}
    $
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Terminology} --- 
\begin{itemize}
    \item \emph{Corpus}
    \item \emph{Document}: Contained in corpus, constitutes one of $n$ instances for model
    \item \emph{Tokens}: Document split into preprocessed words, which are the tokens
    \item \emph{Vocabulary}: Contains unique tokens in corpus
    \item \emph{Alphabet}: Contains unique symbols, of which tokens are composed
    \item Token-level features: 
    \begin{itemize}
        \item \emph{One-hot encoding}: Vector of length of vocabulary, with $1$ at index of token and $0$ everywhere else
        \item \emph{Embeddings}: Measure semantic and syntactic word similarities in higher dimensional space
    \end{itemize}
    \item Document-level features: Generated by pooling token-level features
    \item \emph{Pooling methods}:
    \begin{itemize}
        \item Sum pooling: $e(\boldsymbol{d}) = \sum_{t \in d} e(\boldsymbol{t})$
        \item Mean pooling: $e(\boldsymbol{d}) = \frac{1}{|d|} \sum_{t \in d} e(\boldsymbol{t})$ with token weights:
        \begin{itemize}
            \item \emph{One-hot encoding}: e.g., $[0, 1, 0]$
            \item \emph{Bag-of-words}: e.g., $[1, 2, 0]$
            \item \emph{TF-IDF}: Bag-of-words counts given by: $\frac{\textrm{vocab word frequency in document}}{\textrm{vocab word frequency in corpus}}$
        \end{itemize}
        \item Max pooling: $e(\boldsymbol{d}) = \max_{t \in d} e(\boldsymbol{t})$
    \end{itemize}
\end{itemize}
