\section{Kernel Methods}
\subsection*{Background on Kernel Methods}
\emph{Description} ---
\begin{itemize}
    \item Mechanism for tractably resp. implicitly mapping data into higher-dimensional feature space so that linear models can be used in this feature space
    \item To do so, we can employ the \emph{kernel trick} and the \emph{representer theorem}
    \item The requirements are that the kernel function fulfills \emph{Mercer's theorem}, i.e. the kernel is a Mercer kernel
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Kernel trick} ---
\begin{itemize}
    \item Allows to operate in higher-dimensional feature space, without explicitly calculating this transformation, but instead implicitly computing the inner product in this feature space via a kernel function
    \item Given two inputs $\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}$ and a feature map $\varphi: \mathbb{R}^m \rightarrow \mathbb{R}^k$ we can define an inner product on $\mathbb{R}^k$ via the kernel function: $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \item If a prediction function is described solely in terms of inner products in the input space, it can be lifted into the feature space by replacing the inner product with the kernel function
    \item Kernel trick requires that span of training instances  $span(\varphi(\boldsymbol{x^{(i)}}), ..., \varphi(\boldsymbol{x^{(N)}})) = \mathbb{R}^k$ and, thus, that $N \geq k$\\
    Proof:
    $dim(span(...)) = 
    \left\{
        \begin{aligned}
             & N \quad & \text{if } N < k \\
             & k \quad & \text{if } N \geq k
        \end{aligned}
    \right.
    $
    \item Kernel trick cannot be used in conjunction with feature selection resp. sparsity inducing regularize (e.g. $\ell_1$), as this does not satisfy the representer theorem
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Representer theorem} ---
\begin{itemize}
    \item Allows to avoid directly seeking the $k$ parameters, but only the $n$ parameters that characterize $\boldsymbol{\alpha}$
    \item Allows to avoid calculating $\varphi({\boldsymbol{z}})$ when evaluating novel instance, but only sum over weighted set of n kernel function outputs
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Mercer's theorem} ---
\begin{itemize}
    \item Kernel function is psd and symmetric iff $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \begin{itemize}
        \item Psd: $\boldsymbol{x}^\intercal \boldsymbol{K} \boldsymbol{x} \geq 0$ where $\boldsymbol{K}$ is the kernel matrix
        \item Symmetric: $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = k(\boldsymbol{x^{(j)}}, \boldsymbol{x^{(i)}})$
    \end{itemize}
    \item Kernel that satisfies Mercer's theorem is a Mercer kernel, i.e. we can prove a kernel is a Mercer kernel either if it is psd and symmetric or by finding a feature map such that the kernel function corresponds to an inner product    
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item Feature map $\varphi: \mathbb{R}^m \rightarrow \mathbb{R}^k$
    \item Linear prediction function: $\boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}})$
    \item Regularized loss function: $LO = \sum_{i=1}^n LO(y^{(i)}, \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}}) + \Omega( \boldsymbol{\beta} ))$
    \item Iff $\Omega( \boldsymbol{\beta} ))$ is a non-decreasing function, then the parameters $\boldsymbol{\beta}$ that minimize the loss function can be rewritten as: $\boldsymbol{\beta} = \sum_{i=1}^n \alpha^{(i)} \varphi( \boldsymbol{x^{(i)}} )$
    \item Outcome of novel instance can be predicted as: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} \varphi( \boldsymbol{x^{(i)}}) \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} k(\boldsymbol{x^{(i)}}, \boldsymbol{z})$
    \item Act of prediction becomes act of measuring similarity to training instances in feature map space
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Kernel Types}
\emph{Polynomial kernel} ---
\begin{itemize}
    \item $\varphi(\boldsymbol{x}) = [ 
    x^\alpha ]_{\alpha \in \mathbb{N}^m}$ where $\alpha = (\alpha_1, ..., \alpha_m)$ is the multi-index representing the power and $x^\alpha = x_1^{\alpha_1} \times ... \times  x_m^{\alpha_m}$ is the mononomial term corresponding to the multi-index $\alpha$
    \item E.g. if degree = 2, then $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = 1 + 2 x_1^{(i)} x_1^{(j)} + 2 x_2^{(i)} x_2^{(j)} + ( x_1^{(i)} x_1^{(j)} )^2 + ( x_2^{(i)} x_2^{(j)} )^2 + 2 x_1^{(i)} x_1^{(j)} x_2^{(i)} x_2^{(j)}$
    \item Inner product diverges to infinity
    \item To address this, we often use RBF kernel instead
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{RBF kernel} ---
\begin{itemize}
    \item $\varphi(\boldsymbol{x}) = exp ( -\frac{1}{2} \|\boldsymbol{x}\|^2 ) [ \frac{ \boldsymbol{x}^\alpha }{ \sqrt{\alpha!} } ]_{\alpha \in \mathbb{N}^m}$ 
    \item $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = \sigma^2 exp (- \frac{ \| \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} \|^2 }{ 2 l^2 } )$\\
    Proof:
    \begin{itemize}
        \item $exp ( -\frac{1}{2} \|\boldsymbol{x^{(i)}}\|^2 ) exp ( -\frac{1}{2} \|\boldsymbol{x^{(j)}}\|^2 ) \sum_\alpha [ \frac{ \boldsymbol{x^{(i)\alpha}} \boldsymbol{x^{(j)\alpha}} }{ \alpha! } ]$
        \item Given multinomial series expansion, $\sum_\alpha [ \frac{ \boldsymbol{x^{(i)\alpha}} \boldsymbol{x^{(j)\alpha}} }{ \alpha! } ] = exp(\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}})$
        \item Then, we get $exp ( -\frac{1}{2} \|\boldsymbol{x^{(i)}}\|^2  -\frac{1}{2} \|\boldsymbol{x^{(j)}}\|^2 + \boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}) = exp (- \frac{ \| \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} \|^2 }{ 2 } )$
    \end{itemize}
    \item Gives access to infinite feature space\\
    \hl{Proof}:
    \begin{itemize}
        \item By expanding the square, the kernel can be rewritten as:
        $ k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \sigma^2 \exp(-\frac{\|\boldsymbol{x^{(i)}}\|^2}{2l^2}) \exp(-\frac{\|\boldsymbol{x^{(j)}}\|^2}{2l^2}) \exp(\frac{\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}}{l^2}) $
        \item We can expand the last term:
        $ \exp(\frac{\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}}{l^2}) = \sum_{k=0}^\infty \frac{(\frac{\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}}{l^2})^k}{k!} $.
        \item Then, we get:
        $ k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \sigma^2 \exp(-\frac{\|\boldsymbol{x^{(i)}}\|^2}{2l^2}) \exp(-\frac{\|\boldsymbol{x^{(j)}}\|^2}{2l^2}) \sum_{k=0}^\infty \frac{(\frac{\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}}{l^2})^k}{k!} $.
        \item Assume a feature map of the form $ \phi_k(\boldsymbol{x}) = P_k(\boldsymbol{x}) \exp(-\frac{\|\boldsymbol{x}\|^2}{2l^2}) $, where $ P_k(\boldsymbol{x}) $ is a polynomial
        \item Then, we can equate:
        $ \sum_{k=0}^\infty P_k(\boldsymbol{x^{(i)}}) P_k(\boldsymbol{x^{(j)}}) e^{-\frac{\|\boldsymbol{x^{(i)}}\|^2}{2l^2}} e^{-\frac{\|\boldsymbol{x^{(j)}}\|^2}{2l^2}} = \sum_{k=0}^\infty \frac{(\frac{\boldsymbol{x^{(i)\intercal}} \boldsymbol{x^{(j)}}}{l^2})^k}{k!} e^{-\frac{\|\boldsymbol{x^{(i)}}\|^2}{2l^2}} e^{-\frac{\|\boldsymbol{x^{(j)}}\|^2}{2l^2}} $
        \item We can see that the RBF kernel corresponds to an infinite-dimensional feature space, since the series expansion of the exponential represents an infinite sum of polynomials 
        \item Based on the equation, we can specify the polynomials to $ P_k(\boldsymbol{x}) = \frac{\boldsymbol{x}^k}{\sqrt{k!}} $
        \item This results in a feature map of the form:
        $ \phi_k(\boldsymbol{x}) = \frac{\boldsymbol{x}^k}{\sqrt{k!}} \exp(-\frac{\|\boldsymbol{x}\|^2}{2l^2}) $
    \end{itemize}
    \item Length scale parameter $l$ controls how quickly the similarity decays with distance: If $l$ is large, points with high distance still have high covariance
    \item Variance parameter $\sigma$ controls the vertical scale of the function
    \item RBF kernel is \emph{stationary}, meaning that only the relative distance between two points determines the value output by the kernel function
    \item Challenge: Cannot ignore irrelevant dimensions (whereas e.g. a neural network can do this by setting the associated weights to $0$)
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Periodic kernel} ---
\begin{itemize}
    \item Suitable for capturing periodic patterns
    \item $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = \sigma^2 exp (- \frac{ 2 \sin^2 \frac{ \pi \| \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} \|}{p} }{ l^2 } )$
    \item Period of oscillation $p$ controls the length of the cycle
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Laplace kernel} ---
\begin{itemize}
    \item Suitable for modeling sharper edges than the RBF kernel
    \item $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = \sigma^2 exp (- \frac{ | \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} | }{ l } )$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Kernel compositions} ---
\begin{itemize}
    \item New valid kernels can be composed via:
    \begin{itemize}
        \item Addition: $k_1 + k_2$
        \item Multiplication: $k_1 \times k_2$
        \item Scaling: $c \times k_1$ for $c > 0$
        \item Composition: $f(k_1)$ where $f$ is a polynomial with positive coefficients or the exponential function
    \end{itemize}
    \item Valid kernels:
    \begin{itemize}
        \item $k'(x_1,x_2) = c k(x_1,x_2)$, since $\varphi'(x) = \sqrt{c}\varphi(x)$
        \item $k'(x_1,x_2) = f(x_1) k(x_1,x_2) f(x_2)$, since $\varphi'(x) = f(x)\varphi(x)$
        \item $k'(x_1,x_2) = k_1(x_1,x_2) + k_2(x_1,x_2)$, since the requirements for a valid kernel are that its psd and symmetric, which is retained when two psd and symmetric matrices are added resp. since $\varphi'(x) = 
        \begin{bmatrix} 
        \varphi_1(x)\\
        \varphi_2(x)
        \end{bmatrix}$
        \item $k'(x_1,x_2) = k_1(x_1,x_2) k_2(x_1,x_2)$, since new kernel is given by the $i^{th}$ feature value under feature map $\varphi_1$ multiplied by the $j^{th}$ feature value under feature map $\varphi_2$
        \item $k'(x_1,x_2) = exp(k(x_1,x_2))$, since we can apply Taylor series expansion $\sum_{n=1}^r \frac{k(x_1,x_2)^r}{r!} = exp(k(x_1,x_2)) = k'(x_1,x_2)$ as $r \rightarrow \infty$ and we know that exponentiation, addition, and scaling produces valid new kernels from above
        \item $k'(x_1,x_2) = x_1^\intercal \boldsymbol{A} x_2 $ for psd and symmetric $\boldsymbol{A}$
    \end{itemize}
\end{itemize}

\section{Polynomial Kernel Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $\boldsymbol{y} = \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}})$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Ordinary least squares estimator (OLSE)
    \item Minimize mean squared error: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}} ))^2$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Primal solution:
    \begin{itemize}
        \item Parameters can be estimated as: $\boldsymbol{\beta} = (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1}  \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
        \item Prediction for novel instance: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1}  \boldsymbol{\Phi}^\intercal \boldsymbol{y} \cdot \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal \boldsymbol{\Phi} (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1} \varphi({\boldsymbol{z}}) $
    \end{itemize}
    \item Let us define $\boldsymbol{K} = \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal$ as the kernel matrix of the training data with $K_{ij} = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \item Dual solution $\boldsymbol{\alpha}$ if we have no regularization, i.e. $\lambda = 0$:
    \begin{itemize}
        \item Parameters can be estimated as: $\boldsymbol{\beta} =  \boldsymbol{\Phi}^\intercal \boldsymbol{K}^{-1} \boldsymbol{y}$\\
        Proof:
        \begin{itemize}
            \item $(\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi} + \lambda \boldsymbol{I}_D) \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
            \item $\Rightarrow \boldsymbol{\Phi}^\intercal \boldsymbol{\Phi} \boldsymbol{\beta} + \lambda \boldsymbol{I}_D \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
            \item $\Rightarrow \boldsymbol{I}_D \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \lambda^{-1}  (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$
            \item Since we know from the representer theorem that $\boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha}$, we can say: $\boldsymbol{\alpha} = \lambda^{-1}  (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$ 
            \item We can further develop this to: $\lambda \boldsymbol{\alpha} = (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$
            \item Replacing $\boldsymbol{\beta}$ by $\boldsymbol{\Phi}^\intercal \boldsymbol{\alpha}$ yields: $\lambda \boldsymbol{\alpha} = (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha})$
            \item $\Rightarrow \boldsymbol{\alpha} = (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \lambda \boldsymbol{I}_N)^{-1} \boldsymbol{y} = \boldsymbol{K}^{-1} \boldsymbol{y}$
            \item With this, we can calculate the parameters: $\boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha} = \boldsymbol{\Phi}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \lambda \boldsymbol{I}_N)^{-1} \boldsymbol{y} = \boldsymbol{\Phi}^\intercal \boldsymbol{K}^{-1} \boldsymbol{y}$
        \end{itemize}\\
        Proof 2:
        \begin{itemize}
            \item According to \emph{Sherman-Morrison-Woodbury Formula}, $(FH^{-1}G + E)^{-1}FH^{-1} = E^{-1}F(GE^{-1}F+H)^{-1}$
            \item If we assume $E = \boldsymbol{I}_D, F = \Phi^\intercal, G= \boldsymbol{\Phi},H=\boldsymbol{I}_N$, the formula simplifies to $( \boldsymbol{\Phi}^\intercal  \boldsymbol{\Phi} + \boldsymbol{I}_D)^{-1} \boldsymbol{\Phi}^\intercal = \boldsymbol{I}_D^{-1}  \boldsymbol{\Phi}^\intercal ( \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \boldsymbol{I}_N)^{-1}$
            \item Since $\boldsymbol{I}_D^{-1} = \boldsymbol{I}_D$, we have: $( \boldsymbol{\Phi}^\intercal  \boldsymbol{\Phi} + \boldsymbol{I}_D)^{-1} \boldsymbol{\Phi}^\intercal =  \boldsymbol{\Phi}^\intercal ( \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \boldsymbol{I}_N)^{-1}$
            \item $\Rightarrow ( \boldsymbol{\Phi}^\intercal  \boldsymbol{\Phi} + \boldsymbol{I}_D)^{-1} \boldsymbol{\Phi}^\intercal y = \hat{\boldsymbol{\beta}} =  \boldsymbol{\Phi}^\intercal( \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \boldsymbol{I}_N)^{-1}y$
        \end{itemize}
        \item Prediction for novel instance: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal)^{-1}  \boldsymbol{\Phi} \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal)^{-1} \boldsymbol{k}$ where $\boldsymbol{k} = \boldsymbol{\Phi} \varphi({\boldsymbol{z}}) = [ k(\boldsymbol{x^{(1)}}, \boldsymbol{z}), ..., k(\boldsymbol{x^{(n)}}, \boldsymbol{z}) ]^\intercal = [ \varphi(\boldsymbol{x^{(1)}}) \cdot \varphi(\boldsymbol{z}), ..., \varphi(\boldsymbol{x^{(n)}}) \cdot \varphi(\boldsymbol{z}) ]^\intercal$ is a kernel vector, consisting of kernel values between training instances and new instance
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Algorithm} --- 
Training:
\begin{enumerate}
    \item Compute kernel matrix given RBF kernel\\
    Time complexity: $\mathcal{O}(n^2 \times m)$ for $n^2$ kernel matrix values and $m$ number of features in each instance vector
    \item Perform training by solving $\boldsymbol{\alpha} = \boldsymbol{K}^{-1} \boldsymbol{y}$ for $\boldsymbol{\alpha}$\\
    Time complexity: $\mathcal{O}(n^3)$ 
    \item Store $\boldsymbol{\alpha}$\\
    Space complexity: $\mathcal{O}(n^2)$ 
\end{enumerate}
Prediction:
\begin{enumerate}
    \item Compute kernel vector\\
    Time complexity: $\mathcal{O}(n \times m \times d)$ for $d$ new instances, given $n$ instances in training data and $m$ features in each instance vector
    \item Store $\boldsymbol{k}$\\
    Space complexity: $\mathcal{O}(n \times d)$ for $d$ new instances, given $n$ as length of kernel vector
    \item Predict response using stored kernel vector\\
    Time complexity: $\mathcal{O}(n \times d)$ for $d$ new instances, given $n$ as length of $\boldsymbol{\alpha}$
\end{enumerate}
Value:
\begin{itemize}
    \item Primal solution training is of time complexity $\mathcal{O}(k^3)$ and prediction is of time complexity $\mathcal{O}(k)$
    \item Dual solution speeds this up as seen above in the algorithm
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically 
\end{itemize}