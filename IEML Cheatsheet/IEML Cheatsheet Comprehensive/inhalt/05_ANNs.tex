\section{Neural Networks}
\subsection*{Formulation}
\emph{Formulation} --- 
\begin{itemize}
    \item Model architecture: 
    \begin{itemize}
        \item Input features $(\boldsymbol{X})$ > weights $(\boldsymbol{B})$ > weighted sums $(\boldsymbol{S})$ > activation functions $(\varphi)$ > hidden states $(\boldsymbol{H})$ > weights > ... > hidden states > activation function > outputs
        \item E.g.
        \begin{itemize}
            \item Outputs: Probability $p(y \mid \boldsymbol{x})$ for each class $y$
            \item Activation: Softmax ensures all $P$ add to 1:
            $
            p(y \mid \boldsymbol{x}) = \frac{\exp \left( \boldsymbol{h}_y^{(K)} \right)}{\sum_{y'} \exp \left( \boldsymbol{h}_{y'}^{(K)} \right)}
            $
            \item Hidden layer:
            $
            \boldsymbol{h}^{(K)} = \sigma(\boldsymbol{w}^{(K)}\boldsymbol{h}^{(K-1)})
            $\\
            $...$\\
            $
            \boldsymbol{h}^{(1)} = \sigma(\boldsymbol{w}^{(1)}\boldsymbol{e}(\boldsymbol{x}))
            $
            \item Concatenated vector of word embeddings:
            $
            \boldsymbol{e}(\boldsymbol{x}) = \frac{1}{n} \sum_{w_i} \boldsymbol{e}(w_i)
            $
            \item Transformation: word embedding $\boldsymbol{e}(w_i)$
            \item Inputs: $n$ words
        \end{itemize}
    \end{itemize}
\end{itemize}
\begin{itemize}
    \begin{itemize}
        \item Can be seen as a composition of:
        \begin{itemize}
            \item Encoders that construct disentangled and robust latent representation from input, which maximizes mutual information between representation and input, as according to infomax principle
            \item Linear estimators
        \end{itemize}
    \end{itemize}
    \item Neuron $(j)$ in layer $[k]$ given training instance $\boldsymbol{x}^{(i)[0]}$ resp. instance from previous layer $\boldsymbol{h}^{(i)[k-1]}$ given by:\\
    $\boldsymbol{h}^{(j)[1]} = \varphi( \boldsymbol{x}^{(i)[0]} \cdot \boldsymbol{\beta}^{(j)[1]} )$ resp. $\boldsymbol{h}^{(j)[k]} = \varphi( \boldsymbol{h}^{(i)[k-1]} \cdot \boldsymbol{\beta}^{(j)[k]} )$ 
    \item Outputs for neurons $1, ..., j$ in fixed layer (notation for layer omitted below) given by:\\
    $\boldsymbol{H} = \varphi ( \boldsymbol{X} \boldsymbol{B} ) = \varphi ( \boldsymbol{S} )$ where
    \begin{itemize}
        \item $\boldsymbol{X} \in \mathbb{R}^{n \times m+1}$ (incl. bias term)
        \item $\boldsymbol{B} \in \mathbb{R}^{m+1 \times j}$ with a weight vector for each neuron in each column (incl. bias term)
        \item $\boldsymbol{S} \in \mathbb{R}^{n \times j}$ with the weighted sum (prior to activation) for instance $i$ in neuron $j$ is on the $i^{th}$ row and $j^{th}$ column
        \item The activation function differs by neuron
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Activation functions} --- 
\begin{itemize}
    \item Introduce non-linearities
    \item Can differ by neuron
    \item Sigmoid:
    \begin{itemize}
        \item $[0,1]$
        \item $\varphi(z) = \sigma(z) =  \frac{1}{1+e^{-z}} = \frac{e^z}{e^z + 1}$
        \item $\varphi'(z) = \frac{e^{-z}}{(1+e^{-z})^2}$ with maximum at $0.25$
    \end{itemize}
    \item Hyperbolic Tangent:
    \begin{itemize}
        \item $[-1,1]$
        \item $\varphi(z) = tanh(z) =  \frac{e^z - e^{-z}}{e^z + e^{-z}} = \frac{1 - e^{-2z}}{1 + e^{-2z}}$
        \item $\varphi'(z) = 1-tanh(z)^2$
    \end{itemize}
    \item ReLu:
    \begin{itemize}
        \item $\varphi(z) = max(0,z)$
        \item $\varphi'(z) = 1 \textrm{ if } z > 0; 0 \textrm { otherwise}$
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\theta = \boldsymbol{B}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize standard objectives, e.g. MSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Perform \emph{forward pass} with randomly initialized parameters, to calculate loss
    \item Perform \emph{backpropagation}, to calculate gradient:
    \begin{itemize}
        \item $\frac{\partial L}{\partial \theta} = [ \frac{\partial L}{\partial \boldsymbol{B}^{[0]}} , ..., \frac{\partial L}{\partial \boldsymbol{B}^{[output]}} ]$
        \item $\frac{\partial L}{\partial \boldsymbol{B}^{[k]} } = \frac{\partial L}{ \partial \boldsymbol{H}^{[l]} } \frac{ \partial \boldsymbol{H}^{[l]} }{ \partial \boldsymbol{B}^{[k]} } = C$
        \begin{itemize}
            \item When $l > k+1$, i.e. when going several layers back: $\frac{\partial L}{\partial \boldsymbol{B}^{[k]} } = \frac{\partial L}{ \partial \boldsymbol{H}^{[l]} }  \frac{ \partial \boldsymbol{H}^{[l]} }{ \partial \boldsymbol{S}^{[l-1]} } \frac{ \partial \boldsymbol{S}^{[l-1]} }{ \partial \boldsymbol{H}^{[l-1]} } \frac{ \partial \boldsymbol{H}^{[l-1]} }{ \partial \boldsymbol{B}^{[k]} }$ 
            \item When $l = k+1$, i.e. when going one layer back: $\frac{\partial L}{\partial \boldsymbol{B}^{[k]} } = \frac{\partial L}{ \partial \boldsymbol{H}^{[k+1]} }  \frac{ \partial \boldsymbol{H}^{[k+1]} }{ \partial \boldsymbol{S}^{[k]} } \frac{ \partial \boldsymbol{S}^{[k]} }{ \partial \boldsymbol{B}^{[k]} }$
        \end{itemize}
    \end{itemize}
    \item Perform gradient descent to find best weights 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Challenges} ---
\begin{itemize}
    \item Unstable gradients:
    \begin{itemize}
        \item Can happen since backpropagation computes gradients using the chain rule, meaning many gradients are multiplied across many layers
        \item Caused by poor choice of activation, typically sigmoid or tanh with high absolute input values
        \item Caused when weights are shared across many layers, especially in RNNs
        \item Exploding gradients: If gradients are $> 1$, gradients grow bigger and bigger during backpropagation, algorithm diverges
        \item Vanishing gradients: If gradients are $< 1$ (resp. parameter $\theta$ is $< 1$), gradients approach $0$ during backpropagation, algorithm fails to converge\\
        Proof:
        \begin{itemize}
            \item $h_m = \sigma(\theta h_{m-1} + x_m)$
            \item $ \frac{\partial h_{m+k}}{\partial h_m}  = \prod_{i=0}^{k-1}  \frac{\partial h_{m+k-i}}{\partial h_{m+k-i-1}} = \prod_{i=1}^{k}  \theta \times \sigma'(\theta h_{m+k-i-1} + x_{m+k-i}) $ 
            \item $\leq \prod_{i=1}^{k}  \theta \times 0.25 = \theta ^k \times 0.25^k$ since derivative of sigmoid has $0.25$ as maximum value
            \item $\to 0$ as $k \to \infty$, since $ \theta  < 1$
        \end{itemize}
        \item Solution:
        \begin{itemize}
            \item Use fewer layers
            \item Use ReLU activation function
            \item Use residual networks (ResNet)
            \item Use LSTM or GRU units
            \item Glorot or He initialization: Connection weights of each layer are initialized randomly
            \item Batch normalization
            \item Gradient clipping: Set maximum threshold for gradients during backpropagation
        \end{itemize}
    \end{itemize}
    \item Dying ReLUs:
    \begin{itemize}
        \item Caused when weights are tweaked such that a neuron becomes negative, causing ReLU activation to output 0
        \item Can happen if $\beta_0$ in $x^T w + \beta_0$ is large and negative
        \item Dead neuron cannot be brought back:
        \begin{itemize}
            \item Let $z = x^T w + b^{[l]}$.
            \item $\text{ReLU} \left( b^{[l]} \right) = 0$ if $b^{[l]} \leq 0$
            \item Then $\frac{\partial l}{\partial z} = 0$, $\frac{\partial z}{\partial h} = 0$, $\frac{\partial h}{\partial b^{[l]}} = 0$
            \item $h_{b^{[l]}}$ is guaranteed to be zero for all inputs if $h$ is dead
            \item Then, parameters cannot change and $h$ will remain dead
        \end{itemize}
        \item Solution: Leaky ReLU, ELU, scaled ELU
    \end{itemize}
\end{itemize}