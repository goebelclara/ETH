\section{Linear Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$ where $\boldsymbol{X}$ contains $n$ rows, each of which represents an instance, and $m$ columns, each of which represents a feature
    \item To incorporate offset, first column of $\boldsymbol{X}$ (i.e. first feature) is set to $1$ and first element of $\boldsymbol{\beta}$ is set to $\beta_0$
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item $\boldsymbol{\beta}$ lies in the rowspace of $\boldsymbol{X}$ resp. columnspace of $\boldsymbol{X}^\intercal$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\emph{Ordinary least squares estimator (OLSE)}:
\begin{itemize}
    \item Minimize mean squared error: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( y - \boldsymbol{\beta} \cdot \boldsymbol{x})^2 = (y - \boldsymbol{\beta} \cdot \boldsymbol{x})\boldsymbol{x} = 0$
    resp.     
    $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )) = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (\boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - 2 \boldsymbol{y}^\intercal \boldsymbol{X} \boldsymbol{\beta}) = \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^\intercal (\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Alternatives to OLSE} --- 
MLE:
\begin{itemize}
    \item Yields same result as OLSE
    \item The likelihood is:
    $
    p(\boldsymbol{y} \mid \boldsymbol{\beta}, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2 \right)
    $
    \item The log likelihood is:
    $
    \mathcal{L} = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2
    $
    \item We minimize: $ \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2$
    \item This is equivlent to OLSE
\end{itemize}
Orthogonality principle:
\begin{itemize}
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item We wish to minimize $\| \hat{\boldsymbol{y}} - \boldsymbol{y} \| = \| \boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y} \|$ by selecting $\boldsymbol{\beta}$ appropriately
    \item By the orthogonality principle, $\boldsymbol{x}^{[j]} \cdot (\hat{\boldsymbol{y}} - \boldsymbol{y}) = \boldsymbol{x}^{[j]} \cdot (\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) = 0$ where $\boldsymbol{x}^{[j]}$ is the $j^{th}$ column of $\boldsymbol{X}$\\
    resp.\\
    $\boldsymbol{X}^\intercal (\hat{\boldsymbol{y}} - \boldsymbol{y}) = \boldsymbol{X}^\intercal(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
    \item Alternatively, $\boldsymbol{\beta}$ lies in the columnspace of $\boldsymbol{X}^\intercal$
    \item Then, we can express $\boldsymbol{\beta}$ as $\boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$
    \item This yields an equation system $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$ which can be solved for $\alpha_i$
    \item On that basis, $\boldsymbol{\beta}$ can be calculated
\end{itemize}
Pseudo Inverse:
\begin{itemize}
    \item Yields same result as OLSE
    \item Minimum-norm solution
    \item $\boldsymbol{\beta}$ minimizes MSE if $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item Given matrix projection via SVD, $\boldsymbol{X} \boldsymbol{X}^{\#} \boldsymbol{y}$ is that projection
    \item $\Rightarrow \boldsymbol{\beta} = \boldsymbol{X}^{\#} \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
    \item Shows that $\boldsymbol{\beta}$ is largely determined by $\boldsymbol{X}^{\#}$ and, thus, singular values of $\boldsymbol{X}$ based on SVD
\end{itemize}
PCA:
\begin{itemize}
    \item Instances $y^{(i)}, \boldsymbol{x}^{(i)} = \boldsymbol{\xi}^{(i)}$ can be projected onto hyperplane given by $\boldsymbol{X}\boldsymbol{\beta}$
    \item Projections are given by $\hat{\boldsymbol{\xi}}^{(i)}$
    \item Residuals are given by $e^{(i)} = \boldsymbol{\xi}^{(i)} - \hat{\boldsymbol{\xi}}^{(i)}$
    \item Since $e^{(i)}$ is orthogonal to $\hat{\boldsymbol{\xi}}^{(i)}$, we can write using Pythagorean theorem: $\| e^{(i)} \|^2 = \| \boldsymbol{\xi}^{(i)} \|^2 - \| \hat{\boldsymbol{\xi}}^{(i)} \|^2$
    \item This is a PCA via SVD problem
\end{itemize}
Gradient descent:
\begin{itemize}
    \item Minimum-norm solution
    \item Yields same result as OLSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Hypothesis Testing of Found Parameters} --- 
\begin{itemize}
    \item Let $\boldsymbol{y} | \boldsymbol{X} \sim \mathcal{N} (\boldsymbol{y}, \sigma^2 \boldsymbol{I}) = \mathcal{N} (\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I})$
    \item Let $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^+ \boldsymbol{y}$ be the OLSE where $\boldsymbol{X}^+$ is a scalar
    \item Then, $\boldsymbol{\hat{\beta}} \sim \mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{\beta}, (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \sigma^2 )$\\Proof:
    \begin{itemize}
        \item $\mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X}^{+ \intercal}  )$ since $\boldsymbol{X}^+$ is a scalar
        \item Further, we have $\mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ ((\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal)^\intercal  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1 \intercal}  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  )$ since $(\boldsymbol{X}^\intercal \boldsymbol{X})$ is symmetric
    \end{itemize}
    \item We can estimate $\sigma^2$ unbiasedly as: $\hat{\sigma}^2 = \frac{1}{n-m} \sum_{i \leq n} (\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y})^2$
    \item Then, confidence interval for $\hat{\beta_j}$ given by: $\hat{\beta_j} \pm z_{\alpha/2} \hat{se}(\hat{\beta_j})$ where 
    \begin{itemize}
        \item $z_{\alpha/2} = \Phi^{-1}(\alpha/2)$ is Gaussian CDF
        \item $\hat{se}(\hat{\beta_j})$ is the $j^{th}$ diagonal element of the covariance matrix $\sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$
    \end{itemize}
    \item We can perform a hypothesis test on $\hat{\beta}$ with the \emph{Wald test}:
    \begin{itemize}
        \item $H_0 : \beta = \beta_0$ (typically 0)\\
        $H_1 : \beta \neq \beta_0$
        \item Wald statistic: $W = \frac{\hat{\beta} - \beta_0}{\hat{se}}$
        \item If p-value associated with $W$ is smaller than $\alpha$ resp. if $|W|$ is greater than or equal to the critical value $z_{\alpha/2}$, we reject $H_0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Evaluation} ---
\begin{itemize}
    \item OLSE is unbiased if noise $\epsilon$ has zero mean:
    \begin{itemize}
        \item Given $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \epsilon$, we can substitute $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal (\boldsymbol{X}\boldsymbol{\beta} + \epsilon) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \epsilon$
        \item Taking the expected value on both sides, we have: $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \mathbb{E}(\epsilon)$
        \item Then, $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta}$ if the noise has zero mean
    \end{itemize}
    \item \emph{Gauss Markov theorem}: OLSE is best (lowest variance, lowest MSE) unbiased estimator, if assumptions ($\boldsymbol{X}$ is full rank and there is no multicollinearity, heteroskedasticity, and exogeneity) are met\\
    Proof:
    \begin{itemize}
        \item Let $\hat{\boldsymbol{\beta}} = \boldsymbol{A}^\intercal \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$ be the OLSE
        \item Let $\boldsymbol{C}^\intercal \boldsymbol{y}$ be another unbiased estimator
        \item $\mathbb{V}(\hat{\boldsymbol{\beta}}) = \mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \boldsymbol{A}^\intercal \mathbb{V}(\boldsymbol{y}) \boldsymbol{A}$ since $\boldsymbol{A}$ is constant
        \item We can further develop to: $\boldsymbol{A}^\intercal \sigma^2 \boldsymbol{I}_m \boldsymbol{A} = \sigma^2 \boldsymbol{A}^\intercal \boldsymbol{A}$ since variance is given by error term
        \item Similarly, $\mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y}) = \sigma^2 \boldsymbol{C}^\intercal \boldsymbol{C} $
        \item For the OLSE, we can plug in $(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal$ for $\boldsymbol{A}$ which yields: $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$ 
        \item Then, we have shown that $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) \leq \mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y})$
    \end{itemize}
    \item Nonetheless, there may be biased estimators that generate a lower variance and MSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically, if $\boldsymbol{X}^\intercal \boldsymbol{X}$ is invertible
    \item In case of \emph{multicollinearity}:
    \begin{itemize}
        \item The rank of $X$ is less than full, i.e. there are multiple columns (predictor variables) that are linearly dependent
        \item $\boldsymbol{X}^\intercal \boldsymbol{X}$ is singular, i.e. non-invertible
        \item There are multiple solutions for $\boldsymbol{\beta}$
    \end{itemize}
    \item If it has infinitely many solutions, the preferred solution is the \emph{minimum-norm solution}, which minimizes $\| \boldsymbol{\beta} \|$ and lies in the column space of $\boldsymbol{X}^\intercal$ resp. is a solution to $\boldsymbol{X}\boldsymbol{u}$
\end{itemize}

\section{Linear Minimum Mean Squared Error Estimation (LMMSE)}
\subsection*{Description}
\begin{itemize}
    \item Minimizes mean squared error of two random variables, leveraging information about their mean and covariance
    \item Linear regression with large samples is a data-based proxy for LMMSE, since $\frac{1}{n} \mathbb{E}[\boldsymbol{X}^\intercal\boldsymbol{X}] = \mathbb{E}[\boldsymbol{y}\boldsymbol{y}^\intercal]$ and $\frac{1}{n} \mathbb{E}[\boldsymbol{X}^\intercal\boldsymbol{y}] = \mathbb{E}[x\boldsymbol{y}]$ and as $n \to \infty$ the strong law of large numbers applies
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $\boldsymbol{y} = \boldsymbol{a}x + \boldsymbol{z}$ is a vector of random variables and is observed
    \item $\boldsymbol{a}$ is a known vector
    \item $\boldsymbol{z}$ a zero-mean noise vector
    \item $x$ is a random variable and quantity of interest
    \item We estimate $x$ as $\hat{x} = \boldsymbol{h}^\intercal \boldsymbol{y} = \sum_i h_i y_i$
    \item This can be considered as a projection of $x$ to the space spanned by $\boldsymbol{y}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{h}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize expected squared error: $LO = \mathbb{E}[ | \hat{x} - x | ]$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item By the orthogonality principle, $\mathbb{E} [ ( \hat{x} - x ) y_i ] = \mathbb{E} [ ( \boldsymbol{h}^\intercal \boldsymbol{y} - x ) y_i ] = \mathbb{E} [ ( \sum_{l=1}^n h_l y_l - x ) y_i ] = 0$ for $i = 1, ..., n$
    \item Then, $ \boldsymbol{h}^\intercal \mathbb{E}[\boldsymbol{y}y_i] = \sum_{l=1}^n \mathbb{E}[ y_l y_i ] h_l = \mathbb{E}[ x y_i ]$ for $i = 1, ..., n$ which in matrix notation corresponds to
    $\begin{bmatrix}
    \mathbb{E}[y_1 y_1] & ... & \mathbb{E}[y_1 y_n]\\
    ... & ... & ...\\
    \mathbb{E}[y_n y_1] & ... & \mathbb{E}[y_n y_n]\\
    \end{bmatrix} 
    \begin{bmatrix}
    h_1 \\
    ... \\
    h_n
    \end{bmatrix} = 
    \begin{bmatrix}
    \mathbb{E}[x y_1]\\
    ...\\
    \mathbb{E}[x y_n]
    \end{bmatrix}$
    resp. concisely
    $\mathbb{E}[ \boldsymbol{y} \boldsymbol{y}^\intercal ] \boldsymbol{h} = \mathbb{E}[x  \boldsymbol{y}]$ where output is column vector (similar to linear regression), or $\boldsymbol{h}^\intercal\mathbb{E}[ \boldsymbol{y} \boldsymbol{y}^\intercal ] = \mathbb{E}[x  \boldsymbol{y}^\intercal]$ where output is row vector (peculiar to LMMSE)
    \item Then, $\boldsymbol{h}^\intercal = \frac{\mathbb{E}[x  \boldsymbol{y}^\intercal]}{\mathbb{E}[ \boldsymbol{y} \boldsymbol{y}^\intercal ]}$ 
    \item Note for exam: First set up matrix notation, then (if applicable) replace $y_i$ by its observation $h_0 + h_1x + z$, then evaluate elements in matrix, based on information given, then set up equation system, then solve it for $h_i$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item $\mathbb{E}[ \boldsymbol{y} \boldsymbol{y}^\intercal ]$ is not invertible if there exists an $\boldsymbol{a} \neq 0$ such that $\boldsymbol{y}^\intercal\boldsymbol{a} = 0$ with probability 1 \\
    \hl{Proof}:
    \begin{itemize}
        \item Both $\boldsymbol{y}$ and $\boldsymbol{a}$ are in $\mathbb{R}^n$
        \item Direction 1:
            \begin{itemize}
            \item If $\boldsymbol{y}^\intercal\boldsymbol{a} = 0$, these two vectors are orthogonal. Then, $\boldsymbol{y}$ cannot span the entire space $\mathbb{R}^n$
            \item This means that some rows or columns in the matrix $\boldsymbol{y}\boldsymbol{y}^\intercal$ are linearly dependent
            \item By the invertible matrix theorem, such matrices are not invertible
        \end{itemize}
        \item Direction 2:
        \begin{itemize}
            \item If the matrix $\boldsymbol{y}\boldsymbol{y}^\intercal$ is not invertible, by the invertible matrix theorem, it must have linearly dependent rows or columns
            \item This means that one column or row can be expressed in terms of the sum over all other columns and rows
            \item Then, the equation system $\boldsymbol{y}\boldsymbol{y}^\intercal\boldsymbol{a} = 0$ is underdetermined
            \item Then, it is possible to formulate $\boldsymbol{y}^\intercal\boldsymbol{a} = 0$ such that $\boldsymbol{a} \neq 0$ 
        \end{itemize}
    \end{itemize}
\end{itemize}