\section{Ridge ($\ell_2$) Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$ subject to $\|\boldsymbol{\beta}\|^2 \leq t$ resp. $\|\boldsymbol{\beta}\|^2 - t \leq 0$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize mean squared error subject to constraint
    \item Lagrangian formulation: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2 + \lambda ( \|\boldsymbol{\beta}\|^2 - t )$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} ) + \lambda ( \|\boldsymbol{\beta}\|^2 - t )$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\nabla_{\boldsymbol{\beta}} LO = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Alternative formulations} --- 
Still a OLSE problem:
\begin{itemize}
    \item We can rewrite the objective to minimize $\|\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}\|^2 + \lambda\|\boldsymbol{\beta}\|^2 = \| \begin{bmatrix} 
    \boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y} \\
    \lambda \boldsymbol{\beta}
    \end{bmatrix} \|^2$ as the objective to minimize $\|\boldsymbol{X}'\boldsymbol{\beta} - \boldsymbol{y}'\|^2$ with $\boldsymbol{X}' = 
    \begin{bmatrix} 
    \boldsymbol{X} \\
    \lambda \boldsymbol{I}
    \end{bmatrix}$ 
    and $\boldsymbol{y}' = 
    \begin{bmatrix} 
    \boldsymbol{y} \\
    0
    \end{bmatrix}$
\end{itemize}
Bayesian regression (MAP estimation), where $\boldsymbol{\beta}$ is modeled as a zero-mean Gaussian variable, corresponds to ridge regression if $\lambda$ is chosen as $\frac{\sigma^2}{\tau^2}$, where $\sigma$ is standard deviation of $\boldsymbol{y}$ and $\tau$ is standard deviation of $\boldsymbol{\beta}$, where high $\tau$ implies low confidence in prior:
\begin{itemize}
    \item Prior $p( \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item $\boldsymbol{\beta} \propto \mathcal{N}(0, \tau^2 \boldsymbol{I}_m)$
        \item $p( \boldsymbol{\beta} ) = \frac{ 1 }{ (2\pi\tau^2)^{m/2}} exp ( -\frac{1}{2\tau^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta} ) $
    \end{itemize}
    \item Likelihood $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item Conditional on $\boldsymbol{\beta}$, $\boldsymbol{y} \propto \mathcal{N}(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n)$
        \item $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} ) = \frac{ 1 }{ (2\pi\sigma^2)^{n/2}} exp ( -\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) ) $
    \end{itemize}
    \item Posterior $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} )$: 
    \begin{itemize}
        \item $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto p ( \boldsymbol{y} |\boldsymbol{X}, \boldsymbol{\beta} ) \times p( \boldsymbol{\beta} ) \propto \log (p ( \boldsymbol{y} |\boldsymbol{X}, \boldsymbol{\beta} ) \times p( \boldsymbol{\beta} )) \propto \log (exp ( -\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) ) \times exp( -\frac{1}{2T^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta} )) \propto -\frac{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2}{\sigma^2} - \frac{\|\boldsymbol{\beta} \|^2}{\tau^2}$
    \end{itemize}
    \item If we maximize log likelihood resp. minimize log loss, we have: $\arg\min_{\beta} \frac{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2}{\sigma^2} + \frac{\|\boldsymbol{\beta} \|^2}{\tau^2} = \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2 + \frac{\sigma^2}{\tau^2}\|\boldsymbol{\beta} \|^2$
    \item This mirrors log loss of ridge regression with $\lambda = \frac{\sigma^2}{\tau^2}$
\end{itemize}
Orthogonality principle:
\begin{itemize}
    \item We wish to minimize $\| \hat{\boldsymbol{y}} - \boldsymbol{y} \| = \| \boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y} \|$ by selecting $\boldsymbol{\beta}$ subject to the condition that $\boldsymbol{C}\boldsymbol{\beta} = \boldsymbol{d}$
    \item Let $\boldsymbol{\beta}$ and $\tilde{\boldsymbol{\beta}}$ be solutions of this condition
    \item Then, we can rewrite condition as: $\boldsymbol{C}\boldsymbol{\beta} - \boldsymbol{C}\tilde{\boldsymbol{\beta}} = \boldsymbol{d} - \boldsymbol{d} = \boldsymbol{C}(\boldsymbol{\beta} - \tilde{\boldsymbol{\beta}}) = 0$
    \item Then, $(\boldsymbol{\beta} - \tilde{\boldsymbol{\beta}})$ is in the nullspace of $\boldsymbol{C}$, which is spanned by the columns of $\boldsymbol{B}$
    \item Then, $(\boldsymbol{\beta} - \tilde{\boldsymbol{\beta}})$ can be represented as a linear combination of the basis of the nullspace: $(\boldsymbol{\beta} - \tilde{\boldsymbol{\beta}}) = \boldsymbol{B}\boldsymbol{\beta}'$
    \item From this, we get  $\boldsymbol{\beta}= \boldsymbol{B}\boldsymbol{\beta}' +\tilde{\boldsymbol{\beta}}$
    \item Then, the cost function amounts to $\| \boldsymbol{X}(\boldsymbol{B}\boldsymbol{\beta}' +\tilde{\boldsymbol{\beta}}) - \boldsymbol{y} \| = \| \boldsymbol{X} \boldsymbol{B} \boldsymbol{\beta}' + \boldsymbol{X}\tilde{\boldsymbol{\beta}} - \boldsymbol{y} \| = \| \boldsymbol{X}'\boldsymbol{\beta}' - \boldsymbol{y}' \|$ where $\boldsymbol{X}' = \boldsymbol{X}\boldsymbol{B}$ and $\boldsymbol{y}' = \boldsymbol{y} - \boldsymbol{X}\tilde{\boldsymbol{\beta}}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Effect} ---
\begin{itemize}
    \item Shrinks certain elements of $\boldsymbol{\beta}$ to near 0\\
    Proof:
    \begin{itemize}
        \item Gradient at optimality given by $\frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} + 2 \lambda \boldsymbol{\beta} = 0$
        \item Then, $\boldsymbol{\beta}^* = -\frac{1}{2 \lambda} \frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}}$
        \item This means that each parameter is shrunk by a factor determined by size of $\lambda$ - the larger $\lambda$, the more the parameters are shrunk
        \item Larger parameters experience a larger shrinkage
    \end{itemize}
    \item Addresses multicollinearity:
    \begin{itemize}
        \item SVD for $X = USV^\intercal$
        \item We can show that $X \beta = US\left(S^2 + \lambda I\right)^{-1}SU^\intercal Y$\\
        Proof:
        \begin{itemize}
            \item $X \boldsymbol{\beta}^r = X\left(X^\intercal X + \lambda I\right)^{-1}X^\intercal Y$
            \item $= UDV^\intercal \left((UDV^\intercal)^\intercal UDV^\intercal + \lambda I\right)^{-1}(UDV^\intercal)^\intercal Y$
            \item $= UDV^\intercal \left(VDU^\intercal UDV^\intercal + \lambda I\right)^{-1}VDU^\intercal Y$
            \item $= UDV^\intercal \left(VD^2V^\intercal + \lambda VV^\intercal\right)^{-1}VDU^\intercal Y$
            \item $= UDV^\intercal \left(V(D^2 + \lambda I)V^\intercal\right)^{-1}VDU^\intercal Y$
            \item $= UDV^\intercal V\left(D^2 + \lambda I\right)^{-1}V^\intercal VDU^\intercal Y$
            \item $= UD\left(D^2 + \lambda I\right)^{-1}DU^\intercal Y$
        \end{itemize}
        \item Similarly, we can show that $ \| \beta \|^2 = Y^\intercal US^2\left(S^2 + \lambda I\right)^{-2}U^\intercal Y = \frac{W^\intercal S^2 W}{(S^2 + \lambda I)^2}$ where $W = U^\intercal Y$
        \item In case of multicollinearity, the rank of $X$ is less than full, and $S^2$ cannot be inverted. By adding $\lambda I$ to $S$, ridge regression ensures that the equation remains solvable even if $S$ is not invertible on its own
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Strictly with pd Hessian, since Lagrangian term is strictly convex and the sum of a strictly convex function with a convex function is strictly convex
    \item Has global minimum
    \item Has unique solution, as $(\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})$ has linearly independent columns
    \item Can be solved analytically, as $(\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})$ is always invertible
\end{itemize}

\section{Lasso ($\ell_1$) Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$ subject to $|\boldsymbol{\beta}| \leq t$ resp. $|\boldsymbol{\beta}| - t \leq 0$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize mean squared error subject to constraint
    \item Lagrangian formulation: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2 + \lambda ( |\boldsymbol{\beta}| - t )$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} ) + \lambda ( |\boldsymbol{\beta}| - t )$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Alternative formulations} --- 
Bayesian regression (MAP estimation), where $\boldsymbol{\beta}$ is modeled as a zero-mean Laplacian variable, corresponds to LASSO regression if $\lambda$ is chosen as $\frac{\sigma^2}{b}$, where $\sigma$ is standard deviation of $\boldsymbol{y}$ and $b$ is the scale parameter of the Laplacian prior, where high $b$ implies low confidence in prior:
\begin{itemize}
    \item Prior $p( \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item $\boldsymbol{\beta} \propto \text{Laplacian}(0, b)$
        \item $p( \boldsymbol{\beta} ) = \prod_{j=1}^m \frac{1}{2b} \exp\left(-\frac{|\beta_j|}{b}\right)$
    \end{itemize}
    \item Likelihood $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item Conditional on $\boldsymbol{\beta}$, $\boldsymbol{y} \propto \mathcal{N}(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n)$
        \item $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} ) = \frac{ 1 }{ (2\pi\sigma^2)^{n/2}} \exp\left( -\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) \right)$
    \end{itemize}
    \item Posterior $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} )$: 
    \begin{itemize}
        \item $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto p( \boldsymbol{y} |\boldsymbol{X}, \boldsymbol{\beta} ) \times p( \boldsymbol{\beta} )$
        \item $\log p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto \log \left( \exp\left(-\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})\right) \times \prod_{j=1}^m \exp\left(-\frac{|\beta_j|}{b}\right) \right)$
        \item $\log p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto -\frac{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2}{\sigma^2} - \frac{\|\boldsymbol{\beta}\|_1}{b}$
    \end{itemize}
    \item If we maximize log likelihood resp. minimize log loss, we have: 
    $
    \arg\min_{\boldsymbol{\beta}} \frac{\|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2}{\sigma^2} + \frac{\|\boldsymbol{\beta}\|_1}{b} = \|\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta} \|^2 + \frac{\sigma^2}{b}\|\boldsymbol{\beta}\|_1
    $
    \item This mirrors the log loss of LASSO regressionwith $\lambda = \frac{\sigma^2}{b}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Effect} ---
\begin{itemize}
    \item Shrinks certain elements of $\boldsymbol{\beta}$ to 0\\
    Proof:
    \begin{itemize}
        \item Gradient at optimality given by $\frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} + \frac{\partial \lambda  |\boldsymbol{\beta}|}{\partial \boldsymbol{\beta}} = 0$
        \item $\frac{\partial \lambda  |\boldsymbol{\beta}|}{\partial \boldsymbol{\beta}}$ non-differentiable because there is a sharp edge at $\beta = 0$, but we can work with subgradients for $\beta \neq 0$:
        $\frac{\partial}{\partial \beta} | \beta | = sgn(\beta) = 
        \left\{
            \begin{aligned}
                 & -1 \quad & \beta < 0 \\
                 & 0 \quad & \beta = 0 \\
                 & 1 \quad & \beta > 0   
            \end{aligned}
        \right.$
        \item If we have $-\lambda < \frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} < \lambda$ the optimum is given by $\beta = 0$
        \item This means that some parameters are set to $0$ 
        \item The larger $\lambda$, the more parameters are set to $0$
        \item Small parameter values (i.e. unimportant features) are more likely to be set to $0$
        \item For parameters that are not set to $0$, LASSO regression has a similar effect as ridge regression and shrinks these parameters towards $0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex, but not strictly convex
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Cannot be solved analytically, since $|\boldsymbol{\beta}|$ is not differentiable at $\beta_i = 0$
\end{itemize}