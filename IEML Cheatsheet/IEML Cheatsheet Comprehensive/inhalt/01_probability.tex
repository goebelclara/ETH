\section{Probability and Statistics}
\subsection*{Terminology}
\emph{Notation} --- 
\begin{itemize}
    \item $A \cap B$ is the intersection of $A$ and $B$, i.e. $A$ and $B$
    \item $A \cup B$ is the union of $A$ and $B$, i.e. $A$ or (inclusive) $B$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Kolmogorov axioms} --- 
\emph{Probability space} defined by:
\begin{itemize}
    \item \emph{Sample space}: All possible outcomes $\Omega = \{\omega_1,...,\omega_n\}$, e.g. for a dice toss $\{ 1,2,3,4,5,6 \}$
    \item \emph{Event space}: 
    \begin{itemize}
        \item All possible results
        \item Corresponds to the \emph{powerset} of the sample space:
        \begin{itemize}
            \item Powerset includes the empty set, single-item sets, ..., full-item sets
            \item E.g. powerset of $\{ 1,2,3,4,5,6 \}$ is $\{ \{\}, \{1\}, \{2\}, ..., \{1,2\}\ ,  \{2,3\}\, ...,  \{1,2,3,4,5,6\}\}$ where e.g. event $\{1,2\}$ refers to the event of rolling a $1$ or $2$
            \item Powerset has size $2^{|\Omega|}$
        \end{itemize}
        \item An \emph{event} is a subset of the sample space
        \item E.g. tossing an even number $\{ 2,4,6 \}$ or $P(X \leq r)$
    \end{itemize}
    \item \emph{Probability measure}: Function that assigns a probability to an event, e.g. $p(\textrm{tossing an even number}) = \frac{3}{6} = \frac{1}{2}$ 
\end{itemize}
Axioms:
\begin{itemize}
    \item Event space must be a \emph{sigma algebra}:
    \begin{itemize}
        \item $\Omega \in $ event space
        \item If $A$ is in event space with $P = a$, its complement is also in event space with $P = 1-a$
        \item If $A_1,...A_n$ are in event space with $P=a_1, ..., a_n$, their union is also in event space with $P = a_1 + ... + a_n$
    \end{itemize}
    \item Probability measure must satisfy:
    \begin{itemize}
        \item $0 \leq P(A) \leq 1$
        \item $P({\Omega}) = 1$
        \item If $A_1,A_2,...$ are in event space and do not intersect, then $P(A_1 \cup A_2 \cup ...) = \int_{n=1}^{\infty} P(A_n)$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Variables} --- 
\begin{itemize}
    \item \emph{Target space}: Numeric values that the random variable can take, e.g. for a dice toss $\{ 1,2,3,4,5,6\}$
    \item \emph{Random variable}:
    \begin{itemize}
        \item Function that takes an element in sample space and returns a numeric value, e.g. $\mathcal{X}(3) = 3$
        \item \emph{Discrete random variable}: Characterized by pmf, event given by $\{ \omega \in \Omega |X(\omega) = s\}$
        \item \emph{Continuous random variable}: Characterized by pdf, event given by $\{ \omega \in \Omega |X(\omega) \leq r\}$ (or $>, =, \geq, <$, any unions and intersections)
        \item In general, if case is mixed (e.g. $X$ is discrete, $Y$ is continuous), then the joint probability is defined by the continuous terminology, the marginal probability defined by the terminology of the respective variable, and the conditional probability defined by the terminology of the respective dependent variable
    \end{itemize}
    \item \emph{Independent random variables}:
    \begin{itemize}
        \item $P(A|B) = P(A)$ and $P(B|A) = P(B)$ 
        \item $P(A \cap B) = P(A)P(B)$ resp. $F_{X_1, ..., X_n}(r_1, ..., r_n) = F_{X_1}(r_1), ..., F_{X_n}(r_n)$ and $f_{X_1, ..., X_n}(x_1, ..., x_n) = f_{X_1}(x_1), ..., f_{X_n}(x_n)$\\
        \hl{Proof}:
        \begin{itemize}
            \item Direction 1: $f_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \frac{\partial^n}{\partial x_1 \partial x_2 \ldots \partial x_n} F_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n)$\\
            $= \frac{\partial^n}{\partial x_1 \partial x_2 \ldots \partial x_n} F_{X_1}(x_1) F_{X_2}(x_2) \ldots F_{X_n}(x_n)$\\
            $= \frac{\partial}{\partial x_1} F_{X_1}(x_1) \frac{\partial}{\partial x_2} F_{X_2}(x_2) \ldots \frac{\partial}{\partial x_n} F_{X_n}(x_n)$\\
            $= f_{X_1}(x_1) f_{X_2}(x_2) \ldots f_{X_n}(x_n)$
            \item Direction 2: $F_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) = \int_{-\infty}^{x_1} \int_{-\infty}^{x_2} \ldots \int_{-\infty}^{x_n} f_{X_1, X_2, \ldots, X_n}(\tilde{x}_1, \tilde{x}_2, \ldots, \tilde{x}_n) \, d\tilde{x}_1 \, d\tilde{x}_2 \ldots d\tilde{x}_n$\\
            $= \int_{-\infty}^{x_1} \int_{-\infty}^{x_2} \ldots \int_{-\infty}^{x_n} f_{X_1}(\tilde{x}_1) f_{X_2}(\tilde{x}_2) \ldots f_{X_n}(\tilde{x}_n) \, d\tilde{x}_1 \, d\tilde{x}_2 \ldots d\tilde{x}_n$\\
            $= \int_{-\infty}^{x_1} f_{X_1}(\tilde{x}_1) \, d\tilde{x}_1 \int_{-\infty}^{x_2} f_{X_2}(\tilde{x}_2) \, d\tilde{x}_2 \ldots \int_{-\infty}^{x_n} f_{X_n}(\tilde{x}_n) \, d\tilde{x}_n$\\
            $= F_{X_1}(x_1) F_{X_2}(x_2) \ldots F_{X_n}(x_n)$
        \end{itemize}
        \item Unnormalized correlation: $\mathbb{E}(\mathcal{X}\mathcal{Y})=\mathbb{E}(\mathcal{X})\mathbb{E}(\mathcal{Y})$
        \item Covariance: $\textrm{Cov}(\mathcal{X},\mathcal{Y})=0$
        \item Functions of independent random variables are also independent
        \item A subset of a larger set of independent random variables is also independent\\
        \hl{Proof for discrete case}:
        \begin{itemize}
            \item Assume $X_1, ..., X_n$ are independent
            \item We aim to show that $X_1, ..., X_{n-1}$ are also independent
            \item $P(X_1, ..., X_{n-1}) = \sum_n P(X_1, ..., X_n) = \sum_n P(X_1)...P(X_n) = P(X_1)...P(X_{n-1}) \sum_n P(X_n) = P(X_1)...P(X_{n-1}) \times 1$
        \end{itemize}
        \hl{Proof for continuous case}:
        \begin{itemize}
            \item Assume $X_1, ..., X_n$ are independent
            \item We aim to show that $X_1, ..., X_{n-1}$ are also independent
            \item $F_{X_1, ..., X_{n-1}}(X_1, ..., X_{n-1}) = \lim_{X_n \to \infty} F_{X_1, ..., X_{n-1}}(X_1, ..., X_n) = \lim_{X_n \to \infty} F_{X_1}(X_1) ... F_{X_n}(X_n) = F_{X_1}(X_1) ... F_{X_{n-1}}(X_{n-1}) \lim_{X_n \to \infty} F_{X_n}(X_n) = F_{X_1}(X_1) ... F_{X_{n-1}}(X_{n-1}) \times 1$
        \end{itemize}
    \end{itemize}
    \item \emph{Conditionally independent random variables}: $2$ random variables $\mathcal{X}$ and $\mathcal{Y}$ are conditionally independent, if there is a confounder $\mathcal{L}$ that causally affects both variables, but if we control for this confounder, the variables are not causally connected
    \item \emph{I.I.D. random variables}: Independent and from identical distribution
    \item \emph{Orthogonal random variables}: 
    \begin{itemize}
        \item Unnormalized correlation: $\mathbb{E}(\mathcal{X}\mathcal{Y})=0$
        \item Covariance not particularly defined
        \item Not necessarily independent
    \end{itemize}
    \item \emph{Uncorrelated random variables}: 
    \begin{itemize}
        \item Unnormalized correlation: $\mathbb{E}(\mathcal{X}\mathcal{Y})=\mathbb{E}(\mathcal{X})\mathbb{E}(\mathcal{Y})$
        \item Covariance: $\textrm{Cov}(\mathcal{X},\mathcal{Y}) = 0$
        \item Not necessarily independent
    \end{itemize} 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Events} --- 
\begin{itemize}
    \item \emph{Complement}: $P(A^C) = 1 - P(A)$ and $P(A \cup A^C) = P(A)+P(A^C)$
    \item \emph{Disjoint / mutually exclusive vs. joint / mutually inclusive}
    \item Subset $A \subset B$ with $P(A) < P(B)$
    \item Valid events for continuous random variables: All sets than can be formed from left and right inclusive interval $[0,a]$ are events:
    \begin{itemize}
        \item $(a,1] = [0,a]^c \in$ event space, with $P = 1 - a$
        \item $(a,b] = ([0,a] \cup (b,1])^c = ([0,a] \cup [0,b]^c)^c \in$ event space, with $P = 1 - (a + 1 - b) = 1 - a - 1 + b = b - a$
        \item $\{0\} \in$ event space, with $P = 0$
        \item $\{a\} \in$ event space, with $P = 0$
        \item $[a,b] = \{a\} \cup (a,b] = \{a\} \cup ([0,a] \cup [0,b]^c)^c \in$ event space, with $P = 0 + b - a$
        \item $[a,b) = [a,b] \backslash \{b\} = ([0,a] \cup [0,b]^c)^c \backslash \{b\} \in$ event space, with $P = b - a - 0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{PMF, CDF, PDF} --- 
\begin{itemize}
    \item \emph{Cumulative density function (CDF)}: $F(r) = p(X \leq r)$
    \item \emph{Probability mass function (PMF)} for discrete random variables: $p(x) = p(X = x)$
    \item \emph{Probability density function (PDF)} for continuous random variables: $f(x)$
    \item Properties of CDF and PDF:
    \begin{itemize}
        \item Derivative of CDF by $x$ returns PDF: $f(x) = \frac{\partial F(x)}{\partial x}$
        \item Integral of PDF by $x$ returns CDF: $\int_{-\infty}^r f(x)dx = F(r) = p(X \leq r)$
        \item CDF is monotonically non-decreasing: If $s<r, F(s) < F(r)$
        \item CDF is between 0 and 1: $lim_{r\rightarrow-\infty} F(r) = 0$ and $lim_{r\rightarrow\infty} F(r) = 1$
        \item CDF is right-continuous: $lim_{s\rightarrow-r^+} F(s) = F(r)$
        \item For CDF: $lim_{s\rightarrow-r^-} F(s) = F(x < r) = F(s) - F(x = r)$
        \item $\int_a^b f(x)dx = F(b)-F(a) = p(a < X \leq b)$
        \item $\int_{-\infty}^\infty f(x)dx = 1$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Probabilities} --- 
\begin{itemize}
    \item Probability for single variable:\\
    \emph{Marginal and total probability}:
    \begin{itemize}
        \item If $X$, $Y$ are discrete: $p(x) = \sum_{\mathcal{Y}}p(x,y) = \sum_{\mathcal{Y}} p(x|y)p(y)$
        \item If $X$, $Y$ are continuous: $f(x) =\int_{-\infty}^{\infty} f(x,y)dy = \int_{-\infty}^{\infty} f(y)f(x|y)dy$ and $F(r) = \int_{-\infty}^r \int_{-\infty}^{\infty} f(x,y)dy dx$
        \item If $X$ is discrete, $Y$ is continuous: $p(x) = \int_{-\infty}^{\infty} f(x,y)dy = \int_{-\infty}^{\infty} p(x|y) f(y) dy$
        \item If $X$ is continuous, $Y$ is discrete: $f(x) = \sum_{\mathcal{Y}} f(x,y) = \sum_{\mathcal{Y}} f(x|y) p(y)$ and $ F(r) = \sum_{\mathcal{Y}} p(y) p(X \leq r | y) = \sum_{\mathcal{Y}} p(y) F(r|y)$
    \end{itemize} 
    \item \emph{Joint probability} $P(A, B)$: Probability for combination of variables 
    \begin{itemize}
        \item If $X$ is discrete: $p(x_1,...,x_n)$
        \item If $X$ is continuous: $f(x_1, ...,x_n) = \frac{\partial^n F_{X_1, ..., X_n}(x_1, ..., x_n)}{\partial x_1, ..., \partial x_n}$ and $F(r_1, ..., r_n) = p(x_1 \leq r_1,...,x_n \leq r_n) = \int_{-\infty}^{r_1} ...  \int_{-\infty}^{r_n} f_{X_1, ..., X_n}(x_1, ..., x_n) dx_n ... dx_1$
    \end{itemize}
    \item \emph{Conditional probability} $P(A|B) = \frac{P(A \cap B)}{P(B)}$: Probability for variable, given other variable
    \begin{itemize}
        \item If $X$, $Y$ are discrete: $p(x|y) = \frac{p(x,y)}{p(y)}$
        \item If $X$, $Y$ are continuous: $f(x|y) = \frac{f(x,y)}{f(y)}$
        \item If $X$ is discrete, $Y$ is continuous: $p(x|y) = \frac{f(x,y)}{f(y)}$
        \item If $X$ is continuous, $Y$ is discrete: $f(x|y) = \frac{f(x,y)}{p(y)}$
        \item Properties:
        \begin{itemize}
            \item $P(A|B) = 1 - P(A^C|B)$
            \item $P(A_1|B) + P(A_2|B) + ... = 1$
            \item If conditioning on subset $S$:
            $p(x | S) = 
            \left\{
                \begin{aligned}
                     & p(x) / p(x \in S) \quad & x \in S \\ 
                     & 0 \quad & x \notin S  
                \end{aligned}
            \right.$
        \end{itemize}
    \end{itemize}
    \item Bayesian terminology: 
    \begin{itemize}
        \item \emph{Prior} $P(\textrm{parameter})$
        \item \emph{Posterior} $P(\textrm{parameter} | \textrm{data})$
        \item \emph{Likelihood} $P(\textrm{data} | \textrm{parameter})$
        \item \emph{Evidence} $P(\textrm{data})$
    \end{itemize}
    \item \emph{Bayes theorem}: $\textrm{Posterior } P(A|B) = \frac{\textrm{Likelihood }P(B|A) \times  \textrm{Prior }P(A)}{\textrm{Evidence }P(B)}$ where $P(B)$ can be rewritten in marginalized form over $A$
    \item Attention! In $p(\cdot | \theta)$ the $|$ can either refer to parametrizing on $\theta$ (parameter is part of the distribution's form but isn't observed or fixed) or conditioning on $\theta$ (parameter takes a observed and fixed value, and we evaluate the distribution on this condition)
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Measures}
\emph{$n^{th}$ moment} --- 
$\mathbb{E}(\mathcal{X}^n) = \int_{-\infty}^{\infty}x^n f(x)dx$

{\color{lightgray}\hrule height 0.001mm}

\emph{Expected value} --- 
Generally:
\begin{itemize}
    \item If $X$ is discrete: $\mathbb{E}(\mathcal{X}) = \sum_{\mathcal{X}}x \times p(x)$
    \item If $X$ is continuous: $\mathbb{E}(\mathcal{X}) = \int_{-\infty}^{\infty}x \times f(x)dx$
    \item If $Y$ is discrete: $\mathbb{E}[X] = \sum_{\mathcal{Y}} \mathbb{E}[X | Y = y] p(y) $
    \item If $Y$ is continuous: $\mathbb{E}[X] = \int_{-\infty}^\infty \mathbb{E}[X | Y = y] f(y) dy$
\end{itemize}
For functions:
\begin{itemize}
    \item $g(X)$ is a function 
    \item If $X$ is discrete: $\mathbb{E}(g(\mathcal{X})) = \sum_{\mathcal{X}} g(x) \times p(x)$  
    \item If $X$ is continuous: $\mathbb{E}(g(\mathcal{X})) = \int_{-\infty}^{\infty} g(x) \times f(x)dx$ 
\end{itemize}
For probabilities:
\begin{itemize}
    \item Count as functions 
    \item $A$ is an event, $X$ is a random variable
    \item If $X$ is discrete: $\mathbb{E}[p(X | A)] = \sum_x p(x | A) p(x)$ 
    \item If $X$ is continuous: $\mathbb{E}[p(X | A)] = \int_{-\infty}^{\infty} f(x|A) f(x) dx$ 
    \item If $X$ is discrete: $\mathbb{E}[p(A | X)] = \sum_x p(A | x) p(x) = p(A) $ 
    \item If $X$ is continuous: $\mathbb{E}[p(A | X)] = \int_{-\infty}^{\infty} p(A|x) f(x) dx = p(A)$ 
\end{itemize}
For conditions:
\begin{itemize}
    \item $A$ is an event, $X$ is a random variable
    \item If $X$ is discrete: $\mathbb{E}(X | A) = \sum_x x \times p(x | A) $ resp. 
    \item If $X$ is continuous: $\mathbb{E}(X | A) = \int_{-\infty}^{\infty} x \times f(x|A)dx$
    \item $\mathbb{E}(A | X) = P(A|X)$
\end{itemize}
For vectors:
\begin{itemize}
    \item Expectation of a vector is the expectation of each of its elements
    \item If $X$ is discrete: $\mathbb{E}(\boldsymbol{x}) = \sum_{x_1} ... \sum_{x_n} \boldsymbol{x}^\intercal p(x_1,...,x_n) = \boldsymbol{\mu}$
    \item If $X$ is continuous: $\mathbb{E}(\boldsymbol{x}) = \int_{-\infty}^\infty ... \int_{-\infty}^\infty \boldsymbol{x}^\intercal f_{X_1, ..., X_n}(x_1, ..., x_n) d x_1 ... d x_n = \boldsymbol{\mu}$
\end{itemize}
Properties:
\begin{multicols}{2}
\begin{itemize}
    \item $\mathbb{E}(\alpha)=\alpha$
    \item $\mathbb{E}(\alpha\mathcal{X}+\beta)=\alpha\mathbb{E}(\mathcal{X})+\beta$
    \item $\mathbb{E}(\alpha\mathcal{X} + \beta\mathcal{Y})=\alpha\mathbb{E}(\mathcal{X})+\beta\mathbb{E}(\mathcal{Y})$
    \item For orthogonal variables:
    \begin{itemize}
        \item $\mathbb{E}(\mathcal{X}\mathcal{Y})=0$
        \item $\mathbb{E}((\mathcal{X}+\mathcal{Y})^2)=\mathbb{E}(\mathcal{X}^2) + \mathbb{E}(\mathcal{Y}^2)$
    \end{itemize}
\end{itemize}
\end{multicols}
\begin{itemize}
    \item For independent variables: 
    \begin{itemize}
        \item $\mathbb{E}(\mathcal{X}|\mathcal{Y}) = \mathbb{E}(\mathcal{X})$
        \item $\mathbb{E}(\mathcal{X}\mathcal{Y})=\mathbb{E}(\mathcal{X})\mathbb{E}(\mathcal{Y})$\\
        \hl{Proof}:
        \begin{itemize}
            \item $\mathbb{E}(\mathcal{X}\mathcal{Y}) = \int_{-\infty}^\infty \int_{-\infty}^\infty xy f(x,y) dxdy = \int_{-\infty}^\infty \int_{-\infty}^\infty xy f(x)f(y) dxdy = \int_{-\infty}^\infty xf(x) dy \int_{-\infty}^\infty y f(y) dy = \mathbb{E}(\mathcal{X})\mathbb{E}(\mathcal{Y})$
        \end{itemize}
    \end{itemize}
    \item For vectors: If $\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x}$: $\mathbb{E}(\boldsymbol{A}\boldsymbol{x})=\boldsymbol{A}\mathbb{E}(\boldsymbol{x})$\\
    \hl{Proof}:
    \begin{itemize}
        \item $\mathbb{E}(\boldsymbol{A}\boldsymbol{x})
        = \mathbb{E}[(\boldsymbol{A}^{(1)}\boldsymbol{x}, ..., \boldsymbol{A}^{(m)}\boldsymbol{x})^\intercal]
        $ where $\boldsymbol{A}^{(i)}$ is the $i^{th}$ row of $\boldsymbol{A}$
        \item $= (\mathbb{E}[\boldsymbol{A}^{(1)}\boldsymbol{x}], ..., \mathbb{E}[\boldsymbol{A}^{(m)}\boldsymbol{x}])^\intercal = (\boldsymbol{A}^{(1)}\mathbb{E}[\boldsymbol{x}], ..., \boldsymbol{A}^{(m)}\mathbb{E}[\boldsymbol{x}])^\intercal = \boldsymbol{A}\mathbb{E}(\boldsymbol{x})$
    \end{itemize}
    \item $\mathbb{E}[\mathbb{E}(X | A)] = \mathbb{E}(X)$\\
    \hl{Proof}:
    \begin{itemize}
        \item $\mathbb{E}[\mathbb{E}(X | A)] = \int_{-\infty}^{\infty} f_A(a) \mathbb{E}(X | A) da = \int_{-\infty}^{\infty} f_A(a) \int_{-\infty}^{\infty} x f_{X|A}(x|a) dx da = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x f_{X,A}(x,a) dx da = \int_{-\infty}^{\infty} x \int_{-\infty}^{\infty} f_{X,A}(x,a) da dx = \int_{-\infty}^{\infty} x f_{X}(x) dx = \mathbb{E}(X)$
    \end{itemize}
    \item \emph{Cauchy Schwarz inequality}: $\mathbb{E}(\mathcal{X},\mathcal{Y})^2 \leq \mathbb{E}(\mathcal{X}^2)\mathbb{E}(\mathcal{Y}^2)$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Median} --- Real number $M$ defined by $P(X<M) = P(X>M)$

{\color{lightgray}\hrule height 0.001mm}

\emph{Standard deviation} --- $\sqrt{\mathbb{V}(\mathcal{X})}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Covariance} ---
\begin{itemize}
    \item Univariate variance of a random variable: $\mathbb{V}(\mathcal{X}) = \mathbb{E}((\mathcal{X}-\mathbb{E}(\mathcal{X}))^2) = \mathbb{E}(\mathcal{X}^2) - \mathbb{E}(\mathcal{X})^2$ where $\mathbb{E}(\mathcal{X}^2)$ is the unnormalized correlation resp. inner product
    \item Univariate covariance of two random variables: $\textrm{Cov}(\mathcal{X}, \mathcal{Y}) = \mathbb{E}((\mathcal{X}-\mathbb{E}(\mathcal{X}))(\mathcal{Y}-\mathbb{E}(\mathcal{Y}))) = \mathbb{E}(\mathcal{X}\mathcal{Y}) - \mu_{\mathcal{X}} \mu_{\mathcal{Y}}$ where $\mathbb{E}(\mathcal{X}\mathcal{Y})$ is the unnormalized correlation resp. inner product
    \item Proof (schematically for variance): $\mathbb{V}(\mathcal{X}) = \mathbb{E}((\mathcal{X}-\mathbb{E}(\mathcal{X}))^2) = \mathbb{E}[ \mathcal{X}^2 - \mathcal{X}\mathbb{E}(\mathcal{X}) - \mathcal{X}\mathbb{E}(\mathcal{X}) + \mathbb{E}(\mathcal{X})^2 ] = \mathbb{E}[ \mathcal{X}^2] - \mathbb{E}[\mathcal{X}]\mathbb{E}(\mathcal{X}) - \mathbb{E}[\mathcal{X}]\mathbb{E}(\mathcal{X}) + \mathbb{E}(\mathcal{X})^2 = \mathbb{E}(\mathcal{X}^2) - \mathbb{E}(\mathcal{X})^2$ where $\mathbb{E}(\mathcal{X}^2)$ is the second moment
    \item Multivariate covariance matrix of a vector: 
    \begin{itemize}
        \item $\boldsymbol{\Sigma} = \textrm{Cov}(\boldsymbol{x}) = \mathbb{E}((\boldsymbol{x}-\mathbb{E}(\boldsymbol{x}))(\boldsymbol{x}-\mathbb{E}(\boldsymbol{x}))^\intercal) = \mathbb{E}(\boldsymbol{x}\boldsymbol{x}^\intercal) - \mathbb{E}(\boldsymbol{x})\mathbb{E}(\boldsymbol{x})^\intercal = \boldsymbol{R} - \boldsymbol{\mu}_X \boldsymbol{\mu}_X^\intercal 
        = \begin{bmatrix}
        \mathbb{E}(x_1^2) & ... & \mathbb{E}(x_1x_m) \\
        ... & ... & ... \\
        \mathbb{E}(x_mx_1) & ... & \mathbb{E}(x_m^2)
        \end{bmatrix} 
        - 
        \begin{bmatrix}
        \mathbb{E}(x_1)^2 & ... & \mathbb{E}(x_1)\mathbb{E}(x_m) \\
        ... & ... & ... \\
        \mathbb{E}(x_m)\mathbb{E}(x_1) & ... & \mathbb{E}(x_m)^2
        \end{bmatrix} 
        = \begin{bmatrix}
        \mathbb{V}(x_1) & ... & \textrm{Cov}(x_1,x_m) \\
        ... & ... & ... \\
        \textrm{Cov}(x_m,x_1) & ... & \mathbb{V}(x_m)
        \end{bmatrix}$ where $\boldsymbol{R} = \mathbb{E}(\boldsymbol{x}\boldsymbol{x}^\intercal)$ is the unnormalized correlation matrix
        \item $\boldsymbol{\Sigma}$ and $\boldsymbol{R}$ are symmetric and psd
    \end{itemize}
\end{itemize}
Properties - variance:
\begin{multicols}{2}
\begin{itemize}
    \item $\mathbb{V}(\alpha)=0$
    \item $\mathbb{V}(\alpha\mathcal{X}+\beta)=\alpha^2\mathbb{V}(\mathcal{X})$
    \item $\mathbb{V}(\mathcal{X} + \mathcal{Y})=\mathbb{V}(\mathcal{X})+2\textrm{Cov}(\mathcal{X},\mathcal{Y})+\mathbb{V}(\mathcal{Y})$
    \item For uncorrelated (and independent) variables: $\mathbb{V}(\mathcal{X} + \mathcal{Y})=\mathbb{V}(\mathcal{X})+\mathbb{V}(\mathcal{Y})$
    \item For independent variables: $\mathbb{V}(\mathcal{X}\mathcal{Y})=\mathbb{E}((\mathcal{X}\mathcal{Y})^2)\mathbb{E}(\mathcal{X}\mathcal{Y})^2$
    \item For vector $\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x}$:\\ $\mathbb{V}_y = \boldsymbol{A}\mathbb{V}_X\boldsymbol{A}^\intercal$
    \item For zero-mean variable: $\mathbb{V}(\mathcal{X})= \mathbb{E}(\mathcal{X}^2) - \mathbb{E}(\mathcal{X})^2 = \mathbb{E}(\mathcal{X}^2)$ since $\mathbb{E}(\mathcal{X}) = 0$
\end{itemize}
\end{multicols}
Properties - covariance:
\begin{itemize}
    \item $\textrm{Cov}(\mathcal{X},\mathcal{X}) = \mathbb{V}(\mathcal{X})$
    \item $\textrm{Cov}((\alpha \mathcal{X} + \beta \mathcal{Y}),\mathcal{Z}) = \alpha \textrm{Cov}(\mathcal{X},\mathcal{Z}) + \beta \textrm{Cov}(\mathcal{Y},\mathcal{Z})$
    \item If covariance of $2$ random variables is $0$ resp. $\mathbb{E}(\mathcal{X}\mathcal{Y})=\mathbb{E}(\mathcal{X})\mathbb{E}(\mathcal{Y})$, they are uncorrelated, but not necessarily independent
    \item If unnormalized correlation of $2$ random variables is $0$ resp. $\mathbb{E}(\mathcal{X}\mathcal{Y}) = 0$, they are orthogonal, but not necessarily independent 
    \item For vector $\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x}$:
        \begin{itemize}
            \item $\boldsymbol{\Sigma}_y = \boldsymbol{A}\boldsymbol{\Sigma}_X\boldsymbol{A}^\intercal$
            \item $\boldsymbol{R}_y = \boldsymbol{A}\boldsymbol{R}_X\boldsymbol{A}^\intercal$
        \end{itemize}
    \item For zero-mean variables: $\textrm{Cov}(\mathcal{X},\mathcal{Y})= \mathbb{E}(\mathcal{X}\mathcal{Y}) - \mu_{\mathcal{X}} \mu_{\mathcal{Y}} = \mathbb{E}(\mathcal{X},\mathcal{Y})$ since $\mu_{\mathcal{X}} = \mu_{\mathcal{Y}} = 0$
    \item \emph{Cauchy Schwarz inequality}: $\textrm{Cov}(\mathcal{X},\mathcal{Y})^2 \leq \mathbb{V}(\mathcal{X})\mathbb{V}(\mathcal{Y})$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Correlation} --- Normalized covariance
\begin{itemize}
    \item Univariate correlation of a random variable: $\textrm{Cor}(\mathcal{X}, \mathcal{Y}) = \frac{\textrm{Cov}(\mathcal{X}, \mathcal{Y})}{\sqrt{\mathbb{V}(\mathcal{X})} \sqrt{\mathbb{V}(\mathcal{Y})}}$ 
    \item Multivariate correlation matrix of a vector: 
    \begin{itemize}
        \item $\boldsymbol{P} = \textrm{Cor}(\boldsymbol{\mathcal{X}}) = \begin{bmatrix}
        1 & ... & \textrm{Cor}(\mathcal{X}_1,\mathcal{X}_m) \\
        ... & ... & ... \\
        \textrm{Cor}(\mathcal{X}_m,\mathcal{X}_1) & ... & 1
        \end{bmatrix}$
        \item $\boldsymbol{P}$ is symmetric and psd
        \item Correlation is bounded between 0 and 1, given Cauchy Schwarz Inequality
        \item If correlation of two random variables is 0, they are not necessarily independent
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Probability Distributions}
\emph{Normal distribution} --- 
$\mathcal{X} \sim \mathcal{N}(\mu, \sigma^2)$\\
For univariate, PDF: $\frac{1}{\sqrt{2\pi\sigma}} exp(\frac{-(x-\mu)^2}{2\sigma^2}) = \frac{1}{\sqrt{2\pi\sigma}} exp(-x^2 \frac{1}{2\sigma^2} +2x \frac{\mu}{2\sigma^2} - \frac{\mu^2}{2\sigma^2})$\\
For multivariate, PDF: $\frac{1}{{2\pi\sigma}^{n/2}} \frac{1}{|\boldsymbol{\Sigma}|^{1/2}} exp(-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}))$ where the term in the exponent is a quadratic form\\
Convolution: $\int \mathcal{N}(a;Bc,D) \times \mathcal{N}(c;e,F) dc = \int \mathcal{N}(a;Be,D + BFB^\intercal$

{\color{lightgray}\hrule height 0.001mm}

\emph{Standard normal distribution} --- Normal distribution, standardized via z-score $z = \frac{x-\mu}{\sigma}$, which results in $\mu = 0$ and $\sigma = 1$

{\color{lightgray}\hrule height 0.001mm}

\emph{Bernoulli distribution} --- trial with success (probability $p$) or failure (probability $1-p$)
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Bernoulli}(p)$
    \item PDF: $p(x) p^x (1-p)^x$
    \item Mean: $\mathbb{E}(x) = p$
    \item Variance: $\mathbb{V}(x) = p(1-p)$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Binomial distribution} --- $n$ independent Bernoulli trials with $k$ successes
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Bin}(n,p)$
    \item PDF: $\binom{n}{k} p^k (1-p)^{n-k}$
    \item Mean: $\mathbb{E}(x) = np$
    \item Variance: $\mathbb{V}(x) = np(1-p)$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Poisson distribution} ---
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Pois}(\lambda)$
    \item PDF: $e^{-\lambda} \frac{\lambda^x}{x!}$
    \item Mean: $\mathbb{E}(x) = \lambda$
    \item Variance: $\mathbb{V}(x) = \lambda$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Beta distribution} ---
\begin{itemize}
    \item $X$ takes values $\in [0,1]$
    \item Represents the probability of a Bernoulli process after observing $\alpha-1$ successes and $\beta-1$ failures
\end{itemize}
\begin{multicols}{2}
\begin{itemize}
    \item $\mathcal{X} \sim \textrm{Beta}(\alpha,\beta)$ where $\alpha,\beta > 0$
    \item PDF: $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)+\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}$
    where $\Gamma(\alpha) = \int_0^\infty u^{\alpha-1} e^{-u} du$
    \item Mean: $\mathbb{E}(x) = \frac{\alpha}{\alpha+\beta}$
    \item Variance: $\mathbb{V}(x) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Dirichlet distribution} ---
\begin{itemize}
    \item $X$ takes values $\in [0,1]$
    \item Multivariate extension of Beta distribution
    \item $Dir(\boldsymbol{x} | \boldsymbol{\alpha}) = \frac{1}{B(\boldsymbol{\alpha})} \prod_{k=1}^n u_k^{\alpha_k - 1}$,
    where $B(\boldsymbol{\alpha})$ is the multivariate generalization of the Beta function:
    $B(\boldsymbol{\alpha}) = \frac{\prod_{k=1}^n \Gamma(\alpha_k)}{\Gamma\left(\sum_{k=1}^n \alpha_k\right)}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Uniform distribution} ---
\begin{multicols}{2}
\begin{itemize}
    \item Assume $x$ is uniformly distributed between $[a,b]$
    \item PDF: $f(x) =$\\
        $\left\{
            \begin{aligned}
                 & \frac{1}{b-a} \quad & a \leq x \leq b \\
                 & 0 & \textrm{ otherwise}
            \end{aligned}
        \right.$
    \item CDF: $F(x) =$\\
        $\left\{
            \begin{aligned}
                 & 0 & x < a \\
                 & \frac{x-a}{b-a} & a \leq x \leq b \\
                 & 1 & x > b
            \end{aligned}
        \right.$
\end{itemize}
\end{multicols}

{\color{black}\hrule height 0.001mm}

\subsection*{Other Concepts}
\emph{Law of large numbers} ---
Sample mean of iid variables converges to population mean as $n \rightarrow \infty$
\begin{itemize}
    \item \emph{Weak law of large numbers}: $\lim_{n \to \infty} P\left( \left| m_X - \frac{1}{n} \sum_{k=1}^n X_k \right| < \varepsilon \right) = 1$
    \item \emph{Strong law of large numbers}: $\lim_{n \to \infty} \frac{1}{n} \sum_{k=1}^n X_k = m_X$ with probability $1$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Union bound} --- $P(\bigcup_i A_i) \leq \sum_i P(A_i)$

{\color{lightgray}\hrule height 0.001mm}

\emph{Jensen's inequality} --- Relates expected value of a convex function of a random variable to the convex function of the expected value of that random variable\\
$\mathbb{E}(f(\mathcal{X})) \geq f(\mathbb{E}(\mathcal{X}))$

{\color{lightgray}\hrule height 0.001mm}

\emph{Markov's inequality} --- $p(x \geq t) \leq \frac{\mathbb{E}(x)}{t}$\\
Interesting only for $t \geq \mathbb{E}(x)$ because $p(x \geq t)$ must then be less than or equal to 1\\
Generalizations:
\begin{itemize}
    \item $p(|x| \geq t) \leq \frac{\mathbb{E}(g(|x|))}{g(t)}$
    \item $p(|x| \geq t) \leq \frac{\mathbb{E}(|x|^n)}{t^n}$
\end{itemize}
\hl{Proof}:
\begin{itemize}
    \item $ \mathbb{E}[g(|X|)] = \int_{-\infty}^\infty g(|X|) f_X(x) dx $
    \item $ \mathbb{E}[g(|X|)] = \int_{|X| < t} g(|X|) f_X(x) dx + \int_{|X| \geq t} g(|X|) f_X(x) dx $
    \item $ \mathbb{E}[g(|X|)] \geq \int_{|X| \geq t} g(|X|) f_X(x) dx $
    \item Since $g$ is monotonically increasing, $g(|X|) \geq g(t)$ for $|X| \geq t$. Then:
    $ \int_{|X| \geq t} g(|X|) f_X(x) dx \geq \int_{|X| \geq t} g(t) f_X(x) dx$
    \item $ \int_{|X| \geq t} g(t) f_X(x) dx = g(t) \int_{|X| \geq t} f_X(x) dx = g(t) P(|X| \geq t)$
    \item Then: $\mathbb{E}[g(|X|)] \geq g(t) P(|X| \geq t)$ 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Hoeffding's Lemma} --- For random variable with $\mathbb{E}[x] = 0$, and $a \leq x \leq b$, and $s > 0$: $\mathbb{E}[\exp(sx)] = \exp(s^2(b-a)^2/8)$

{\color{lightgray}\hrule height 0.001mm}

\emph{Hoeffding's Inequality} --- For random variables $x_i$ that fall in the interval $[a_i,b_i]$ with probability $1$, and $S_n = \sum_{i=1}^n x_i$, and $t > 0$: \begin{itemize}
    \item $P\left(S_n - \mathbb{E}_X S_n \geq t\right) \leq \exp\left(-\frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)$
    \item $P\left(S_n - \mathbb{E}_X S_n \leq -t\right) \leq \exp\left(-\frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)$
\end{itemize}
Proof:
\begin{itemize}
    \item Consider the probability $P(S_n - \mathbb{E}[S_n] \geq t)$
    \item Using Markov's inequality:
    $
    P(S_n - \mathbb{E}[S_n] \geq t) = P\left(e^{s(S_n - \mathbb{E}[S_n])} \geq e^{st}\right) \leq \frac{\mathbb{E}[e^{s(S_n - \mathbb{E}[S_n])}]}{e^{st}}
    $
    \item Using independence of $X_1, \dots, X_n$:
    $
    \mathbb{E}[e^{s(S_n - \mathbb{E}[S_n])}] = \prod_{i=1}^n \mathbb{E}[e^{s(X_i - \mathbb{E}[X_i])}]
    $
    \item For each term, we use the fact that $X_i \in [a_i, b_i]$, and apply the lemma inequality:
    $
    \prod_{i=1}^n \mathbb{E}[e^{s(X_i - \mathbb{E}[X_i])}] \leq \prod_{i=1}^n\exp\left(\frac{s^2(b_i - a_i)^2}{8}\right)
    $
    \item Plugging this back in:
    $
    e^{-st} \times \mathbb{E}[e^{s(S_n - \mathbb{E}[S_n])}] \leq e^{-st} \times \prod_{i=1}^n \exp\left(\frac{s^2(b_i - a_i)^2}{8}\right) = e^{-st} \times \exp\left(\frac{s^2}{8} \sum_{i=1}^n (b_i - a_i)^2\right) = P(S_n - \mathbb{E}[S_n] \geq t) \leq \exp\left(-st + \frac{s^2}{8} \sum_{i=1}^n (b_i - a_i)^2\right)
    $
    \item If we set $s = \frac{4t}{\sum_{i=1}^n (b_i - a_i)^2}$ to minimize the bound, we get:
    $
    P(S_n - \mathbb{E}[S_n] \geq t) \leq \exp\left(-\frac{2t^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)
    $
    \item Similarly for $P(S_n - \mathbb{E}[S_n] \leq -t)$,
\end{itemize}
In the special case of normalized sums of iid variables, where $\tilde{S} = S_n / n$ and $t = n\epsilon$:
\begin{itemize}
    \item Delta given by:
    \begin{itemize}
        \item $
        P(\tilde{S}_n - \mathbb{E}_X \tilde{S}_n \geq \epsilon) \leq \exp\left(-\frac{2n\epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2 / n}\right)
        $
        \item As $n \to \infty$, this $P(\tilde{S}_n - \mathbb{E}_X \tilde{S}_n \geq \epsilon) \to 0$ 
    \end{itemize}
    \item Absolute deviation given by:
    \begin{itemize}
        \item $
        P(|\tilde{S}_n - \mathbb{E}_X \tilde{S}_n| \geq \epsilon) = P(\tilde{S}_n - \mathbb{E}_X \tilde{S}_n \geq \epsilon \lor \tilde{S}_n - \mathbb{E}_X \tilde{S}_n \leq -\epsilon)
        $
        \item By the union bound:
        $
        = P(\tilde{S}_n - \mathbb{E}_X \tilde{S}_n \geq \epsilon) + P(\tilde{S}_n - \mathbb{E}_X \tilde{S}_n \leq -\epsilon)
        $
        \item By Hoeffding's inequality:
        $
        \leq 2 \exp\left(-\frac{2n\epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2 / n}\right)
        $
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Chebychev's inequality} --- $p( |x - \mu_x| \geq \alpha | \sigma_x |) \leq \frac{1}{\alpha^2}$ resp. $p( |x - \mu_x| \geq \alpha) \leq \frac{| \sigma_x |}{\alpha^2}$\\
Interesting only for $\alpha > 1$\\
Implications:
\begin{itemize}
    \item For $n$ variables: $p( |S_n - \mu_x| \geq \epsilon) \leq \frac{\sigma_x^2}{n \epsilon^2}$ where $S_n = \frac{1}{n} \sum_{k=1}^n X_k$ is the sample mean
\end{itemize}
\hl{Proof}:
\begin{itemize}
    \item $
    P(|S_n - m_X| \geq \varepsilon) = P\left(\left| \frac{1}{n} \sum_{k=1}^n X_k - m_X \right| \geq \varepsilon\right)
    $
    \item Using Chebyshev's inequality, using the fact that $S_n$ has mean $m_X$ and variance $\text{Var}(S_n)$:
    $
    P(|S_n - m_X| \geq \varepsilon) \leq \frac{\text{Var}(S_n)}{\varepsilon^2}
    $
    \item
    $
    \text{Var}(S_n) = \text{Var}\left(\frac{1}{n} \sum_{k=1}^n X_k\right) = \frac{1}{n^2} n \text{Var}(X_k) = \frac{\sigma_X^2}{n}
    $
    \item Then, we get:
    $
    P(|S_n - m_X| \geq \varepsilon) \leq \frac{\sigma_X^2}{n \varepsilon^2}
    $
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Sufficient statistics} --- 
\begin{itemize}
    \item $Z=g(Y)$ is a sufficient statistic for estimating $X$ if $X$ can be estimated as well from $Z$ as from $Y$, i.e. condensing $Y$ to $Z$ does not entail any loss of information about $X$
    \item Conditioned on $Z$, $Y$ is independent of $X$: $p(Y|Z,X) = p(Y|Z)$
    \item For sufficient statistics, the MLE of $X$ from $Y$ is the same as the MLE of $X$ from $Z$: $argmag_x p(Y|X) \rho(y) = argmag_x p(Z|X) \rho(y)$
    \item $p(X|Z) = p(X|Y)$
    
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Hypothesis Testing}
\emph{Terminology} ---
\begin{itemize}
    \item \emph{Hypothesis}:
    \begin{itemize}
        \item $H_0$: Accepted null hypothesis, e.g. $p=p_0$, $p_1-p_2=p_{0,1}-p_{0,2}=0$
        \item $H_A$: Alternative hypothesis, e.g. $p \neq p_0$, $p_1-p_2 \neq p_{0,1}-p_{0,2} \neq 0$
    \end{itemize}
    \item Errors:
    \begin{itemize}
        \item \emph{True positive}: Chose $H_0$, and $H_0$ obtains
        \item \emph{False negative, type I error}: Chose $H_A$, but $H_0$ obtains
        \item \emph{True negative}: Chose $H_A$, and $H_A$ obtains
        \item \emph{False positive, type II error}: Chose $H_0$, but $H_A$ obtains
    \end{itemize}
    \item \emph{Significance level} $\alpha$: 
    \begin{itemize}
        \item $\alpha \geq p(\textrm{type I error}) = p(\bar{x} \geq c \mid H_0)$ with equality for continuous variables 
        \item If $\alpha$ is small, the probability that we are erroneously rejecting $H_0$ is very small
        \item Set by us, typically at $5\%$
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $\alpha = p(\bar{x} \geq c \mid H_0) = p(\sqrt{n}\bar{x} \geq \sqrt{n}c \mid H_0) = p(z_n \geq \sqrt{n}c \mid H_0) = 1-\Phi(\sqrt{n}c)$ 
        where 
        \begin{itemize}
            \item $\Phi$ is the CDF of the normal distribution 
            \item $z_n \mid H_0 = \frac{\bar{x}-0}{1/\sqrt{n}} = \sqrt{n}\bar{x}$
        \end{itemize}
    \end{itemize}
    \item \emph{Critical value} $z$: 
    \begin{itemize}
        \item For two-sided: $z_{\alpha/2}$, $z_{1 - \alpha/2}$
        \item For one-sided upper tail: $z_{1 - \alpha}$
        \item For one-sided lower tail: $z_{\alpha}$
        \item Associated z-score with $\alpha$
        \item Corresponds to critical value $c$ prior to z-score transformation
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $\alpha = 1-\Phi(\sqrt{n}c) \Rightarrow c = \frac{1}{\sqrt{n}} \Phi^{-1}(1 - \alpha)$ where $\Phi$ is the CDF of the normal distribution
    \end{itemize}
    \item \emph{P-value} $p$: 
    \begin{itemize}
        \item For two-sided: $p = P(|z| \geq z_n)$
        \item For one-sided upper tail: $p = P(z \geq z_n)$
        \item For one-sided lower tail: $p = P(z \leq z_n)$
        \item Probability, given $H_0$ that we observe a value as or more extreme as the observed value $z_n$ 
        \item Smallest significance level resp. largest confidence level, at which we can reject $H_0$ given the sample observed
        \item If p-value is less than significance level resp. if observed value is more extreme than critical value, reject $H_0$, because the probability that we are erroneously doing so is very small
    \end{itemize}
    \item \emph{Confidence level}: $1-\alpha$, probability, given $H_0$, that we retain $H_0$
    \item \emph{Beta}: $\beta = p(\textrm{type II error})$
    \item \emph{Power}: 
    \begin{itemize}
        \item $1-\beta = p(\textrm{type II error}) = p(\bar{x} \geq c \mid H_1)$
        \item Probability, given $H_A$, that we reject $H_0$
        \item If $\mathcal{X} \sim \mathcal{N}(\theta,1)$ and $H_0: \theta = 0$:
        $1-\beta = p(\bar{x} \geq c \mid H_1) = p(\sqrt{n}(\bar{x}-1) \geq \sqrt{n}(c-1) \mid H_1) = p(z_n \geq \sqrt{n}(c-1) \mid H_1) = p(z_n \geq \sqrt{n}(c-1) \mid H_0) = 1-\Phi(\sqrt{n}(c-1))$ where 
        \begin{itemize}
            \item $\Phi$ is the CDF of the normal distribution 
            \item $z_n \mid H_1 = \frac{\bar{x}-1}{1/\sqrt{n}} = \sqrt{n}(\bar{x}-1)$
            \item We can switch from $\mid H_1$ to $\mid H_2$ because the two distributions follow the same form, just shifted
        \end{itemize}
    \end{itemize}
    \item Test types:
    \begin{itemize}
        \item \emph{Two-sided}: $H_0: p = p_0, H_A: p \neq p_0$
        \item \emph{One-sided upper tail}: $H_0: p \leq p_0, H_A: p > p_0$
        \item \emph{One-sided lower tail}: $H_0: p \geq p_0, H_A: p < p_0$
    \end{itemize}
    \item Calculating \emph{test statistic}:
    \begin{itemize}
        \item $z_n \mid H_0 = \frac{\bar{x}-\mu_0}{\sigma/\sqrt{n}}$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Multiple comparisons problem} ---
Accumulation of false positive rate ($\alpha$) for $K$ tests, due to independence of tests:
$P(|\textrm{false rejections of }H_0| > 0) = 1-P(|\textrm{false rejections of }H_0| = 0) = 1-(1-\alpha)^K$

{\color{lightgray}\hrule height 0.001mm}

\emph{Corrections for multiple comparisons problem} ---
\emph{Bonferroni correction}: New significance level set to $\alpha* = \alpha / K$

{\color{lightgray}\hrule height 0.001mm}

\emph{Neyman Pearson test} ---
\begin{itemize}
    \item Maximizes power while controlling type I errors
    \item Sets $\alpha$ such that $\alpha \geq p(\textrm{type I error})$

    \item Then minimizes $p(\textrm{type II error})$
    \item This is achieved by a likelihood-ratio test with threshold $\theta$, such that $\alpha$ equals or is as close as possible to $ p(\textrm{type I error})$: 
    \begin{itemize}
        \item If $\Lambda(x) = \frac{p(x|p_0)}{p(x|p_A)} > \theta$, we reject $H_0$
        \item Then, we have $P(\Lambda(x) > \theta | H_0) = P(\frac{p(x|p_0)}{p(x|p_A)} > \theta | H_0 ) = P(\textrm{type I error}) = \alpha$
        \item The smaller $\alpha$, the larger $\theta$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Bayesian Hypothesis Testing} ---
\begin{itemize}
    \item If $\Lambda(x) = \frac{p(x|p_0)}{p(x|p_A)} > \theta$, we reject $H_0$
    \item $\theta = \frac{k(p_A,p_0)P(p_0)}{k(p_0,p_A)P(p_A)}$
    \item In this case, $\theta$ subsumes both the prior $p(x)$ and the costs $k(\hat{x},x)$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Confidence Sets}
Frequentist confidence sets:
\begin{itemize}
    \item If we have $p(y|x)$ and set a threshold $\theta$
    \item Go through each discrete $x$
    \item Construct a set $J$ of values of $y$, such that $\sum_y p(y | X = x) \geq \theta$
    \item Return the smallest possible set of $y$ values (starting with $y$ value contributing the most) such that $p(y \in J | X = x) \geq \theta$
\end{itemize}