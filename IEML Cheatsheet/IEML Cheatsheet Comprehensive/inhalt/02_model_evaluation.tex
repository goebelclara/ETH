\section{Model Evaluation}
\subsection*{Estimator Evaluation Criteria}
\emph{Criteria} --- 
\begin{itemize}
    \item Consistency: $\hat{\boldsymbol{\theta}} \rightarrow \boldsymbol{\theta}$ as $n \rightarrow \infty$
    \item Bias: $\mathbb{E}(\hat{\boldsymbol{\theta}}) - \boldsymbol{\theta}$
    \begin{itemize}
        \item Unbiased: $\mathbb{E}(\hat{\boldsymbol{\theta}}) = \boldsymbol{\theta}$
        \item Asymptotically unbiased: $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ] = 0$ as $n \rightarrow \infty$ 
        \item Asymptotically efficient: $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ] = I$ as $n \rightarrow \infty$  where $I$ is Fisher information 
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Bias Variance Tradeoff}
\begin{itemize}
    \item Mean squared error $\mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \boldsymbol{y})^2 ]$ can be decomposed into: $( \mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}) )^2 + \mathbb{V}(\hat{f}(\boldsymbol{X})) + \mathbb{E}[\epsilon^2] = \textrm{bias}^2 + \textrm{variance} + \textrm{irreducible error}$\\
    Proof:
    \begin{itemize}
        \item $\boldsymbol{y} = f(\boldsymbol{X}) + \epsilon$
        \item $\mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \boldsymbol{y})^2 ] = \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E}[\hat{f}(\boldsymbol{X})] + \mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}) + \epsilon)^2 ] = \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E}[\hat{f}(\boldsymbol{X})] )^2 ] + \mathbb{E}[(\mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}))^2] + \mathbb{E}[\epsilon^2] - 2 \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) ]$
        \item Third term $\mathbb{E}[\epsilon^2]$ is the variance of $\boldsymbol{y}$: $ = \mathbb{E}[\epsilon^2] - \mathbb{E}[\epsilon]^2 = \mathbb{V}(\boldsymbol{y}) = \sigma^2$
        \item Fourth term $2 \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) ]$ equals 0:
        \begin{itemize}
            \item $2 \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) ] = 2( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ]$ because $( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) $ is deterministic
            \item In last equation, second term equals 0, so whole equation is 0
        \end{itemize}
        \item Then, we are left with: $\textrm{variance} + \textrm{bias}^2 + \textrm{irreducible error}$
    \end{itemize}
    \item \emph{Bias}: Error generated by the fact that we approximate a complex relationship via a simpler model (small function class) with a certain presupposed parametric form
    \item \emph{Variance}: Error generated by the fact that we estimate the model parameters with a noisy training sample (small sample), rather than the population
    \item \emph{Irreducible error}: Error generated by measurement error and the fact that we estimate $\boldsymbol{y}$ as a function of $\boldsymbol{X}$, when it is a function of many other factors
    \item \emph{Bias variance tradeoff}: Bias and variance cannot be reduced simultaneously
    \begin{itemize}
        \item High variance associated with overfitting: Model corresponds too closely to particular training set resp. performs poorly on unseen data, but well on training set 
        \item High bias associated with underfitting: Model fails to capture underlying relationships resp. performs poorly on both training set and unseen data
    \end{itemize}    
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Approximating Generalisation Loss via Empirical Loss}
\emph{Via resampling methods} --- \\
\emph{Cross-validation}:
\begin{itemize}
    \item Partition data $\mathcal{Z}$ into $K$ equally sized disjoint subsets: $\mathcal{Z} = \mathcal{Z}_1 \cup \mathcal{Z}_2 \cup ... \cup \mathcal{Z}_K$
    \item Produce estimator $\hat{f}^{-v}$ from $\mathcal{Z} \backslash \mathcal{Z}_v$ for $v \leq K$
    \item Empirical loss given by: $\hat{\mathcal{R}}^{cv} = \frac{1}{n} \sum_{i\leq n} LO(y_i - \hat{f}^{-k(i)} (x_i))$ where $k(i)$ maps $i$ to partition $\mathcal{Z}_{k(i)}$ where $(x_i,y_i)$ belongs
\end{itemize}

Bootstrapping:
\begin{itemize}
    \item Draw $B$ samples with replacement of size $n$ from data $\mathcal{Z}$: $\mathcal{Z}^{*b}$
    \item Compute estimate $S( \mathcal{Z}^{*b} )$ for each bootstrap sample
    \item For each estimate $S( \mathcal{Z}^{*b} )$, we can give a mean and variance:
    \begin{itemize}
        \item $\bar{S} = \frac{1}{B} \sum_b S( \mathcal{Z}^{*b} )$
        \item $\sigma^2(S) = \frac{1}{B-1} \sum_b ( S( \mathcal{Z}^{*b} ) - \bar{S})^2$
    \end{itemize}
    \item Out-of-bag loss given by: $\hat{\mathcal{R}}^{bs} = \frac{1}{n} \sum_{i=1}^n \frac{1}{| C^{-i} |} \sum_{b \in C^{-i}} LO(y_i - \hat{f}^{*b} (x_i))$ where $C^{-i}$ contains all bootstrap indices $b$ so that $\mathcal{Z}^{*b}$ does not contain $(x_i,y_i)$
    \item Empirical loss given by: $\hat{\mathcal{R}}(\mathcal{A}) = \frac{1}{B} \sum_{b=1}^B \frac{1}{n} \sum_{i=1}^n LO(y_i - \hat{f}^{*b} (x_i))$ 
    \item Empirical loss of bootstrap uses training data to estimate $\hat{\mathcal{R}}$, i.e. it is generally too optimistic. We can correct this by combining the empirical and out-of-bag loss:
    \begin{itemize}
        \item Probability that $(x_i,y_i)$ is not in sample $\mathcal{Z}^{*b}$ of size $n$ is given by $(1-\frac{1}{n})^n = \frac{1}{e} \textrm{ as } n \rightarrow \infty \approx \frac{1}{3}$
        \item Probability that $(x_i,y_i)$ is in sample $\mathcal{Z}^{*b}$ of size $n$ is given by $1 - \frac{1}{e} \textrm{ as } n \rightarrow \infty \approx \frac{2}{3}$
        \item We then define: $\hat{\mathcal{R}}^{(0.632)} = 0.368 \hat{\mathcal{R}}(\mathcal{A}) + 0.632 \hat{\mathcal{R}}^{bs}$
    \end{itemize}
\end{itemize}