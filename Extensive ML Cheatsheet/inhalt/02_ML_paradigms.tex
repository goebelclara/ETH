\section{ML Paradigms}
\subsection*{Paradigms I}
\subsection*{Frequentism}
\emph{Description} --- 
\begin{itemize}
    \item Parametric approach
    \item $\boldsymbol{\theta}$ as fixed, unknown quantity, $\boldsymbol{X}$ as random, and known quantity
    \item Makes point estimate
    \item Focuses on maximizing likelihood $p(\boldsymbol{X}|\boldsymbol{\theta})$ to infer posterior $p(\boldsymbol{\theta}|\boldsymbol{X})$
    \item Only requires differentiation methods
    \item High variance, but low bias
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{MLE estimator}
\begin{itemize}
    \item Maximizes log-likelihood: $\hat{\boldsymbol{\theta}} = argmax_{\boldsymbol{\theta}}(L) = argmax_{\boldsymbol{\theta}}(\prod_{i=1}^n p(\boldsymbol{x_i} | \boldsymbol{\theta})) = argmax_{\boldsymbol{\theta}}(\sum_{i=1}^n log(p(\boldsymbol{x_i} | \boldsymbol{\theta})))$
    \item \emph{Score}: 
    \begin{itemize}
        \item The score is the derivative of the log-likelihood: $\Lambda = \frac{\partial}{\partial \boldsymbol{\theta}} log(p( \boldsymbol{x} | \boldsymbol{\theta})) = \frac{ \frac{\partial}{\partial \boldsymbol{\theta}} p( \boldsymbol{x} | \boldsymbol{\theta})}{ p( \boldsymbol{x} | \boldsymbol{\theta}) }$
        \item The expected score is given by: $\mathbb{E}(\Lambda) = \int p( \boldsymbol{x} | \boldsymbol{\theta}) \frac{ \frac{\partial}{\partial \boldsymbol{\theta}} p( \boldsymbol{x} | \boldsymbol{\theta})}{ p( \boldsymbol{x} | \boldsymbol{\theta}) } dx = \frac{\partial}{\partial \boldsymbol{\theta}} \int p( \boldsymbol{x} | \boldsymbol{\theta}) dx = \frac{\partial}{\partial \boldsymbol{\theta}} \times 1 = 0$
    \end{itemize}
    \item Advantages
    \begin{itemize}
        \item Consistent: $\hat{\boldsymbol{\theta}} \rightarrow \boldsymbol{\theta}$ as $n \rightarrow \infty$
        \item Asymptotically normal: $\frac{1}{\sqrt{n}} (\hat{\boldsymbol{\theta}} - \boldsymbol{\theta})$ coverges to $\mathcal{N}(0 , \boldsymbol{J}^{-1} (\boldsymbol{\theta}) \boldsymbol{I}(\boldsymbol{\theta}) \boldsymbol{J}^{-1} (\boldsymbol{\theta}) )$ where $\boldsymbol{J} = -\mathbb{E}[ \frac{ \partial^2 log( p(\boldsymbol{x} | \boldsymbol{\theta}) ) }{ \partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\intercal } ]$ and where $\boldsymbol{I}$ is the Fisher information
        \item Asymptotically efficient: $\hat{\boldsymbol{\theta}}$ minimizes $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ]$ as $n \rightarrow \infty$ 
        \begin{itemize}
            \item Not necessarily the best estimator, especially for small samples in a multivariate context
            \item (cf. Rao-Cramer bound)
        \end{itemize}
        \item Equivariant: If $\hat{\boldsymbol{\theta}}$ is MLE of $\boldsymbol{\theta}$, then $g(\hat{\boldsymbol{\theta}})$ is MLE of $g(\boldsymbol{\theta})$
    \end{itemize}
    \item Proofs of advantages
    \begin{itemize}
        \item Asymptotically normal: 
        \begin{itemize}
            \item We start with the score and set it to 0 for optimization with regard to $\boldsymbol{\theta}$: 
            $\Lambda = \frac{\partial}{\partial \boldsymbol{\theta}} log(p( \boldsymbol{x} | \boldsymbol{\theta})) = 0$
            \item With a Taylor expansion, we can show that $( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} ) \sqrt{n} = \frac{1}{\sqrt{n}} \Lambda [ -\frac{1}{n} \frac{\partial^2}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\intercal} \sum_{i=1}^n log (p( x_i | \boldsymbol{\theta})) ]^{-1}$ where $\Lambda$ is the score 
            \item We set $\boldsymbol{J} = [ -\frac{1}{n} \frac{\partial^2}{\partial \boldsymbol{\theta} \partial \boldsymbol{\theta}^\intercal} \sum_{i=1}^n log (p( x_i | \boldsymbol{\theta})) ]$
            \item $\frac{1}{\sqrt{n}} \Lambda$ is a random vector with covariance matrix $\boldsymbol{I}$ and converges to the normal distribution $\sim \mathcal{N}(0,\boldsymbol{I})$
            \item Then, $( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} ) \sqrt{n} = \boldsymbol{J}^{-1} \frac{1}{\sqrt{n}} \Lambda \sim \boldsymbol{J}^{-1}\mathcal{N}(0,\boldsymbol{I})$
            \item $\mathbb{V}(\boldsymbol{J}^{-1} \frac{1}{\sqrt{n}} \Lambda) = \mathbb{E}[\boldsymbol{J}^{-1}  \boldsymbol{I} \boldsymbol{J}^{-1}]$ 
            \item This equality is given because $\mathbb{V}(x) = \mathbb{E}[x-\mathbb{E}(x)] = \mathbb{E}[x]$ if $\mathbb{E}(x) = 0$, which is the case here, given that the expected score is $0$
            \item So we have shown that $\hat{\boldsymbol{\theta}} - \boldsymbol{\theta} ) \sqrt{n} = \boldsymbol{J}^{-1} \frac{1}{\sqrt{n}} \Lambda \sim \mathcal{N}(0,\boldsymbol{J}^{-1}\boldsymbol{I}\boldsymbol{J}^{-1})$
        \end{itemize}
        \item Equivariant: 
        \begin{itemize}
            \item Let $t = g(\boldsymbol{\theta})$ and $h = g^{-1}$ 
            \item Then, $\boldsymbol{\theta} = h(t) = h(g(\boldsymbol{\theta}))$
            \item For all $t$ we have: $L(t) = \prod_i p(\boldsymbol{x^({i})} | h(t)) = p(\boldsymbol{x^({i})} | \boldsymbol{\theta}) = L(\boldsymbol{\theta})$
            \item Hence, for all $t$ we can say: $L(t) = L(\boldsymbol{\theta})$ and $L(\hat{t}) = L(\hat{\boldsymbol{\theta})}$
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Probably Approximately Correct (PAC) estimator}
\begin{itemize}
    \item Generates probabilistic bounds for parameter $\boldsymbol{\theta}$ that is approximately known with a high probability:
    \begin{itemize}
        \item Probability of being correct: $1-\delta$
        \item Degree of approximation: $\epsilon$
    \end{itemize}
\end{itemize}
Setting:
\begin{itemize}
    \item \emph{Hypothesis class $\mathcal{H}$}: Set of functions that can be expressed by the algorithm
    \item \emph{Concept class $\mathcal{C}$}: Set of possible target functions that represent true mappings from input to output
    \item \emph{Specific concept $c$}:
    \begin{itemize}
        \item True function $c$ that maps inputs to outputs
        \item Estimated function $\hat{c}$
    \end{itemize}
    \item \emph{Learning algorithm $\mathcal{A}$}:
    \begin{itemize}
        \item Receives samples $\mathcal{Z} = \{(x_1, y_1), \ldots, (x_n, y_n)\}$ as inputs
        \item $c(x_i) = y_i$ for all $i$
        \item $\mathcal{A}$ outputs $\hat{c} \in \mathcal{H}$
    \end{itemize}
\end{itemize}
PAC learning model:
\begin{itemize}
    \item $\mathcal{A}$ can learn $c$ from $\mathcal{C}$ if, given a sufficiently large sample, it outputs $\hat{c}$ that generalizes well with high probability\\
    resp.\\
    if given $\mathcal{Z}$ of size $n > \textrm{poly}\big(1/\epsilon, 1/\delta, \textrm{size}(c)\big)$, it outputs $\hat{c}$ such that:
    $
    P(\mathcal{R}(\hat{c}) \leq \epsilon) \geq 1 - \delta
    $
    where:
    \begin{itemize}
        \item $\epsilon$: Error tolerance (how much $\hat{c}$ deviates from $c$), is between $0$ and $0.5$
        \item $\delta$: Confidence (how likely $\hat{c}$ generalizes well), is between $0$ and $0.5$
        \item $\textrm{size}(c)$: Complexity of the concept
    \end{itemize}
    \item A concept class $\mathcal{C}$ is \emph{PAC-learnable} from a hypothesis class $\mathcal{H}$ if there is an algorithm $\mathcal{A}$ that can learn any concept in $\mathcal{C}$
    \item If algorithm $\mathcal{A}$ runs in time polynomial in $1/\epsilon$ and $1/\delta$, then $\mathcal{C}$ is \emph{efficiently PAC-learnable}
    \item \emph{Strong PAC learning}: Demand arbitrarily small error $\epsilon$ with high probability $1 - \delta$
    \item \emph{Weak PAC learning}: Demand that risk is bounded for large (not trivial) error $\epsilon$, used frequently in ensemble learning
\end{itemize}
Scenario 1: If $\mathcal{C}$ is finite and $\mathcal{H} = \mathcal{C}$:\\
Deterministic scenario:
\begin{itemize}
    \item In this case, algorithm $\mathcal{A}$ returns a consistent hypothesis $\hat{c}$, i.e.
    $
    \mathcal{R}(\hat{c}) = 0
    $
    \item We aim to bound the risk of $\hat{c}$ as follows:
    $
    P(\mathcal{R}(\hat{c}) > \epsilon) \leq \delta
    $
    \item According to \emph{Hoeffding's inequality} for a single hypothesis:
    $
    P(|\mathcal{R}(c) - \mathcal{R}(\hat{c})| > \epsilon) = P(\mathcal{R}(c) > \epsilon) \leq \exp(-n \epsilon)
    $
    \item To account for all hypotheses in $\mathcal{C}$, we apply a \emph{union bound}:
    $
    P(\exists c \in \mathcal{C} : \mathcal{R}(c) > \epsilon) \leq |\mathcal{C}| \times \exp(-n \epsilon)
    $
    \item We then get:
    $
    |\mathcal{C}| \times \exp(-n \epsilon) \leq \delta
    $
    \item From this, we can derive:
    $
    \log(|\mathcal{C}|) - n \epsilon \leq \log(\delta)
    $\\
    $
    \log(|\mathcal{C}|) - \log(\delta) \leq n \epsilon
    $\\
    $
    \frac{1}{\epsilon} \big[\log(|\mathcal{C}|) + \log\left(\frac{1}{\delta}\right)\big] \leq n
    $
    \item Then, we have derived a minimum required sample size to bound the risk of $\hat{c}$ as follows:
    $
    P(\mathcal{R}(\hat{c}) > \epsilon) \leq \delta
    $
\end{itemize}
Stochastic scenario:
\begin{itemize}
    \item If the instance's true label is not deterministic, but modeled by a distribution $\mathcal{D}$
    \item \emph{Bayes optimal classifier:}
    \begin{itemize}
        \item Classifier that achieves minimum possible error
        \item Outputs labels based on true probabilities of labels given input features:
        $
        \hat{y} = argmax_y p(y \mid \boldsymbol{x})
        $
        \item If the Bayes optimal classifier is not in $\mathcal{C}$, there will always be a gap between the error of the best hypothesis $\hat{c}$ and Bayes optimal error
        \item We then aim to bound the risk of $\hat{c}$ as follows:
        $
        P\big(\mathcal{R}(\hat{c}) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) \leq \epsilon\big) \geq 1 - \delta
        $
    \end{itemize}
\end{itemize}
Scenario 2: If $\mathcal{C}$ is finite and $\mathcal{H} \neq \mathcal{C}$:
\begin{itemize}
    \item According to \emph{Hoeffding's inequality} for a single hypothesis:
    $
    P\big(\mathcal{R}(\hat{c}^*) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) > \epsilon\big) \leq 2 \exp(-n \epsilon^2)
    $
    \item To account for all hypotheses in $\mathcal{C}$, we apply a \emph{union bound}:
    $
    P\big(\exists c \in \mathcal{C}: \mathcal{R}(\hat{c}^*) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) > \epsilon\big) \leq 2 |\mathcal{C}| \exp(-n \epsilon^2)
    $
\end{itemize}
Scenario 3: If $\mathcal{C}$ has finite VC-dimension, but is infinite:
\begin{itemize}
    \item The \emph{VC-dimension} of a concept class $\mathcal{C}$ is a measure of its complexity: VC-dimension $V_c$ is the size of the largest set $A$ that can be shattered by $\mathcal{C}$
    \begin{itemize}
        \item A set of instances $A$ is \emph{shattered} by the concept class $\mathcal{C}$ if, for every subset $S \subseteq A$, there is a concept $c_S \in \mathcal{C}$ such that $S = c_S \cap A$
        \item This means the concept class can "realize" or "perfectly label" every possible labeling of $A$
        \item E.g. for $2^n$ points, there are $2^n$ possible ways to assign binary labels to the points. If $\mathcal{C}$ can express all $2^n$ labelings, it shatters $A$
    \end{itemize}
    \item If $V_c > 2$:
    $
    P\big(\mathcal{R}(\hat{c}^*_n) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) > \epsilon\big) \leq g  n^{V_c} \exp\left(-\frac{n \epsilon^2}{32}\right)
    $
    \item If $\mathcal{C}$ has a finite VC-dimension:
    $
    P\big(\mathcal{R}(\hat{c}^*_n) - \inf_{c \in \mathcal{C}} \mathcal{R}(c) > \epsilon\big) \leq g n^{V_c} \exp\left(-\frac{n \epsilon^2}{32}\right) \to 0$ as $n \to \infty$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Bayesianism}
\emph{Description} --- 
\begin{itemize}
    \item Parametric approach
    \item $\boldsymbol{\theta}$ as random, unknown quantity, $\boldsymbol{X}$ as random, and known quantity
    \item Makes estimate in form of distribution
    \item Leverages prior and likelihood to infer posterior: $p(\boldsymbol{\theta}|\boldsymbol{X}, \boldsymbol{y}) = \frac{p(\boldsymbol{\theta})p(\boldsymbol{y}|\boldsymbol{X}, \boldsymbol{\theta})}{p(\boldsymbol{y}|\boldsymbol{X})} = \frac{p(\boldsymbol{\theta})p(\boldsymbol{y}|\boldsymbol{X}, \boldsymbol{\theta})}{\int p(\boldsymbol{\theta}) p(\boldsymbol{y}|\boldsymbol{X}, \boldsymbol{\theta}) d\boldsymbol{\theta}} \propto p(\boldsymbol{\theta})p(\boldsymbol{y}|\boldsymbol{X}, \boldsymbol{\theta}) = p(\boldsymbol{\theta}, \boldsymbol{y} |\boldsymbol{X})$ 
    \item Focuses on minimizing cost function $\mathbb{E}[k(\boldsymbol{\theta}',\boldsymbol{\Theta}) | \boldsymbol{X}, \boldsymbol{y}] = \int_{\boldsymbol{\theta}} p(\boldsymbol{\theta} | \boldsymbol{X}, \boldsymbol{y}) \times k(\boldsymbol{\theta}',\boldsymbol{\theta}) d\boldsymbol{\theta} \propto \int_{\boldsymbol{\theta}} p(\boldsymbol{\theta}, \boldsymbol{y} | \boldsymbol{X}) \times k(\boldsymbol{\theta}',\boldsymbol{\theta}) d\boldsymbol{\theta}$ resp. $\sum p(\boldsymbol{\theta} | \boldsymbol{X}, \boldsymbol{y}) \times k(\boldsymbol{\theta}',\boldsymbol{\theta})$
    \item Requires integration methods for normalizing constant in denominator, which can be intractable, in which case MAP estimator can provide an alternative
    \item Low variance, but high bias 
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Mean estimator}
\begin{itemize}
    \item Minimizes mean squared error as cost function $k(\hat{\boldsymbol{\theta}},\boldsymbol{\theta}) = | \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} |^2$
    \item The resulting estimate is the mean of the posterior: $\hat{\boldsymbol{\theta}} = \mathbb{E}[ \boldsymbol{\theta} | \boldsymbol{X}, \boldsymbol{y} ]$
    \item Returns estimate that reflects central tendency and overall uncertainty
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{MAP estimator}
\begin{itemize}
    \item Generally, maximizes posterior: $\hat{\boldsymbol{\theta}} = argmax_{\boldsymbol{\theta}}( p(\boldsymbol{\theta}|\boldsymbol{X}) )$ resp. $p(\boldsymbol{\theta}|\boldsymbol{X}) p(\boldsymbol{X})$ 
    \item In discrete cases, MAP minimizes following cost function: 
    \begin{align*}
        k(\hat{\boldsymbol{\theta}},\boldsymbol{\theta}  = 
        \left\{
            \begin{aligned}
                 & 1 \quad & \hat{\boldsymbol{\theta}} \neq \boldsymbol{\theta} \\
                 & 0 \quad & \hat{\boldsymbol{\theta}} = \boldsymbol{\theta}
            \end{aligned}
        \right.
    \end{align*}
    \item Returns single point estimate
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Statistical Learning}
\emph{Description} --- 
\begin{itemize}
    \item We want to minimize expected risk $\mathcal{R}(f) = \mathbb{E}_{X,Y}[1{f(X) \neq Y}]$, but this is difficult because
    \begin{itemize}
        \item We don't have access to the joint distribution of $X,Y$
        \item We cannot find $f$, without any assumptions on its structure
        \item It's unclear how to minimize the expected value
    \end{itemize}
    \item Therefore, we make following choices:
    \begin{itemize}
        \item We collect sample $Z$
        \item We restrict space of possible choices of $f$ to a set $\mathcal{H}$
        \item We use a loss function to approximate the expected value
    \end{itemize}
    \item With these choices, we approximate the expected risk via the empirical risk $\hat{\mathcal{R}}(f) = \hat{L}(Z,f) = \frac{1}{n} \sum_i L(y_i, f(x_i))$
\end{itemize}

{\color{black}\hrule height 0.001mm}
_
{\color{black}\hrule height 0.001mm}

\subsection*{Paradigms II}
\subsection*{Probabilistic}
Probabilistic inference by learning about posterior
\emph{Generative} --- 
\begin{itemize}
    \item Learn about posterior indirectly via prior and likelihood
    \item Requires to learn more parameters vs. discriminative approach
    \item E.g. N-gram models, Markov random fields, RNNs
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Discriminative} --- 
\begin{itemize}
    \item Learn about posterior directly
    \item Requires to learn fewer parameters vs. generative approach
    \item E.g. conditional random fields, RNNs
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Distribution-free}
\emph{Learned} --- 
\begin{itemize}
    \item E.g. perceptron, SVM
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Manually crafted} --- 
\begin{itemize}
    \item E.g. conetxt free grammars, linear indexed grammars
\end{itemize}