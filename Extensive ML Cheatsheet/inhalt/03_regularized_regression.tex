\section{Ridge ($\ell_2$) Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$ subject to $\|\boldsymbol{\beta}\|^2 \leq t$ resp. $\|\boldsymbol{\beta}\|^2 - t \leq 0$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize mean squared error subject to constraint
    \item Lagrangian formulation: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2 + \lambda ( \|\boldsymbol{\beta}\|^2 - t )$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} ) + \lambda ( \|\boldsymbol{\beta}\|^2 - t )$
    \item Still a OLSE problem, since we can rewrite the objective to minimize $(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y})$ as the objective to minimize $\|(\boldsymbol{X}'\boldsymbol{\beta} - \boldsymbol{y}')\|^2$ with $\boldsymbol{X}' = 
    \begin{bmatrix} 
    \boldsymbol{X} \\
    \lambda \boldsymbol{I}
    \end{bmatrix}$ 
    and $\boldsymbol{y}' = 
    \begin{bmatrix} 
    \boldsymbol{y} \\
    0
    \end{bmatrix}$
    \item Corresponds to MAP estimation, when $\boldsymbol{X}$ is modeled as a vector of independent zero-mean Gaussian random variables
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\nabla_{\boldsymbol{\beta}} LO = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Effect} ---
\begin{itemize}
    \item Shrinks certain elements of $\boldsymbol{\beta}$ to near 0\\
    Proof:
    \begin{itemize}
        \item Gradient at optimality given by $\frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} + 2 \lambda \boldsymbol{\beta} = 0$
        \item Then, $\boldsymbol{\beta}^* = -\frac{1}{2 \lambda} \frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}}$
        \item This means that each parameter is shrunk by a factor determined by size of $\lambda$ - the larger $\lambda$, the more the parameters are shrunk
        \item Larger parameters experience a larger shrinkage
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Strictly with pd Hessian, since Lagrangian term is strictly convex and the sum of a strictly convex function with a convex function is strictly convex
    \item Has global minimum
    \item Has unique solution, as $(\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})$ has linearly independent columns
    \item Can be solved analytically, as $(\boldsymbol{X}^\intercal \boldsymbol{X} + \lambda \boldsymbol{I})$ is always invertible
\end{itemize}

\section{Lasso ($\ell_1$) Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$ subject to $|\boldsymbol{\beta}| \leq t$ resp. $|\boldsymbol{\beta}| - t \leq 0$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize mean squared error subject to constraint
    \item Lagrangian formulation: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2 + \lambda ( |\boldsymbol{\beta}| - t )$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} ) + \lambda ( |\boldsymbol{\beta}| - t )$
    \item Corresponds to MAP estimation, when $\boldsymbol{X}$ is modeled as a vector of independent zero-mean Laplacian random variables
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Effect} ---
\begin{itemize}
    \item Shrinks certain elements of $\boldsymbol{\beta}$ to 0\\
    Proof:
    \begin{itemize}
        \item Gradient at optimality given by $\frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} + \frac{\partial \lambda  |\boldsymbol{\beta}|}{\partial \boldsymbol{\beta}} = 0$
        \item $\frac{\partial \lambda  |\boldsymbol{\beta}|}{\partial \boldsymbol{\beta}}$ non-differentiable because there is a sharp edge at $\beta = 0$, but we can work with subgradients for $\beta \neq 0$:
        \begin{align*}
        \frac{\partial}{\partial \beta} | \beta | = sgn(\beta) = 
        \left\{
            \begin{aligned}
                 & -1 \quad & \beta < 0 \\
                 & 0 \quad & \beta = 0 \\
                 & 1 \quad & \beta > 0   
            \end{aligned}
        \right.
        \end{align*}
        \item If we have $-\lambda < \frac{\partial ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )}{\partial \boldsymbol{\beta}} < \lambda$ the optimum is given by $\beta = 0$
        \item This means that some parameters are set to $0$ - the larger $\lambda$, the more parameters are set to $0$
        \item Small parameter values (i.e. unimportant features) are more likely to be set to $0$
        \item For parameters that are not set to $0$, LASSO regression has a similar effect as ridge regression and shrinks these parameters towards $0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex, but not strictly convex
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Cannot be solved analytically, since $|\boldsymbol{\beta}|$ is not differentiable at $\beta_i = 0$
\end{itemize}