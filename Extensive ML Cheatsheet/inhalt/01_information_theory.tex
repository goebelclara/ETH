\section{Information Theory}
\subsection*{Description}
\emph{Entropy} --- 
\begin{itemize}
    \item $H(x) = -\sum_x p(x)log(p(x)) = -\sum_{x,y} p(x,y)log(p(x))$ resp. $H(x) = -\int p(x)log(p(x))dx$
    \item Measure of randomness in a variable resp. quantifies uncertainty of a distribution
\end{itemize}
Properties:
\begin{itemize}
    \item $H(x) \geq 0$
    \item $H(x)$ is maximized, when $x$ is a uniform random variable
    \item For independent variables: $H(x,y) = H(x) + H(y)$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Conditional entropy} --- 
\begin{itemize}
    \item $H(x|y) = -\sum_{x,y} p(y)p(x|y)log(p(x|y)) = -\sum_{x,y} p(x,y)log(\frac{p(x,y)}{p(y)})$ 
    \item Measure of how much information of $x$ is revealed by $y$
\end{itemize}
Properties:
\begin{itemize}
    \item $0 \leq H(x|y) \leq H(x)$ with equality if when $x$ is independent with $y$ resp. if $y$ completely determines $x$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Mutual information} --- 
\begin{itemize}
    \item $I(x;y) = H(x) - H(x|y) = -\sum_{x,y} p(x,y) log(\frac{p(x)p(y)}{p(x,y)})$ 
    \item Measure of how much information of $x$ is left after $y$ is revealed
\end{itemize}
Properties:
\begin{itemize}
    \item $0 \leq I(x;y) \leq H(x)$ with equality if $y$ completely determines $x$ resp. if $x$ is independent with $y$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{KL divergence} --- 
\begin{itemize}
    \item $KL(p;q) = \sum_x p(x) log(\frac{p(x)}{q(x)})$ 
    \item Measures the extra information or inefficiency when approximating a true distribution over $x$, $p$, with a predicted one, $q$
\end{itemize}
Properties:
\begin{itemize}
    \item $KL(p;q) \geq 0$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Cross entropy} --- 
\begin{itemize}
    \item $CE(p|q) = KL(p;q) + H(p) = -\sum_x p(x)log(q(x))$ 
    \item Measures the total uncertainty when using the predicted distribution $q$ to represent the true distribution $p$, combining both the model's error and the intrinsic uncertainty of the true distribution 
\end{itemize}
Properties:
\begin{itemize}
    \item $KL(p;q) \geq 0$
\end{itemize}