\section{Decision Tree for Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Non-parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\begin{itemize}
    \item Predictor space is split along $J-1$ internal nodes
    \item Thus, we get $J$ distinct regions resp. terminal nodes $R_1, ..., R_J$
\end{itemize}

{\color{black}\hrule height 0.001mm}
    
\subsection*{Algorithm}

\begin{itemize}
    \item Splitting approach: Recursive binary splitting (CART):
    \begin{itemize}
        \item Top-down: Successively splits the predictor space
        \item Greedy: At each step, the best split is made
        \item Steps:
        \begin{enumerate}
            \item Select predictor $x_j$ and threshold $s$ such that splitting the predictor space into the regions $\{x_j \leq s\}$ and $\{x_j > s\}$ leads to the smallest MSE
            \item Repeat the process for one of the previously identified regions
            \item The process ends when the maximum depth or minimum number of observations is reached
        \end{enumerate}
    \end{itemize}
    \item Prediction approach:
    \begin{itemize}
        \item For each observation falling into $R_j$, the prediction is the mean or median of the response values for the training observations in $R_j$, denoted as $\hat{y}_{R_j}$
        \item At each leaf node, $2$ numbers are stored: The sum of outputs and the number of samples
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find regions $R_1, ..., R_J$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize $\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}R_j)^2$
\end{itemize}

\section{Decision Tree for Classification}
\subsection*{Description}
\emph{Task} --- Classification

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Non-parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\begin{itemize}
    \item Predictor space is split along $J-1$ internal nodes
    \item Thus, we get $J$ distinct regions resp. terminal nodes $R_1, ..., R_J$
\end{itemize}

{\color{black}\hrule height 0.001mm}
    
\subsection*{Algorithm}

\begin{itemize}
    \item Splitting approach: Recursive binary splitting (CART):
    \begin{itemize}
        \item Top-down: Successively splits the predictor space
        \item Greedy: At each step, the best split is made
        \item Steps:
        \begin{enumerate}
            \item Select predictor $x_j$ and threshold $s$ such that splitting the predictor space into the regions $\{x_j \leq s\}$ and $\{x_j > s\}$ leads to the smallest Gini-index or cross-entropy
            \item Repeat the process for one of the previously identified regions
            \item The process ends when the maximum depth or minimum number of observations is reached
        \end{enumerate}
    \end{itemize}
    \item Prediction approach:
    \begin{itemize}
        \item For each observation falling into $R_j$, the prediction is the most common response class for the training observations in $R_j$, denoted as $\hat{y}_{R_j}$
        \item At each leaf node, $k$ numbers are stored: Number of samples in each of the $k$ classes
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find regions $R_1, ..., R_J$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Gini-index:
    \begin{itemize}
        \item $G_j = \sum_{k=1}^K \hat{p}_{jk} (1 - \hat{p}_{jk})$
        where $\hat{p}_{jk}$ is the proportion of training observations in $R_j$ that are from class $k$
        \item $G_j$ takes a small value when nodes are pure, i.e., $\hat{p}_{jk}$ is $0$ or $1$
    \end{itemize}
    \item Cross-entropy:
    \begin{itemize}
        \item $D_j = -\sum_{k=1}^K \hat{p}_{jk} \log(\hat{p}_{jk})$
        where $\hat{p}_{jk}$ is the proportion of training observations in $R_j$
        \item Alternatively $J(\boldsymbol{B}) = -\frac{1}{h} \sum_{i=1}^h \sum_{k=1}^K y_i^{(k)} \log (\hat{p}_i^{(k)})$
        where $y_i^{(k)}$ indicates whether the $i^{th}$ training observation belongs to class $k$
        \item $D_j$ (or $J(\boldsymbol{B})$) takes a small value when nodes are pure, i.e., $\hat{p}_{jk}$ (or $\hat{p}_i^{(k)}$ is $0$ or $1$
    \end{itemize}
    \item Information gain: Reduction in cross-entropy
\end{itemize}