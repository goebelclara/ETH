\section{Linear Algebra}
\subsection*{Vector Properties}
\emph{Linear independence} --- Linear combination $\boldsymbol{A}\boldsymbol{u} = u_1 \boldsymbol{a_1} + ... + u_n \boldsymbol{a_n} = \sum_{i=1}^n u_i \boldsymbol{a_i}$ only has unique solutions for $\boldsymbol{u}$ (\emph{unique representation theorem}), if $\boldsymbol{A}\boldsymbol{u}=0$ then  $\boldsymbol{u}=0$, and $\boldsymbol{A}$ is full rank

{\color{lightgray}\hrule height 0.001mm}

\emph{Unit vector} --- $\boldsymbol{u} = \frac{\boldsymbol{\tilde{u}}}{\|\boldsymbol{\tilde{u}}\|}$, therefore $\|\boldsymbol{u}\|^2 = 1$

{\color{lightgray}\hrule height 0.001mm}

\emph{Inner product} --- $\boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{u}^\intercal\boldsymbol{v} = \sum_{i=1}^n u_i v_i = cos(\varphi) \textrm{ } \|\boldsymbol{u}\| \textrm{ } 
 \|\boldsymbol{v}\|$ --- 
Properties:
\begin{multicols}{2}
\begin{itemize}
    \item $\boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{v} \cdot \boldsymbol{u}$
    \item $(\boldsymbol{u}+\boldsymbol{v}) \cdot \boldsymbol{w} = \boldsymbol{u} \cdot \boldsymbol{w} + \boldsymbol{v} \cdot \boldsymbol{w}$
    \item $(\alpha\boldsymbol{u}) \cdot \boldsymbol{v} = \alpha (\boldsymbol{u} \cdot \boldsymbol{v})$
    \item Positive definite: $\boldsymbol{u} \cdot \boldsymbol{u} \geq 0$
    \item $\boldsymbol{u} \cdot \boldsymbol{u} = 0 \Leftrightarrow \boldsymbol{u} = 0_v$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Norm} --- 
$\|\boldsymbol{u}\| = \sqrt{\boldsymbol{u} \cdot \boldsymbol{u}}$ --- 
Properties:
\begin{multicols}{2}
\begin{itemize}
    \item $\|\boldsymbol{u}+\boldsymbol{v}\| = \|\boldsymbol{u}\| + \| \boldsymbol{v}\|$
    \item $\|\alpha\boldsymbol{u}\| = |\alpha| \|\boldsymbol{u}\|$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Cauchy Schwarz inequality} --- 
$\|\boldsymbol{u} \cdot \boldsymbol{v}\| \leq \|\boldsymbol{u}\| \textrm{ } \|\boldsymbol{v}\|$ with equality iff $\varphi = 0$ i.e. $\boldsymbol{u} = \alpha\boldsymbol{v}$ or if $\boldsymbol{u} \textrm{ or } \boldsymbol{v} = 0_v$\\
Proof:
\begin{itemize}
    \item First direction of proof: If $\boldsymbol{u} = \alpha\boldsymbol{v}$ or $\boldsymbol{u} \textrm{ or } \boldsymbol{v} = 0_v$, we can show that the equality holds
    \item Second direction of proof: If $\boldsymbol{u} \neq \alpha\boldsymbol{v}$ or $\boldsymbol{u} \textrm{ and } \boldsymbol{v} \neq 0_v$, we can show that the inequality cannot hold:
    \begin{itemize}
        \item $\boldsymbol{u}$ can be decomposed into $\boldsymbol{u}_v + \boldsymbol{u}_{v^\bot}$
        \item Then, we have $\|\boldsymbol{u} \cdot \boldsymbol{v}\| = \| (\boldsymbol{u}_v + \boldsymbol{u}_{v^\bot}) \cdot \boldsymbol{v} \| = \|\boldsymbol{u}_v\| \cdot \|\boldsymbol{v}\|$
        \item Based on Pythagorean theorem, we know that $\|\boldsymbol{u}\|^2 > \|\boldsymbol{u}_v\|^2$
        \item Then, we have $\|\boldsymbol{u} \cdot \boldsymbol{v}\| = \|\boldsymbol{u}_v\| \cdot \|\boldsymbol{v}\| < \|\boldsymbol{u}\| \cdot \|\boldsymbol{v}\|$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Triangle inequality} --- 
$\|\boldsymbol{u} + \boldsymbol{v}\| \leq \|\boldsymbol{u}\| + \|\boldsymbol{v}\|$ with equality iff $\varphi = 0$ i.e. $\boldsymbol{u} = \alpha\boldsymbol{v}$ or if $\boldsymbol{u} \textrm{ or } \boldsymbol{v} = 0_v$

{\color{lightgray}\hrule height 0.001mm}

\emph{Orthogonal vectors} ---  Properties:
\begin{itemize}
    \item $\boldsymbol{u} \cdot \boldsymbol{v} = 0$
    \item $\|\boldsymbol{u} + \boldsymbol{v}\|^2 = \|\boldsymbol{u}\|^2 + \|\boldsymbol{v}\|^2$
    \item \emph{Pythagorean theorem}: $\|\boldsymbol{u} - \boldsymbol{v}\|^2 = \|\boldsymbol{u}\|^2 + \|\boldsymbol{v}\|^2$
    \item Non-zero pairwise orthogonal vectors $\boldsymbol{u_n}$ and $\boldsymbol{u_m}$ are linearly independent\\
    Proof:
    \begin{itemize}
        \item Let $\sum_n \alpha_n \boldsymbol{u_n} = 0_v$
        \item Then, $ = 0_v \cdot \boldsymbol{u_m} = (\sum_n \alpha_n \boldsymbol{u_n}) \cdot \boldsymbol{u_m} = \sum_n \alpha_n (\boldsymbol{u_n}) \cdot \boldsymbol{u_m}) = \alpha_m \|\boldsymbol{u_m}\|^2$
        \item Then, $\alpha_m = 0$ for all m, meaning that all $\boldsymbol{u_m}$ are linearly independent
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Orthonormal vectors} ---  Vectors are orthonormal iff $\|\boldsymbol{u}\| = \|\boldsymbol{v}\| = 1$ and $\boldsymbol{u} \cdot \boldsymbol{v} = 0$

{\color{lightgray}\hrule height 0.001mm}

\emph{Projection} --- Projection of $\boldsymbol{v} \in V$ onto $\boldsymbol{s} \in S$ given by: $\boldsymbol{v}_S = \frac{\boldsymbol{v} \cdot \boldsymbol{s}}{\|\boldsymbol{s}\|^2}\boldsymbol{s} = (\boldsymbol{v} \cdot \boldsymbol{s})\boldsymbol{s}$ if $\boldsymbol{s}$ is a unit vector

{\color{black}\hrule height 0.001mm}

\subsection*{Vector Spaces}
\emph{Vector space $V$} --- Properties:
\begin{multicols}{2}
\begin{itemize}
    \item Additive closure: If $\boldsymbol{u},\boldsymbol{v} \in V$ then $\boldsymbol{u} + \boldsymbol{v} \in V$ 
    \item Scalar closure: If $\boldsymbol{u} \in V$ then $\alpha\boldsymbol{u} \in V$ 
    \item $\exists 0_v$ such that $\boldsymbol{u} + 0_v = \boldsymbol{u}$
    \item $\exists \boldsymbol{-u}$ such that $\boldsymbol{u} + \boldsymbol{-u} = 0_v$
    \item $\boldsymbol{u} + \boldsymbol{v} = \boldsymbol{v} + \boldsymbol{u}$ 
    \item $(\boldsymbol{u} + \boldsymbol{v}) + \boldsymbol{w} = \boldsymbol{u} + (\boldsymbol{v} + \boldsymbol{w})$ 
    \item $\alpha(\beta\boldsymbol{u})= (\alpha\beta)\boldsymbol{u}$ 
    \item $\alpha(\boldsymbol{u} + \boldsymbol{v}) = \alpha\boldsymbol{u} + \alpha\boldsymbol{v}$ 
    \item $\boldsymbol{u}(\alpha + \beta) = \alpha\boldsymbol{u} + \beta\boldsymbol{u}$ 
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Subspace $S$} --- Properties:
$S$ is a subspace of $V$ iff:
\begin{multicols}{2}
\begin{itemize}
    \item $0_v \in S$
    \item Additive closure
    \item Scalar closure
\end{itemize}
\end{multicols}
Proof:
\begin{itemize}
    \item If $S$ is a subspace of $V$ subspace properties immediately follow
    \item If subspace properties are satisfied for $S$, $S$  must be a subspace of $V$ because operations are inherited (for addition, multiplication) resp. can be derived from subspace properties (for $0_V, -v$)
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Invariant subspace $H$} --- $H$ is an invariant subspace of $S$ spanned by $\boldsymbol{S}$ if $\boldsymbol{S}\boldsymbol{h} \in H$ for all $\boldsymbol{h} \in H$ --- Properties:
\begin{itemize}
    \item $\boldsymbol{S}$ has an eigenvector in $H$
    \item If $\boldsymbol{S}$ is symmetric, $H^\bot$ is also an invariant subspace of $\boldsymbol{S}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Orthogonal complement $S^{\bot}$} --- Subspace, composed of set of vectors that are orthogonal to $S$ --- Properties:
\begin{multicols}{2}
\begin{itemize}
    \item The intersection of $S$ and $S^{\bot}$ is $\{0_v\}$
    \item $\textrm{dim}(S) + \textrm{dim}(S^{\bot})$ = $\textrm{dim}(V)$
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Span} --- Span of $\{\boldsymbol{s_i}\}_{i=1}^n$ is the set of all vectors that can be expressed as a linear combination of $\{\boldsymbol{s_i}\}_{i=1}^n$.
$\sum_{i=1}^n u_i \boldsymbol{s_i}$
\\
Span of matrix $\boldsymbol{A}$ is the span of its column vectors.
$\boldsymbol{A}\boldsymbol{u} = u_1 \boldsymbol{a_1} + ... + u_n \boldsymbol{a_n} = \sum_{i=1}^n u_i \boldsymbol{a_i}$
\\
A span is a subspace, since for a linear combination, we can derive additive closure and scalar closure.

{\color{lightgray}\hrule height 0.001mm}

\emph{(Orthonormal) basis} --- Unique set of all (orthonormal) vectors that are linearly independent and span the whole of a subspace.
\begin{itemize}
    \item \emph{Orthonormal representation theorem}: Any vector $\boldsymbol{x} \in S$ can be expressed in terms of orthonormal basis: $\boldsymbol{x} = \sum_i (\boldsymbol{x} \cdot \boldsymbol{s_i})\boldsymbol{s_i}$
    \item \emph{Parveval's theorem}: Extension of orthonormal representation theorem: $\boldsymbol{x} \cdot \boldsymbol{y} = \sum_i (\boldsymbol{x} \cdot \boldsymbol{s_i})(\boldsymbol{y} \cdot \boldsymbol{s_i})$
    \item \emph{Gram Schmidt orthonormalization}: Procedure to generate orthonormal basis $\{\boldsymbol{s_i}\}_{i=1}^n$ from linearly independent vectors $\{\boldsymbol{x^{(i)}}\}_{i=1}^n$:
    \begin{itemize}
        \item $\boldsymbol{\tilde{s_1}} = \boldsymbol{x_1}$
        \item $\boldsymbol{\tilde{s_k}} = \boldsymbol{x_k} - \sum_{i=1}^{k-1} (\boldsymbol{x_k} \cdot \boldsymbol{s_1})\boldsymbol{s_1}$ for $k > 1$
        \item $\boldsymbol{s_i} = \frac{\boldsymbol{\tilde{s_i}}}{\|\boldsymbol{\tilde{s_i}}\|}$
    \end{itemize}    
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Dimension} --- Number of vectors in basis of $S$.

{\color{lightgray}\hrule height 0.001mm}

\emph{Orthogonal vectors in spaces} --- 
\begin{itemize}
    \item Let $S$ be spanned by orthonormal $\boldsymbol{s_1},\boldsymbol{s_2},...$ and $\boldsymbol{v} \in V$
    \item \emph{Orthogonal decomposition theorem}: $\boldsymbol{v} = \boldsymbol{v}_{S} + \boldsymbol{v}_{S^\bot}$ where $\boldsymbol{v} \in V$, $\boldsymbol{v}_{S} \in S$ and $\boldsymbol{v}_{S^\bot} \in S^\bot$
    \item \emph{Orthogonality principle}
    \begin{itemize}
        \item $\boldsymbol{v}_{S}$ is the projection of $\boldsymbol{v} \in V$ to $S$ iff $(\boldsymbol{v}-\boldsymbol{v}_{S}) \cdot \boldsymbol{s_i} = 0$
        \item This can be rewritten to linear equation system $\boldsymbol{v} \cdot \boldsymbol{s_i} = \boldsymbol{v}_{S} \cdot \boldsymbol{s_i} = \sum_k \alpha_k (\boldsymbol{s_k} \cdot \boldsymbol{s_i})$ since $\boldsymbol{v}_{S} = \sum_k \alpha_k \boldsymbol{s_k}$
    \end{itemize}
    \item $\boldsymbol{v}_{S^\bot} = \boldsymbol{v} - \boldsymbol{v}_{S} = \boldsymbol{v} - \sum_k (\boldsymbol{v} \cdot \boldsymbol{s_k}) \boldsymbol{s_k}$ 
    \item \emph{Approximation in a subspace theorem}: 
    \begin{itemize}
        \item Unique best representation of $\boldsymbol{v}$ in $S$ is given by projection of $\boldsymbol{v}$ to $S$: $\|\boldsymbol{v} - \boldsymbol{s'}\| \geq \|\boldsymbol{v} - \boldsymbol{v}_S\|$ for some arbitrary $\boldsymbol{s'} \in S$
        \item Any subset $U$ of $S$ is closest to $\boldsymbol{v}$  iff it is closest to $\boldsymbol{v}_S$\\
        Proof:
        \begin{itemize}
            \item $\textrm{argmin}_u \| \boldsymbol{v} - \boldsymbol{u} \| = \textrm{argmin}_u \| \boldsymbol{v} - \boldsymbol{u} \|^2 = \textrm{argmin}_u \| \boldsymbol{v}_S + \boldsymbol{v}_{S^\bot} - \boldsymbol{u} \|^2 = \textrm{argmin}_u \| \boldsymbol{v}_S - \boldsymbol{u}\|^2 + \|\boldsymbol{v}_{S^\bot}\|^2$ given Pythagorean theorem $ = \textrm{argmin}_u \| \boldsymbol{v}_S - \boldsymbol{u}\|^2$
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Linear Equations}
Let $\boldsymbol{X}\boldsymbol{b} = \boldsymbol{y}$ where $\boldsymbol{X} \in \mathbb{R}^{n \times m}$, $\boldsymbol{b} \in \mathbb{R}^{m}$, $\boldsymbol{y} \in \mathbb{R}^{n}$ and $\boldsymbol{b}$ is unknown
\begin{itemize}
    \item Number of distinct equations = Number of linearly independent rows in $[\boldsymbol{X} | \boldsymbol{b}]$ = $\textrm{rank}([\boldsymbol{X} | \boldsymbol{b}])$ $\leq$ $\textrm{min}(n,m+1)$
    \item Number of LHS solutions should = Number of RHS solutions = $\textrm{rank}(\boldsymbol{X})$ $\leq$ $\textrm{min}(n,m)$
\end{itemize}
Solutions:
\begin{itemize}
    \item If $\textrm{rank}(\boldsymbol{X}) < \textrm{rank}([\boldsymbol{X} | \boldsymbol{b}])$, system is inconsistent (no solution)
    \item If $\textrm{rank}(\boldsymbol{X}) = \textrm{rank}([\boldsymbol{X} | \boldsymbol{b}]) < m$, system is singular  (infinitely many solutions) and underdetermined because we have fewer distinct equations than unknowns
    \item If $\textrm{rank}(\boldsymbol{X}) = \textrm{rank}([\boldsymbol{X} | \boldsymbol{b}]) = m = n$, system is non-singular (exactly one solution) and exactly determined 
    \item If $\textrm{rank}(\boldsymbol{X}) = \textrm{rank}([\boldsymbol{X} | \boldsymbol{b}]) = m < n$, system is non-singular and overdetermined 
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{General Matrix Properties}
\emph{Matrices} ---
\begin{itemize}
    \item $\boldsymbol{A} \in \mathbb{R}^{n \times m}$ with elements $A_{ij}$, rows $i = 1,...,n$, columns $j = 1,...,m$
    \item Transpose $\boldsymbol{A^\intercal}$
    \item Identity matrix $\boldsymbol{I}$ with 1 on diagonal, 0 elsewhere
    \item Scalar matrix $\boldsymbol{K}$ with $k$ on diagonal, 0 elsewhere
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Operations} ---
\begin{itemize}
    \item Element-wise addition: Returns matrix of same size
    \item Element-wise scalar multiplication: Returns matrix of same size
    \item Matrix multiplication: 
    \begin{itemize}
        \item $\boldsymbol{A}^{n \times p}\boldsymbol{B}^{p \times m}=\boldsymbol{C}^{n \times m}$
        \begin{multicols}{2}
        \begin{itemize}
            \item $r_v \times c_v = s$
            \item $c_v \times r_v = M$
            \item $M \times c_v = c_v$
            \item $r_v \times M = r_v$ 
            \item $M \times M = M$
        \end{itemize}
        \end{multicols}
        \item Element in $\boldsymbol{C}$ is sum-product of row in $\boldsymbol{A}$ and column in $\boldsymbol{B}$: $C_{ij} = \boldsymbol{A}^{(i)} \cdot \boldsymbol{B}^{(j)}$
        \item Column vector in $\boldsymbol{C}$ is a linear combination of the columns in $\boldsymbol{A}$: $\boldsymbol{C}^{(j)} = \boldsymbol{A} \boldsymbol{B}^{(j)} = \sum_p \boldsymbol{A}^{(j=p)} b_{p}^{(j)}$
        \item Row vector in $\boldsymbol{C}$ is a linear combination of the rows in $\boldsymbol{B}$: $\boldsymbol{C}^{(i)} = \boldsymbol{A}^{(i)} \boldsymbol{B} = \sum_p a_{p}^{(i)} \boldsymbol{B}^{(i=p)}$
        \item $\boldsymbol{C} = \boldsymbol{A}[\boldsymbol{B^{(j=1)}} | ... | \boldsymbol{B^{(j=m)}}]$
        \item $\boldsymbol{C} = [\boldsymbol{A^{(i=1)}} | ... | \boldsymbol{A^{(i=n)}}]^\intercal \boldsymbol{B} = [\boldsymbol{A^{(i=1)}} \boldsymbol{B} | ... | \boldsymbol{A^{(i=n)}} \boldsymbol{B}]^\intercal$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Impliations} ---

\begin{itemize}
    \item $\boldsymbol{A} \boldsymbol{e_k} = \boldsymbol{A^{(j=k)}}$ and $\boldsymbol{e_k}^\intercal \boldsymbol{A} = \boldsymbol{A^{(i=k)}}$ where $\boldsymbol{e_k}$ = 1 on $k^{th}$ element and 0 everywhere else
    \item Matrix form:
    \begin{itemize}
        \item In following $^{(j)}$ refers to column vector and $^{(i)}$ to row vector, however written as column vector
        \item $\boldsymbol{u} \cdot \boldsymbol{v} = \boldsymbol{u}^\intercal \boldsymbol{v} = \sum_i u_i v_i = c$
        \item $\boldsymbol{u} \boldsymbol{v}^\intercal = \boldsymbol{C}$\\
        with $u_i v_j = C_{ij}$
        \item $\boldsymbol{A} \boldsymbol{u} = \sum_{j=i} \boldsymbol{A}^{(j)} u_i = \boldsymbol{c}$\\
        with $\boldsymbol{A}^{(i)} \cdot \boldsymbol{u} = \boldsymbol{A}^{(i) \intercal} \boldsymbol{u} = c_i$
        \item $\boldsymbol{u}^\intercal \boldsymbol{A} = \sum_{j=i} \boldsymbol{A}^{(i) \intercal} u_j = \boldsymbol{c}^\intercal$\\
        with $\boldsymbol{u} \cdot \boldsymbol{A}^{(j)} = \boldsymbol{u}^\intercal \boldsymbol{A}^{(j)} = c_j$
        \item $\boldsymbol{A} \boldsymbol{B} = \sum_{j=i} \boldsymbol{A}^{(j)} \boldsymbol{B}^{(i) \intercal} = \boldsymbol{C}$\\
        with $\boldsymbol{A}^{(i)} \cdot \boldsymbol{B}^{(j)} = \boldsymbol{A}^{(i) \intercal} \boldsymbol{B}^{(j)} = C_{ij}$
    \end{itemize}
    \item Moving between instance-level $\rightarrow$ data-level:
    \begin{itemize}
        \item $\boldsymbol{x^{(i)}} \boldsymbol{y} = \boldsymbol{a}$ $\rightarrow$ $\boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{a}$ where $\boldsymbol{X}$ consists of rows $\boldsymbol{x^{(i)}}$
        \item $\boldsymbol{x^{(i)}} \boldsymbol{x^{(i)}}^\intercal = \boldsymbol{A}$ $\rightarrow$ $\boldsymbol{X}^\intercal \boldsymbol{X} = \boldsymbol{A}$ where $\boldsymbol{X}$ consists of rows $\boldsymbol{x^{(i)}}$
        \item $\boldsymbol{x^{(i)}} \cdot \boldsymbol{\beta} = y_i$ $\rightarrow$ $\boldsymbol{X}\boldsymbol{\beta} = \boldsymbol{y}$ where $\boldsymbol{X}$ consists of rows $\boldsymbol{x^{(i)}}$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Properties} ---
\begin{multicols}{2}
\begin{itemize}
    \item $(\boldsymbol{A} + \boldsymbol{B})^\intercal = \boldsymbol{A}^\intercal + \boldsymbol{B}^\intercal$
    \item $(\alpha\boldsymbol{A})^\intercal = \alpha \boldsymbol{A}^\intercal$
    \item $(\boldsymbol{A} \boldsymbol{B})^\intercal = \boldsymbol{B}^\intercal \boldsymbol{A}^\intercal$
    \item $(\boldsymbol{A} + \boldsymbol{B}) + \boldsymbol{C} = \boldsymbol{A} + (\boldsymbol{B} + \boldsymbol{C})$
    \item $\boldsymbol{A} + \boldsymbol{B} = \boldsymbol{B} + \boldsymbol{A}$
    \item $\alpha(\boldsymbol{A} + \boldsymbol{B}) = \alpha\boldsymbol{A} + \alpha\boldsymbol{B}$
    \item $(\alpha + \beta)\boldsymbol{A}= \alpha\boldsymbol{A} + \beta\boldsymbol{A}$
    \item $(\alpha\beta)\boldsymbol{A}= \alpha(\beta\boldsymbol{A})$
    \item $(\boldsymbol{A} + \boldsymbol{B})\boldsymbol{x} = \boldsymbol{A}\boldsymbol{x} + \boldsymbol{B}\boldsymbol{x} = \boldsymbol{C}\boldsymbol{x}$
    \item $(\boldsymbol{A} \boldsymbol{B})\boldsymbol{x} = \boldsymbol{A} (\boldsymbol{B} \boldsymbol{x}) = \boldsymbol{C}\boldsymbol{x}$
    \item $\boldsymbol{A} = 0.5(\boldsymbol{A} + \boldsymbol{A}^\intercal) + 0.5(\boldsymbol{A} - \boldsymbol{A}^\intercal) = \boldsymbol{B} + \boldsymbol{C}$ where $\boldsymbol{B}$ is symmetric, but not $\boldsymbol{C}$
    \item $\boldsymbol{A} = \boldsymbol{A}\boldsymbol{I} = \boldsymbol{I}\boldsymbol{A}$
    \item $\boldsymbol{A}k = \boldsymbol{A}\boldsymbol{K} = \boldsymbol{K}\boldsymbol{A}$
    \item $\textrm{rank}(\boldsymbol{A}\boldsymbol{B}) = \textrm{min(rank}(\boldsymbol{A}), \textrm{rank}(\boldsymbol{B}))$
    \item $\boldsymbol{A}^\intercal\boldsymbol{A}$ satisfies: 
    \begin{itemize}
        \item Symmetric
        \item Psd
        \item Has rank $m$ iff it is pd
        \item Invertible iff it has rank $m$ and it is pd
        \item $\textrm{rank}(\boldsymbol{A}^\intercal\boldsymbol{A}) = \textrm{rank}(\boldsymbol{A}) = \textrm{rank}(\boldsymbol{A}^\intercal)$
        \item $\textrm{rank}(\boldsymbol{A}^\intercal\boldsymbol{A}) = \textrm{rank}([\boldsymbol{A}^\intercal\boldsymbol{A} | \boldsymbol{A}^\intercal\boldsymbol{x}])$
    \end{itemize}
\end{itemize}
\end{multicols}

{\color{lightgray}\hrule height 0.001mm}

\emph{Matrix terminology} ---
\begin{itemize}
    \item Kernel $\textrm{null}(\boldsymbol{X})$ contains set of vectors $\boldsymbol{b}$ such that linear map $\boldsymbol{X}\boldsymbol{b} = 0$
    \item Nullity = $\textrm{dim(null}(\boldsymbol{X}))$
    \item Image $\textrm{range}(\boldsymbol{X})$ contains set of vectors $\boldsymbol{b}$ generated by linear map $\boldsymbol{X}\boldsymbol{b}$ resp. is space spanned by columns of $(\boldsymbol{X})$
    \item Row space is space spanned by rows of $(\boldsymbol{X})$ 
    \item Column rank = $\textrm{dim(colspace}(\boldsymbol{X}))$ = number of linearly independent columns, row rank = $\textrm{dim(rowspace}(\boldsymbol{X}))$ = number of linearly independent rows
    \item Rank = column rank = row rank = $\textrm{dim(range}(\boldsymbol{X}))$ = $\textrm{dim(range}(\boldsymbol{X}^\intercal))$ $\leq min(n,m)$
    \item \emph{Rank nullity theorem}: $\textrm{Rank}(\boldsymbol{X}) + \textrm{nullity}(\boldsymbol{X}) = m$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Matrices as linear maps} ---
$\boldsymbol{X}$ maps $\boldsymbol{b}$ from $\mathbb{R}^m$ to $\mathbb{R}^n$: $\boldsymbol{X}\boldsymbol{b} = \boldsymbol{y}$ with $\boldsymbol{X} \in \mathbb{R}^{n \times m}$, $\boldsymbol{b} \in \mathbb{R}^{m}$, $\boldsymbol{y} \in \mathbb{R}^{n}$
\begin{itemize}
    \item Injective: $\boldsymbol{X}\boldsymbol{b} = \boldsymbol{y}$ has at most one solution, happens iff columns of X are linearly independent ($\textrm{rank}(\boldsymbol{X}) = m \leq n$)
    \item Surjective: $\boldsymbol{X}\boldsymbol{b} = \boldsymbol{y}$ has at least one solution, happens iff rows of X are linearly independent ($\textrm{rank}(\boldsymbol{X}) = n \leq m$)
    \item Bijective: Mapping is both injective and surjective, i.e. $m=n$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Projection matrices} ---
Let $S$ be spanned by orthonormal $\{\boldsymbol{b_i}\}_{i=1}^n$, which are column vectors of the matrix $\boldsymbol{B} \in \mathbb{R}^{m \times n}$
\begin{itemize}
    \item Projection of $\boldsymbol{x}$ onto $S$ is given by: $\boldsymbol{u} = \sum_i (\boldsymbol{x} \cdot \boldsymbol{b_i}) \boldsymbol{b_i} = \sum_i \boldsymbol{b_i} \boldsymbol{b_i}^\intercal \boldsymbol{x} = \boldsymbol{B}\boldsymbol{B}^\intercal\boldsymbol{x} = \boldsymbol{C}\boldsymbol{x}$
    \item Projection of $\boldsymbol{x}$ onto $S^\bot$ is given by: $\boldsymbol{x} - \boldsymbol{u} = \boldsymbol{I}\boldsymbol{x} - \boldsymbol{C}\boldsymbol{x}$
\end{itemize}


{\color{black}\hrule height 0.001mm}

\subsection*{Square Matrix Properties}

\emph{Square matrix terminology} ---
\begin{itemize}
    \item Diagonal matrix:
    \begin{itemize}
        \item Def: Has $\{d_i\}_{i=1}^n$ on diagonal and 0 everywhere else
        \item For diagonal matrices: $\boldsymbol{D}\boldsymbol{D}^\intercal = \boldsymbol{D}^\intercal\boldsymbol{D}$
    \end{itemize} 
    \item Inverse matrix:
    \begin{itemize}
        \item Def: $\boldsymbol{A}^{-1}\boldsymbol{A} = \boldsymbol{I}$
        \item Is unique
        \item For diagonal matrices: $\boldsymbol{A}^{-1}$ can be calculated by inverting all diagonal elements  
    \end{itemize} 
    \item Symmetric (Hermitian) matrix: $\boldsymbol{A}^\intercal = \boldsymbol{A}$
    \item Orthogonal (unitary) matrix: 
    \begin{itemize}
        \item Def: $\boldsymbol{A}^\intercal = \boldsymbol{A}^{-1}$
        \item $\boldsymbol{A}\boldsymbol{A}^\intercal = \boldsymbol{A}^\intercal\boldsymbol{A} = \boldsymbol{I}$
        \item Rows and columns are orthonormal
        \item $\|\boldsymbol{A} \boldsymbol{x}\| = \|\boldsymbol{x}\|$
        \item $(\boldsymbol{A}\boldsymbol{x}) \cdot (\boldsymbol{A}\boldsymbol{y}) = \boldsymbol{x} \cdot \boldsymbol{y}$
    \end{itemize}
    \item Involution matrix: $\boldsymbol{A}^{-1} = \boldsymbol{A}$
    \item Determinant: 
    \begin{itemize}
        \item Function which maps $\boldsymbol{A}$ to a scalar
        \item Properties:
        \begin{multicols}{2}
        \begin{itemize}
            \item $\textrm{det}(\boldsymbol{I}) = 1$
            \item $\textrm{det}(\boldsymbol{A}\boldsymbol{B}) = \textrm{det}(\boldsymbol{A})\textrm{det}(\boldsymbol{B})$
            \item $\textrm{det}(\boldsymbol{A}^\intercal) = \textrm{det}(\boldsymbol{A})$
            \item $\textrm{det}(\boldsymbol{A}^{-1}) = (\textrm{det}(\boldsymbol{A}))^{-1}$
            \item $\textrm{det}(\alpha\boldsymbol{A}) = \alpha^2\textrm{det}(\boldsymbol{A})$
        \end{itemize}
        \end{multicols}
    \end{itemize}
    
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Invertible matrix theorem} --- Following statements are equivalent for square matrix $\boldsymbol{A} \in \mathbb{R}^{n \times n}$: 
\begin{itemize}
    \item $\boldsymbol{A}$ is invertible
    \item Only solution to $\boldsymbol{A}\boldsymbol{x} = 0$ is $\boldsymbol{x} = 0_v$\\
    Proof:
    \begin{itemize}
        \item $\boldsymbol{A}^{-1} \boldsymbol{A} \boldsymbol{x} = 0 \Rightarrow \boldsymbol{I} \boldsymbol{x} = 0 \Rightarrow \boldsymbol{x} = 0_v$
    \end{itemize}
    \item $\boldsymbol{A}$ is non-singular
    \item Columns (and rows) of $\boldsymbol{A}$ are linearly independent
    \item $\textrm{rank}(\boldsymbol{A}) = n$
    \item $\textrm{det}(\boldsymbol{A}) = 0$
\end{itemize}
Inversely, if $\boldsymbol{A}$ is not invertible, the columns and rows are not linearly independent, etc. 

{\color{lightgray}\hrule height 0.001mm}

\emph{Properties} --- 
\begin{itemize}
    \item For symmetric $\boldsymbol{A}$: $( \boldsymbol{x} + \boldsymbol{A}^{-1} \boldsymbol{b} )^\intercal \boldsymbol{A} ( \boldsymbol{x} + \boldsymbol{A}^{-1} \boldsymbol{b} ) - \boldsymbol{b}^\intercal \boldsymbol{A}^{-1} \boldsymbol{b} = \boldsymbol{x}^\intercal \boldsymbol{A} \boldsymbol{x} + 2 \boldsymbol{x}^\intercal \boldsymbol{b}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Matrix inversion lemma} ---
\begin{itemize}
    \item Let $\boldsymbol{B} \in \mathbb{R}^{n \times n}$, $\boldsymbol{D} \in \mathbb{R}^{m \times m}$, $\boldsymbol{C} \in \mathbb{R}^{n \times m}$.\\
    Then, $\boldsymbol{A} = \boldsymbol{B}^{-1} + \boldsymbol{C}\boldsymbol{D}^{-1}\boldsymbol{C}^\intercal$ is invertible: $\boldsymbol{A}^{-1} = \boldsymbol{B} - \boldsymbol{B}\boldsymbol{C} (\boldsymbol{D}+\boldsymbol{C}^\intercal\boldsymbol{B}\boldsymbol{C})^{-1}\boldsymbol{C}^\intercal\boldsymbol{B}$
    \item Let $\boldsymbol{v} \in \mathbb{R}^{n}$.\\
    Then, $(\alpha \boldsymbol{I} + \boldsymbol{v}\boldsymbol{v}^\intercal)^{-1}\boldsymbol{v} = (\alpha + \|\boldsymbol{v}\|^2)^{-1}\boldsymbol{v} = \boldsymbol{v}^\intercal(\alpha \boldsymbol{I} + \boldsymbol{v}\boldsymbol{v}^\intercal)^{-1} = \boldsymbol{v}^\intercal(\alpha + \|\boldsymbol{v}\|^2)^{-1}$
\end{itemize}
 
{\color{lightgray}\hrule height 0.001mm}

\emph{Quadratic form} --- Quadratic form of square matrix $\boldsymbol{M}$: $\boldsymbol{x}^\intercal\boldsymbol{M}\boldsymbol{x}$. Can be expressed as quadratic form of a symmetric matrix $\boldsymbol{A}$: $\boldsymbol{x}^\intercal\boldsymbol{A}\boldsymbol{x}$ where $\boldsymbol{A} = 0.5 \times (\boldsymbol{M} + \boldsymbol{M}^\intercal) + 0.5 \times (\boldsymbol{M} - \boldsymbol{M}^\intercal)$.

{\color{lightgray}\hrule height 0.001mm}

\emph{Eigenvectors and eigenvalues} --- 
\begin{itemize}
    \item $\boldsymbol{q}$ is an eigenvector of $\boldsymbol{A}$ associated with an eigenvalue $\lambda$ if it remains on the same line after transformation by a linear map: $\boldsymbol{A}\boldsymbol{q} = \lambda\boldsymbol{q}$
    \item Let $\boldsymbol{A} \in \mathbb{R}^{n \times n}$. $\boldsymbol{A}$ can have between $1-n$ eigenvalues, each with multiple eigenvectors. Eigenvectors for distinct eigenvalues are linearly independent.
    \item If there exists a non-trivial solution for $\boldsymbol{q}$, $(\boldsymbol{A}-\lambda\boldsymbol{I})$ is not invertible and characteristic polynomial $\textrm{det}(\boldsymbol{A}-\lambda\boldsymbol{I}) = 0$
    \item \emph{Eigendecomposition resp. diagonalization}: $\boldsymbol{A} = \boldsymbol{Q}\boldsymbol{\Lambda}\boldsymbol{Q}^{-1}$ where $\boldsymbol{Q}$ is a matrix with the eigenvectors as columns and $\boldsymbol{\Lambda}$ is a diagonal matrix with the eigenvalues on the diagonal
    \item $\textrm{det}(\boldsymbol{A}) = \textrm{det}(\boldsymbol{Q}\boldsymbol{\Lambda}\boldsymbol{Q}^{-1}) = \prod_{i=1}^n \lambda_i$
    \item \emph{Symmetric eigendecomposition resp. unitary diagonalization}: For symmetric $\boldsymbol{A}$: $\boldsymbol{A} = \boldsymbol{Q}\boldsymbol{\Lambda}\boldsymbol{Q}^\intercal$ where $\boldsymbol{Q}$ is an orthogonal matrix with the eigenvectors as columns and $\boldsymbol{\Lambda}$ is a diagonal matrix with the eigenvalues on the diagonal
    \item \emph{Spectral theorem}: Square matrix $\boldsymbol{A}$ is symmetrically diagonizable, iff $\boldsymbol{A}\boldsymbol{A}^\intercal = \boldsymbol{A}^\intercal\boldsymbol{A}$
    \item \emph{Spectral theorem for symmetric matrices}: Every symmetric matrix $\boldsymbol{A}$ is symmetrically diagonizable (due to Spectral theorem) and all its eigenvalues are real
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Positive definite (pd) and positive semi-definite matrices (psd)} --- 
\begin{multicols}{2}
\begin{itemize}
    \item $\boldsymbol{A} \succ 0$ iff $\boldsymbol{x}^\intercal\boldsymbol{A}\boldsymbol{x} > 0$
    \item $\boldsymbol{A} \succeq 0$ iff $\boldsymbol{x}^\intercal\boldsymbol{A}\boldsymbol{x} \geq 0$
\end{itemize}
\end{multicols}
Properties:
\begin{itemize}
    \item If $\boldsymbol{A}$ is p(s)d, $\alpha\boldsymbol{A}$ is also p(s)d
    \item If $\boldsymbol{A}$ and $\boldsymbol{B}$ are p(s)d, $\boldsymbol{A} + \boldsymbol{B}$ is also p(s)d
    \item If $\textrm{det}(\boldsymbol{A}) = \prod_{i=1}^n \lambda_i > (\geq) \textrm{ } 0$ resp. $\{\lambda_i\}_{i=1}^n > (\geq) \textrm{ } 0$ for pd (psd)
\end{itemize}
Pd properties:
\begin{itemize}
    \item $\boldsymbol{I}$ is pd
    \item If $\boldsymbol{A}$ is pd, $\boldsymbol{A}^{-1}$ is pd
    \item \emph{Cholesky decomposition}: If $\boldsymbol{A}$ is pd, $\boldsymbol{A} = \boldsymbol{B}\boldsymbol{B}^\intercal$
    \item If $\boldsymbol{A}$ and $\boldsymbol{B}$ are pd, $(\boldsymbol{A}\boldsymbol{B})^{-1} = \boldsymbol{B}^{-1}\boldsymbol{A}^{-1}$
\end{itemize}
Psd properties:
\begin{itemize}
    \item If $\boldsymbol{A}$ is psd, $\boldsymbol{B}\boldsymbol{A}\boldsymbol{B}^\intercal$ is psd
\end{itemize}