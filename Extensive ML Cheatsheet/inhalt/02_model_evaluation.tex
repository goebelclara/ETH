\section{Model Evaluation}
\subsection*{Estimator Evaluation Criteria}
\emph{Crtiteria} --- 
\begin{itemize}
    \item Consistency: $\hat{\boldsymbol{\theta}} \rightarrow \boldsymbol{\theta}$ as $n \rightarrow \infty$
    \item Bias: $\mathbb{E}(\hat{\boldsymbol{\theta}}) - \boldsymbol{\theta}$
    \begin{itemize}
        \item Unbiased: $\mathbb{E}(\hat{\boldsymbol{\theta}}) = \boldsymbol{\theta}$
        \item Asymptotically unbiased: $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ] = 0$ as $n \rightarrow \infty$ 
        \item Asymptotically efficient: $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ] = I$ as $n \rightarrow \infty$  where $I$ is Fisher information 
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Fisher information} --- 
\begin{itemize}
    \item $\boldsymbol{I} = \mathbb{E} [(\Lambda)^2] = \mathbb{E} [ ( \frac{\partial}{ \partial \boldsymbol{\theta} } log( p ( \boldsymbol{x} | \boldsymbol{\theta} ) ) )^2 ] = \mathbb{V} ( \frac{ \partial log( p(\boldsymbol{x} | \boldsymbol{\theta}) ) }{ \partial \boldsymbol{\theta}} )$ where $\Lambda$ is the score
    \item Equality is given because $\mathbb{V}(x) = \mathbb{E}[(x-\mathbb{E}(x))^2] = \mathbb{E}[x^2]$ if $\mathbb{E}(x) = 0$, which is the case here, given that the expected score is $0$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Rao-Cramer bound} --- 
\begin{itemize}
    \item Shows that there does not exist an asymptotically unbiased parameter estimator
    \item For each unbiased estimator, $\mathbb{E} [ ( \hat{\boldsymbol{\theta}} - \boldsymbol{\theta} )^2 ] \geq \frac{1}{I}$ where $I$ is the Fisher information
    \item For estimators in general, $\frac{(\frac{\partial}{\partial \boldsymbol{\theta}} \textrm{bias} + 1)^2}{\boldsymbol{I}} + \textrm{bias}^2 \leq \mathbb{E}[(\hat{\boldsymbol{\theta}}-\boldsymbol{\theta})^2]$, so there is a trade-off if the bias derivative is negative and the squared bias is positive, whereby a biased estimator may produce better results than an unbiased estimator
\end{itemize}
Proof: 
\begin{itemize}
    \item Given Cauchy Schwarz inequality, we can say:
    $\mathbb{E}[ (\Lambda - \mathbb{E}((\Lambda)) (\hat{\boldsymbol{\theta}} - \mathbb{E}(\hat{\boldsymbol{\theta}})) ]^2 \leq \mathbb{E}[(\Lambda - \mathbb{E}((\Lambda))^2] \mathbb{E}[(\hat{\boldsymbol{\theta}} - \mathbb{E}(\hat{\boldsymbol{\theta}}))^2]$ where $\Lambda$ is the score
    \item We know that $\mathbb{E}(\Lambda) = 0$
    \item Let's look at the LHS of the equation:
    \begin{itemize}
        \item Since $\mathbb{E}(\Lambda) = 0$, we can simplify to $\mathbb{E}[ \Lambda (\hat{\boldsymbol{\theta}} - \mathbb{E}(\hat{\boldsymbol{\theta}})) ] = \mathbb{E}[ \Lambda \hat{\boldsymbol{\theta}}] - \mathbb{E}[\Lambda] \mathbb{E}[\hat{\boldsymbol{\theta}}] = \mathbb{E}[ \Lambda \hat{\boldsymbol{\theta}}] - 0$
        \item This can be developed to: $\mathbb{E}[ \Lambda \hat{\boldsymbol{\theta}}] = \int p( \boldsymbol{x} | \boldsymbol{\theta}) \frac{ \frac{\partial}{\partial \boldsymbol{\theta}} p( \boldsymbol{x} | \boldsymbol{\theta})}{ p( \boldsymbol{x} | \boldsymbol{\theta}) } \hat{\boldsymbol{\theta}} dx = \frac{\partial}{\partial \boldsymbol{\theta}} (\int p( \boldsymbol{x} | \boldsymbol{\theta}) \hat{\boldsymbol{\theta}} dx - \boldsymbol{\theta}) + 1$ where the last part ($- \boldsymbol{\theta}) + 1$) can be added, because $\frac{\partial}{\partial \boldsymbol{\theta}} -\boldsymbol{\theta} = -1$ and we compensate this with $+1$
        \item This is equal to the derivative of the bias + 1: $\frac{\partial}{\partial \boldsymbol{\theta}} (\int p( \boldsymbol{x} | \boldsymbol{\theta}) \hat{\boldsymbol{\theta}} dx - \boldsymbol{\theta}) + 1 = \frac{\partial}{\partial \boldsymbol{\theta}} (\mathbb{E}[\hat{\boldsymbol{\theta}}] - \boldsymbol{\theta}) + 1 = \frac{\partial}{\partial \boldsymbol{\theta}} \textrm{bias} + 1$
    \end{itemize}
    \item Let's look at the RHS of the equation: Since $\mathbb{E}(\Lambda) = 0$, first term is $\mathbb{E}(\Lambda^2) = \boldsymbol{I}$
    \item Then, we have: $(\frac{\partial}{\partial \boldsymbol{\theta}} \textrm{bias} + 1)^2 \leq \boldsymbol{I} \times \mathbb{E}[(\hat{\boldsymbol{\theta}}-\mathbb{E}(\hat{\boldsymbol{\theta}}))^2] = \boldsymbol{I} \times \mathbb{E}[(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta} - \mathbb{E}(\hat{\boldsymbol{\theta}}) + \boldsymbol{\theta})^2] = ... = \boldsymbol{I} \times \mathbb{E}[(\hat{\boldsymbol{\theta}} - \boldsymbol{\theta})^2] - \textrm{bias}^2$
    \item Then, we have $\frac{(\frac{\partial}{\partial \boldsymbol{\theta}} \textrm{bias} + 1)^2}{\boldsymbol{I}} + \textrm{bias}^2 \leq \mathbb{E}[(\hat{\boldsymbol{\theta}}-\boldsymbol{\theta})^2]$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Bias Variance Tradeoff}
\begin{itemize}
    \item Mean squared error $\mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \boldsymbol{y})^2 ]$ can be decomposed into: $( \mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}) )^2 + \mathbb{V}(\hat{f}(\boldsymbol{X})) + \mathbb{E}[\epsilon^2] = \textrm{bias}^2 + \textrm{variance} + \textrm{irreducible error}$\\
    Proof:
    \begin{itemize}
        \item $\boldsymbol{y} = f(\boldsymbol{X}) + \epsilon$
        \item $\mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \boldsymbol{y})^2 ] = \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E}[\hat{f}(\boldsymbol{X})] + \mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}) + \epsilon)^2 ] = \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E}[\hat{f}(\boldsymbol{X})] )^2 ] + \mathbb{E}[(\mathbb{E}[\hat{f}(\boldsymbol{X})] - f(\boldsymbol{X}))^2] + \mathbb{E}[\epsilon^2] - 2 \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) ]$
        \item Fourth term equals 0:
        \begin{itemize}
            \item $2 \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) ] = 2( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) \mathbb{E}[ (\hat{f}(\boldsymbol{X}) - \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] ) ]$ because $( \mathbb{E} [ \hat{f}(\boldsymbol{X}) ] - f(\boldsymbol{X}) + \epsilon ) $ is deterministic
            \item In last equation, second term equals 0, so whole equation is 0
        \end{itemize}
        \item Then, we are left with: $\textrm{variance} + \textrm{bias}^2 + \textrm{irreducible error}$
    \end{itemize}
    \item Bias: Error generated by the fact that we approximate a complex relationship via a simpler model (small function class) with a certain presupposed parametric form
    \item Variance: Error generated by the fact that we estimate the model parameters with a noisy training sample (small sample), rather than the population
    \item Irreducible error: Error generated by measurement error and the fact that we estimate $\boldsymbol{y}$ as a function of $\boldsymbol{X}$, when it is a function of many other factors
    \item Bias variance tradeoff: Bias and variance cannot be reduced simultaneously
    \begin{itemize}
        \item High variance associated with overfitting: Model corresponds too closely to particular training set resp. performs poorly on unseen data, but well on training set 
        \item High bias associated with underfitting: Model fails to capture underlying relationships resp. performs poorly on both training set and unseen data
    \end{itemize}    
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Approximating Generalisation Loss via Empirical Loss}
\emph{Via resampling methods} --- \\
Cross-validation:
\begin{itemize}
    \item Partition data $\mathcal{Z}$ into $K$ equally sized disjoint subsets: $\mathcal{Z} = \mathcal{Z}_1 \cup \mathcal{Z}_2 \cup ... \cup \mathcal{Z}_K$
    \item Produce estimator $\hat{f}^{-v}$ from $\mathcal{Z} \backslash \mathcal{Z}_v$ for $v \leq K$
    \item Empirical loss given by: $\hat{\mathcal{R}}^{cv} = \frac{1}{n} \sum_{i\leq n} LO(y_i - \hat{f}^{-k(i)} (x_i))$ where $k(i)$ maps $i$ to partition $\mathcal{Z}_{k(i)}$ where $(x_i,y_i)$ belongs
\end{itemize}

Bootstrapping:
\begin{itemize}
    \item Draw $B$ samples with replacement of size $n$ from data $\mathcal{Z}$: $\mathcal{Z}^{*b}$
    \item Compute estimate $S( \mathcal{Z}^{*b} )$ for each bootstrap sample
    \item For each estimate $S( \mathcal{Z}^{*b} )$, we can give a mean and variance:
    \begin{itemize}
        \item $\bar{S} = \frac{1}{B} \sum_b S( \mathcal{Z}^{*b} )$
        \item $\sigma^2(S) = \frac{1}{B-1} \sum_b ( S( \mathcal{Z}^{*b} ) - \bar{S})^2$
    \end{itemize}
    \item Empirical loss given by: $\hat{\mathcal{R}}(\mathcal{A}) = \frac{1}{B} \sum_{b=1}^B \frac{1}{n} \sum_{i=1}^n LO(y_i - \hat{f}^{*b} (x_i))$ 
    \item Out-of-bag loss given by: $\hat{\mathcal{R}}^{bs} = \frac{1}{n} \sum_{i=1}^n \frac{1}{| C^{-i} |} \sum_{b \in C^{-i}} LO(y_i - \hat{f}^{*b} (x_i))$ where $C^{-i}$ contains all bootstrap indices $b$ so that $\mathcal{Z}^{*b}$ does not contain $(x_i,y_i)$
    \item Empirical loss of bootstrap uses training data to estimate $\hat{\mathcal{R}}$, i.e. it is generally too optimistic. We can correct this by combining the empirical and out-of-bag loss:
    \begin{itemize}
        \item Probability that $(x_i,y_i)$ is not in sample $\mathcal{Z}^{*b}$ of size $n$ is given by $(1-\frac{1}{n})^n = \frac{1}{e} \textrm{ as } n \rightarrow \infty \approx \frac{1}{3}$
        \item Probability that $(x_i,y_i)$ is in sample $\mathcal{Z}^{*b}$ of size $n$ is given by $1 - \frac{1}{e} \textrm{ as } n \rightarrow \infty \approx \frac{2}{3}$
        \item We then define: $\hat{\mathcal{R}}^{(0.632)} = 0.368 \hat{\mathcal{R}}(\mathcal{A}) + 0.632 \hat{\mathcal{R}}^{bs}$
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Common Loss Measures}
\emph{Confusion matrix} --- 
\begin{itemize}
    \item Precision = Correctly predicted positive / predicted positive
    \item Recall = Correctly predicted positive / positive in reality
    \item Accuracy = $(TP + TN) / N$
    \item F1 score = Harmonic mean between precision and recall = $(2PR)/(P+R)$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Curve scores} --- 
\begin{itemize}
    \item ROC AUC curve: 
    \begin{itemize}
        \item Plots the TPR (recall, y) against the False Positive Rate FPR (x) at various threshold levels
        \item A perfect model has an AUC of 1, a random model has an AUC of 0.5
        \item Useful when we care about the trade-off between sensitivity and specificity
    \end{itemize}
    \item Precision recall curve:
    \begin{itemize}
        \item Plots the precision (y) against the recall (x) at various threshold levels
        \item A perfect model has precision and recall of 1, the curve for a random model is a horizontal line at the baseline precision, reflecting the proportion of positive samples in the dataset
        \item Useful when we care about the trade-off between sensitivity and specificity
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Structured prediction and NLP} --- 
\begin{itemize}
    \item Some outcomes are more similar than others, e.g. if a translation is almost right vs. a translation is completely wrong
    \item NLP evaluation metrics:
    \begin{itemize}
        \item Intrinsic evaluation:
        \begin{itemize}
            \item Held-out log likelihood $l(\boldsymbol{w})$
            \item Perplexity: $perplexity(\boldsymbol{w}) = 2^{-\frac{l(\boldsymbol{w})}{M}}$
            \item Lower perplexity = higher likelihood
            \item Cosine similarity of embeddings for words, which are humanly considered similar
            \item Word analogy testing (e.g. king vs. queen, man vs. woman)
        \end{itemize}
        \item Extrinsic evaluation on downstream task
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\hl{TBA}