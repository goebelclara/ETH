\section{Language Models}
\subsection*{Background}
\begin{itemize}
    \item Alphabet $\Sigma$: Finite set of symbols; in NLP: vocabulary of tokens
    \item String $\boldsymbol{y}$ over an alphabet: Finite sequence of alphabet symbols
    \item Set of all possible strings (incl. empty string $\varepsilon$) given by Kleene closure, i.e. the set of all possible strings (incl. the empty string $\varepsilon$) that can be formed by concatenating 0 or more elements from $\Sigma$
    \item BOS: Beginning of sequence token
    \item EOS: End of sequence token
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Description}
\emph{Task} --- 
\begin{itemize}
    \item \emph{Structured prediction task}: Predict over exponentially (or even infinitely) large set of classes
    \item Assign a probability to $\boldsymbol{y}$, i.e., fit a probability distribution over all $\Sigma^*$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
Globally normalized language model:
\begin{itemize}
    \item Compute the probability of sentence $\boldsymbol{y}$ and normalize over all $\boldsymbol{y} \in \Sigma^*$:
    $
    p(\boldsymbol{y}) = \frac{\prod_{t=1}^{|\boldsymbol{y}|} \theta_{y \leq t}}{Z}
    $
    where $\theta_{y \leq t}$ is the weight for transition from $y_{t-1} \to y_t$, conditioned on having taken the path $y_1, \ldots, y_{t-1}$ and Z is the normalization constant given by $Z = \sum_{\boldsymbol{y'} \in \Sigma^*} \prod_{t=1}^{|\boldsymbol{y'}|} \theta_{y' \leq t}$
    \item Challenge: Since $\Sigma^*$ is infinite, $Z$ is infinite
    \item Solution: Locally normalized language model
\end{itemize}
Locally normalized language model:
\begin{itemize}
    \item Probability of a word $y_t$ in a sentence $\boldsymbol{y}$ is conditioned only on the preceding context
    \item Compute the probability of sentence $\boldsymbol{y}$ and normalize over all possible words in the vocabulary:
    $
    p(\boldsymbol{y}) = p(y_1 \mid \textrm{BOS}) \times p(y_2 \mid  \textrm{BOS} \ y_1) \times ... \times p(y_N \mid \boldsymbol{y}_{<N}) \times p(\textrm{EOS} \mid \boldsymbol{y})
    $
    \item Can be visualized as a prefix tree:
    \begin{itemize}
        \item \hl{INSERT IMAGE 1}
        \item Probabilities of all children nodes, given their parent node, sum to 1
        \item All nodes have EOS as a child to ensure that each of the (possibly infinitely many) paths has a finite length
    \end{itemize}
\end{itemize}
Tight models:
\begin{itemize}
    \item If the language model is locally normalized and sums to 1, the language model is \emph{tight}
    \item To enforce tightness:
    $
    p(\textrm{EOS} \mid \cdots) > \delta > 0
    $
    \begin{itemize}
        \item Assume we want the probability sentences of less than length $n$ to be $(1-\delta)$
        \item Then, the probability of a sentence of length $n$ is $\delta$
        \item The probability over all sentences is then given by: $
        \sum_{n=1}^\infty (1-\delta)^{n-1} \delta = \frac{\delta}{1-(1-\delta)} = 1
        $ because it forms a geometric series
    \end{itemize}
\end{itemize}
