\section{Attention}
\subsection*{Description}
\begin{itemize}
    \item Attention helps specify which inputs we need to pay attention to when producing a given output
    \item Can be used:
    \begin{itemize}
        \item As cross-attention: Between encoder and decoder
        \item As self-attention: Within a single hidden layer resp. within the encoder or decoder
    \end{itemize}
    \item Usually applied after embedding and before applying an activation function
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Naive method}
\begin{itemize}
    \item Let $\boldsymbol{E}$ be the embedding matrix $\mathbb{R}^{s \times d}$ where $s$ = number of tokens, $d$ = embedding dimensionality
    \item Steps:
    \begin{itemize}
        \item Re-weight embeddings: $\tilde{\boldsymbol{E}} = \boldsymbol{E} \boldsymbol{w}$
        \item Compute similarity matrix: $\boldsymbol{S} = \sigma( \tilde{\boldsymbol{E}} \tilde{\boldsymbol{E}}^\intercal )$ where $\sigma$ is softmax, normalizing the rows of $\boldsymbol{S}$ to sum to 1, $\tilde{\boldsymbol{E}} \tilde{\boldsymbol{E}}^\intercal$ is similarity between two tokens in $\tilde{\boldsymbol{E}}$
        \item Compute attention-weighted embedding matrix: $\boldsymbol{A} = \boldsymbol{S}\tilde{\boldsymbol{E}}$
    \end{itemize}
    \item In this context, $\boldsymbol{E}$ and $\boldsymbol{w}$ are trainable parameters
    \item Challenge: Similarity matrix is symmetric, given that dot product $\tilde{\boldsymbol{E}} \tilde{\boldsymbol{E}}^\intercal$ is symmetric, i.e. attention paid by token $i$ to token $j$ ($S_{ij}$) is same as attention paid by token $j$ to token $i$ ($S_{ji}$)
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Adjusted method}
\begin{itemize}
    \item Steps:
    \begin{itemize}
        \item Generate three sets of re-weighted embeddings: 
        \begin{itemize}
            \item $\boldsymbol{Q} = \boldsymbol{E} \boldsymbol{W}^q$ resp. $\boldsymbol{q}_i = \boldsymbol{e}_i \boldsymbol{W}^q$
            \begin{itemize}
                \item $\boldsymbol{E}$ $(m \times h)$
                \item $\boldsymbol{W}_q$ $(h \times d_k)$
                \item $\boldsymbol{Q}$ $(m \times d_k)$
                \item $\boldsymbol{q}_i$ is row vector $(1 \times d_k)$
                \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
            \end{itemize}
            \item $\boldsymbol{K} = \boldsymbol{E} \boldsymbol{W}^k$ resp. $\boldsymbol{k}_i = \boldsymbol{e}_i \boldsymbol{W}^k$
            \begin{itemize}
                \item $\boldsymbol{E}$ $(n \times h)$
                \item $\boldsymbol{W}_k$ $(h \times d_k)$
                \item $\boldsymbol{K}$ $(n \times d_k)$
                \item $\boldsymbol{k}_i$ is row vector $(1 \times d_k)$
                \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
            \end{itemize}
            \item $\boldsymbol{V} = \boldsymbol{E} \boldsymbol{W}^v$ resp. $\boldsymbol{v}_i = \boldsymbol{e}_i \boldsymbol{W}^v$ where 
            \begin{itemize}
                \item $\boldsymbol{E}$ $(n \times h)$
                \item $\boldsymbol{W}_v$ $(h \times d_v)$
                \item $\boldsymbol{V}$ $(n \times d_v)$
                \item $\boldsymbol{v}_i$ is row vector $(1 \times d_v)$
                \item $\boldsymbol{e}_i$ is row vector $(1 \times h)$
            \end{itemize}
            \item (Note: if $\boldsymbol{Q,K,V}$ contain multiple heads, they are expanded in this step)
        \end{itemize}
        \item Compute similarity matrix: $\boldsymbol{A} = \sigma(\frac{\boldsymbol{Q}\boldsymbol{K}^\intercal}{\sqrt{d_k}})$ in $(m \times n)$ resp. $\boldsymbol{\alpha}_{ti} = \frac{exp( \boldsymbol{q}_t \cdot \boldsymbol{k}_i )}{\sum_{i'} exp( \boldsymbol{q}_t \cdot \boldsymbol{k}_{i'} )}$
        \item Compute attention-weighted embedding matrix: $\boldsymbol{Z} = \boldsymbol{A}\boldsymbol{V}$ in $(m \times d_v)$ resp. $\boldsymbol{z}_t = \sum_i \alpha_{ti} \boldsymbol{v}_i$ 
        \item (Note: if $\boldsymbol{A}$ contains multiple heads, they are flattened in this step by multiplying with $d_v$)
    \end{itemize}
    \item In cross-attention:
    \begin{itemize}
        \item Attention for decoder-encoder alignment
        \item $\boldsymbol{Q}$ is generated with decoder input during traning
        \item $\boldsymbol{V},\boldsymbol{K}$ are generated with encoder outputs during training
    \end{itemize}
    \item In self-attention:
    \begin{itemize}
        \item Attention for encoder resp. decoder inputs
        \item $\boldsymbol{Q}, \boldsymbol{V},\boldsymbol{K}$ are generated with input during training
        \item In masked self-attention:
        \begin{itemize}
            \item We first calculate $\boldsymbol{P} =\boldsymbol{Q}\boldsymbol{K}^\intercal$ where masked elements (e.g. states with time $\geq m$ in decoder) are set to $-\infty$
            \item $\boldsymbol{S} = \sigma(\frac{\boldsymbol{P}}{\sqrt{d_k}})$
        \end{itemize}
    \end{itemize}
    \item In multi-head attention:
    \begin{itemize}
        \item Creates multiple sets of $\boldsymbol{Q,K,V}$ and calculates attention correspondingly
        \item Concatenates generated matrices $\boldsymbol{Z}$
    \end{itemize}
\end{itemize}

\section{Encoder Decoder RNNs}
\subsection*{Description}
\emph{Task} --- Generate embeddings, perform sequence-to-sequence tasks

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\emph{Formulation} --- 
\begin{itemize}
    \item Model architecture: 
    \begin{itemize}
        \item Inputs fed into encoder in reverse order
        \item Encoder: 
        \begin{itemize}
            \item Sequence-to-vector
            \item Hidden states $\boldsymbol{h}_n^{l (e)} = f ( \boldsymbol{W}_l^{(e)} \boldsymbol{h}_{n-1}^{l(e)} + \boldsymbol{W}_{l-1}^{(e)} \boldsymbol{h}_{n}^{l-1(e)} )$ where $f$ is activation function, $l$ is layer, $n$ is time step in input sequence
        \end{itemize}
        \item Outputs from encoder to decoder are weighted by attention weights:
        \begin{itemize}
            \item Context vector $\boldsymbol{z}_m = \sum_i \alpha_{mi} \boldsymbol{h}_i^{(e)}$ where 
            \begin{itemize}
                \item $\boldsymbol{h}_i^{(e)}$ is the encoder hidden state (= $V$)
                \item $\alpha_{mi}$ is attention weight at decoder time step $m$ for encoder hidden state $\boldsymbol{h}_i^{(e)}$ 
                \item $\alpha_{mi} = \textrm{softmax}(\boldsymbol{h}_{m-1}^{(d)} \cdot \boldsymbol{h}_i^{(e)})$ where $\boldsymbol{h}_{m-1}^{(d)}$ is previous decoder hidden state (= $Q$) and $\boldsymbol{h}_i^{(e)}$ are the final encoder hidden states at each time step (= $K$)
            \end{itemize}
        \end{itemize}
        \item Alongside context vectors, target sequence inputs are fed into decoder with one time step lag during training
        \item Decoder: 
        \begin{itemize}
            \item Vector-to-sequence
            \item Hidden states $\boldsymbol{h}_m^{(d)} = f ( \boldsymbol{W}_2^{(d)} \boldsymbol{h}_{m-1}^{(d)} + \boldsymbol{W}_1^{(d)} \boldsymbol{w'}_{m-1} )$ where $f$ is activation function, $m$ is time step in target sequence, $\boldsymbol{w'}_{m-1}$ is word at index $m-1$ in target sequence (assuming one layer), $\boldsymbol{h}_0^{(d)} = \boldsymbol{h}_N^{(e)}$, i.e. last encoder output is first decoder input
        \end{itemize}
    \end{itemize}
    \item Challenge: Sequential, cannot be parallelized. Solution: Transformers
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Variants}
\emph{Variants} --- 
\begin{itemize}
    \item ELMO: Bi-directional LSTM
\end{itemize}

\section{Encoder Decoder Transformers}
\subsection*{Description}
\emph{Task} --- Generate embeddings, perform sequence-to-sequence tasks

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\emph{Formulation} --- 
\begin{itemize}
    \item Model architecture: 
    \begin{itemize}
        \item Inputs fed into encoder in reverse order
        \item Encoder: 
        \begin{itemize}
            \item Sequence-to-vector
            \item Takes in input sequence token embeddings (semantic vector) and positional embeddings (sinusoidal pointer vector for word position, given that model is not sequential) and adds them
            \item Multi-head self-attention, applied to all tokens jointly, where $\boldsymbol{Q,K,V}$ are tokens in input sequence
            \item Addition and normalization: Skip connections (from token + positional embeddings) added back and normalized
            \item Feed-forward network, parallel for each token
            \item Addition and normalization: Skip connections (from first addition and normalization) added back and normalized
            \item Generates hidden states $\boldsymbol{h}_n^{(e)} = f ( \boldsymbol{W}_1^{(e)} \boldsymbol{h}_{n-1}^{(e)} + \boldsymbol{W}_2 \boldsymbol{w}_n )$ where $f$ is activation function, $\boldsymbol{w}_n$ is token at index $n$ in input sequence
        \end{itemize}
        \item Outputs from encoder to decoder are weighted by attention weights:
        \begin{itemize}
            \item Context vector $\boldsymbol{z}_m = \sum_{n=1}^N \alpha_{m,n} \boldsymbol{h}_n^{(e)}$ where 
            \begin{itemize}
                \item $\boldsymbol{h}_n^{(e)}$ is the encoder hidden state (= $V$)
                \item $\alpha_{m,n}$ is attention weight at decoder time step $m$ for encoder hidden state $\boldsymbol{h}_n^{(e)}$
                \item $\alpha_{m,n} = \phi(\boldsymbol{h}_{m-1}^{(d)}, [\boldsymbol{h}_1^{(e)}, ..., \boldsymbol{h}_N^{(e)}]$ where $\boldsymbol{h}_{m-1}^{(d)}$ is previous decoder hidden state (= $Q$) and $[\boldsymbol{h}_1^{(e)}), ..., \boldsymbol{h}_N^{(e)})]$ are the final encoder hidden states at each time step (= $K$)
            \end{itemize}
        \end{itemize}
        \item Alongside context vectors, target sequence inputs are fed into decoder with one time step lag during training
        \item Decoder: 
        \begin{itemize}
            \item Vector-to-sequence
            \item Takes in target sequence token embeddings (semantic vector) and positional embeddings (sinusoidal pointer vector for word position, given that model is not sequential) and adds them
            \item Masked multi-head self-attention, applied to all tokens jointly, where $\boldsymbol{Q,K,V}$ are tokens in target sequence
            \item Addition and normalization: Skip connections (from token + positional embeddings) added back and normalized
            \item Cross-attention to incorporate encoder outputs
            \item Addition and normalization: Skip connections (from first addition and normalization) added back and normalized
            \item Feed-forward network, parallel for each token
            \item Addition and normalization: Skip connections (from second addition and normalization) added back and normalized
            \item Hidden states $\boldsymbol{h}_m^{(d)} = f ( \boldsymbol{W}_3^{(d)} \boldsymbol{h}_{m-1}^{(d)} + \boldsymbol{W}_2^{(d)} \boldsymbol{w'}_{m-1} + \boldsymbol{W}_1^{(d)} \boldsymbol{z}_m)$ where $f$ is activation function, $m$ is time step in target sequence, $\boldsymbol{w'}_{m-1}$ is token at index $m-1$ in target sequence, $\boldsymbol{h}_0^{(d)} = \boldsymbol{h}_N^{(e)}$, i.e. last encoder output is first decoder input
        \end{itemize}
        \item Linear layer applied to $\boldsymbol{h}_M^{(d)}$
        \item Softmax layer applied to select token with highest probability: Since it is intractable to search for best sequence overall (explore), we turn to deterministic or stochastic variants (exploit): 
        \begin{itemize}
            \item Greedy decoding: Select highest-probability token at each step
            \item Beam search: Keep $n$-highest-probability tokens in memory (beam size) and return $k$-most-likely sequences (top beams)
            \item Nucleus sampling: Sample tokens from items that cover $p\%$ of PMF
        \end{itemize}
    \end{itemize}
    \item Advantage: 
    \begin{itemize}
        \item Relies on attention to obtain a fixed-size representation of a sequence (in contrast to RNN)
        \item Allows to learn longer-range dependencies than RNNs
        \item Allows for parallelization, whereas RNNs must run sequentially
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Variants}
\emph{Variants} --- 
\begin{itemize}
    \item GPT: Uni-directional, decoder-only transformer, predicts next word in sequence
    \item BERT: Bi-directional, encoder-only transformer, predicts masked word from context
\end{itemize}