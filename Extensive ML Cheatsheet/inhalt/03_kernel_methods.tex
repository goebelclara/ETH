\section{Kernel Methods}
\subsection*{Background on Kernel Methods}
\emph{Description} ---
\begin{itemize}
    \item Mechanism for tractably resp. implicitly mapping data into higher-dimensional feature space so that linear models can be used in this feature space
    \item To do so, we can employ the \emph{kernel trick} and the \emph{representer theorem}
    \item The requirements are that the kernel function fulfills \emph{Mercer's theorem}, i.e. the kernel is a Mercer kernel
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Kernel trick} ---
\begin{itemize}
    \item Allows to operate in higher-dimensional feature space, without explicitly calculating this transformation, but instead implicitly computing the inner product in this feature space via a kernel function
    \item Given two inputs $\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}$ and a feature map $\varphi: \mathbb{R}^m \rightarrow \mathbb{R}^k$ we can define an inner product on $\mathbb{R}^k$ via the kernel function: $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \item If a prediction function is described solely in terms of inner products in the input space, it can be lifted into the feature space by replacing the inner product with the kernel function
    \item Kernel trick requires that span of training instances  $span(\varphi(\boldsymbol{x^{(i)}}), ..., \varphi(\boldsymbol{x^{(N)}})) = \mathbb{R}^k$ and, thus, that $N \geq k$.\\
    Proof:
    \begin{align*}
    dim(span(...)) = 
    \left\{
        \begin{aligned}
             & N \quad & \text{if } N < k \\
             & k \quad & \text{if } N \geq k
        \end{aligned}
    \right.
    \end{align*}
    \item Kernel trick cannot be used in conjunction with feature selection resp. sparsity inducing regularize (e.g. $\ell_1$), as this does not satisfy the representer theorem
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Representer theorem} ---
\begin{itemize}
    \item Allows to avoid directly seeking the $k$ parameters, but only the $n$ parameters that characterize $\boldsymbol{\alpha}$
    \item Allows to avoid calculating $\varphi({\boldsymbol{z}})$ when evaluating novel instance, but only sum over weighted set of n kernel function outputs
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Mercer's theorem} ---
\begin{itemize}
    \item Kernel function is psd and symmetric iff $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \begin{itemize}
        \item Psd: $\boldsymbol{x}^\intercal \boldsymbol{K} \boldsymbol{x} \geq 0$ where $\boldsymbol{K}$ is the kernel matrix
        \item Symmetric: $k(\boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}}) = k(\boldsymbol{x^{(j)}}, \boldsymbol{x^{(i)}})$
    \end{itemize}
    \item Kernel that satisfies Mercer's theorem is a Mercer kernel, i.e. we can prove a kernel is a Mercer kernel either if it is psd and symmetric or by finding a feature map such that the kernel function corresponds to an inner product    
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item Feature map $\varphi: \mathbb{R}^m \rightarrow \mathbb{R}^k$
    \item Linear prediction function: $\boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}})$
    \item Regularized loss function: $LO = \sum_{i=1}^n LO(y^{(i)}, \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}}) + \Omega( \boldsymbol{\beta} ))$
    \item Iff $\Omega( \boldsymbol{\beta} ))$ is a non-decreasing function, then the parameters $\boldsymbol{\beta}$ that minimize the loss function can be rewritten as: $\boldsymbol{\beta} = \sum_{i=1}^n \alpha^{(i)} \varphi( \boldsymbol{x^{(i)}} )$
    \item Outcome of novel instance can be predicted as: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} \varphi( \boldsymbol{x^{(i)}}) \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} k(\boldsymbol{x^{(i)}}, \boldsymbol{z})$
    \item Act of prediction becomes act of measuring similarity to training instances in feature map space
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Kernel Types}
\emph{Polynomial kernel} ---
\begin{itemize}
    \item $\varphi(\boldsymbol{x}) = [ 
    x^\alpha ]_{\alpha \in \mathbb{N}^m}$ where $\alpha = (\alpha_1, ..., \alpha_m)$ is the multi-index representing the power and $x^\alpha = x_1^{\alpha_1} \times ... \times  x_m^{\alpha_m}$ is the mononomial term corresponding to the multi-index $\alpha$
    \item E.g. if degree = 2, then $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = 1 + 2 x_1^{(i)} x_1^{(j)} + 2 x_2^{(i)} x_2^{(j)} + ( x_1^{(i)} x_1^{(j)} )^2 + ( x_2^{(i)} x_2^{(j)} )^2 + 2 x_1^{(i)} x_1^{(j)} x_2^{(i)} x_2^{(j)}$
    \item Inner product diverges to infinity
    \item To address this, we often use RBF kernel instead
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{RBF kernel} ---
\begin{itemize}
    \item Gives access to infinite feature space
    \item $\varphi(\boldsymbol{x}) = exp ( -\frac{1}{2} \|\boldsymbol{x}\|^2 ) [ \frac{ \boldsymbol{x}^\alpha }{ \sqrt{\alpha!} } ]_{\alpha \in \mathbb{N}^m}$ 
    \item $k( \boldsymbol{x^{(i)}}, \boldsymbol{x^{(j)}} ) = \sigma^2 exp (- \frac{ \| \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} \|^2 }{ 2l^2 } )$\\
    Proof:
    \begin{itemize}
        \item $exp ( -\frac{1}{2} \|\boldsymbol{x^{(i)}}\|^2 ) exp ( -\frac{1}{2} \|\boldsymbol{x^{(j)}}\|^2 ) \sum_\alpha$\\
        $[ \frac{ \boldsymbol{x^{(i)}}^\alpha \boldsymbol{x^{(j)}}^\alpha }{ \alpha! } ]$
        \item Given multinomial series expansion, $\sum_\alpha [ \frac{ \boldsymbol{x^{(i)}}^\alpha \boldsymbol{x^{(j)}}^\alpha }{ \alpha! } ] = exp(\boldsymbol{x^{(i)}}^\intercal \boldsymbol{x^{(j)}})$
        \item $exp ( -\frac{1}{2} \|\boldsymbol{x^{(i)}}\|^2  -\frac{1}{2} \|\boldsymbol{x^{(j)}}\|^2 + \boldsymbol{x^{(i)}}^\intercal \boldsymbol{x^{(j)}}) = exp (- \frac{ \| \boldsymbol{x^{(i)}} - \boldsymbol{x^{(j)}} \|^2 }{ 2\sigma^2 } )$
    \end{itemize}
    \item Length scale parameter $l$ controls how quickly the similarity decays with distance: If $l$ is large, points with high distance still have high covariance
    \item Variance parameter $\sigma$ controls the vertical scale of the function
    \item RBF kernel is stationary, meaning that only the relative distance between two points determines the value output by the kernel function
    \item Challenge: Cannot ignore irrelevant dimensions (whereas e.g. a neural network can do this by setting the associated weights to $0$)
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Kernel compositions} ---
\begin{itemize}
    \item New valid kernels can be composed via:
    \begin{itemize}
        \item Addition: $k_1 + k_2$
        \item Multiplication: $k_1 \times k_2$
        \item Scaling: $c \times k_1$ for $c > 0$
        \item Composition: $f(k_1)$ where $f$ is a polynomial with positive coefficients or the exponential function
    \end{itemize}
    \item Valid kernels:
    \begin{itemize}
        \item $k'(x_1,x_2) = c k(x_1,x_2)$, since $\varphi'(x) = \sqrt{c}\varphi(x)$
        \item $k'(x_1,x_2) = f(x_1) k(x_1,x_2) f(x_2)$, since $\varphi'(x) = f(x)\varphi(x)$
        \item $k'(x_1,x_2) = k_1(x_1,x_2) + k_2(x_1,x_2)$, since the requirements for a valid kernel are that its psd and symmetric, which is retained when two psd and symmetric matrices are added
        \item $k'(x_1,x_2) = k_1(x_1,x_2) k_2(x_1,x_2)$, since new kernel is given by the $i^{th}$ feature value under feature map $\varphi_1$ multiplied by the $j^{th}$ feature value under feature map $\varphi_2$
        \item $k'(x_1,x_2) = exp(k(x_1,x_2))$, since we can apply Taylor series expansion $\sum_{n=1}^r \frac{k(x_1,x_2)^r}{r!} = exp(k(x_1,x_2)) = k'(x_1,x_2)$ as $r \rightarrow \infty$ and we know that exponentiation, addition, and scaling produces valid new kernels from above
    \end{itemize}
\end{itemize}

\section{Polynomial Kernel Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $\boldsymbol{y} = \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}})$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Ordinary least squares estimator (OLSE)
    \item Minimize mean squared error: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \varphi(\boldsymbol{x^{(i)}} ))^2$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Primal solution:
    \begin{itemize}
        \item Parameters can be estimated as: $\boldsymbol{\beta} = (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1}  \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
        \item Prediction for novel instance: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1}  \boldsymbol{\Phi}^\intercal \boldsymbol{y} \cdot \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal \boldsymbol{\Phi} (\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi})^{-1} \varphi({\boldsymbol{z}}) $
    \end{itemize}
    \item Let us define $\boldsymbol{K} = \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal$ as the kernel matrix of the training data with $K_{ij} = \varphi(\boldsymbol{x^{(i)}}) \cdot \varphi(\boldsymbol{x^{(j)}})$
    \item Dual solution $\boldsymbol{\alpha}$ if we have no regularization, i.e. $\lambda = 0$:
    \begin{itemize}
        \item Parameters can be estimated as: $\boldsymbol{\beta} =  \boldsymbol{\Phi}^\intercal \boldsymbol{K}^{-1} \boldsymbol{y}$\\
        Proof:
        \begin{itemize}
            \item $(\boldsymbol{\Phi}^\intercal \boldsymbol{\Phi} + \lambda \boldsymbol{I}) \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
            \item $\Rightarrow \boldsymbol{\Phi}^\intercal \boldsymbol{\Phi} \boldsymbol{\beta} + \lambda \boldsymbol{I} \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{y}$
            \item $\Rightarrow \boldsymbol{I} \boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \lambda^{-1}  (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$
            \item Since we know from the representer theorem that $\boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha}$, we can say: $\boldsymbol{\alpha} = \lambda^{-1}  (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$ 
            \item We can further develop this to: $\lambda \boldsymbol{\alpha} = (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\beta})$
            \item Replacing $\boldsymbol{\beta}$ by $\boldsymbol{\Phi}^\intercal \boldsymbol{\alpha}$ yields: $\lambda \boldsymbol{\alpha} = (\boldsymbol{y} - \boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha})$
            \item $\Rightarrow \boldsymbol{\alpha} = (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \lambda \boldsymbol{I})^{-1} \boldsymbol{y} = \boldsymbol{K}^-1 \boldsymbol{y}$
            \item With this, we can calculate the parameters: $\boldsymbol{\beta} = \boldsymbol{\Phi}^\intercal \boldsymbol{\alpha} = \boldsymbol{\Phi}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal + \lambda \boldsymbol{I})^{-1} \boldsymbol{y} = \boldsymbol{\Phi}^\intercal \boldsymbol{K}^{-1} \boldsymbol{y}$
        \end{itemize}
        \item Prediction for novel instance: $\boldsymbol{\beta} \cdot \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal)^{-1}  \boldsymbol{\Phi} \varphi({\boldsymbol{z}}) = \boldsymbol{y}^\intercal (\boldsymbol{\Phi} \boldsymbol{\Phi}^\intercal)^{-1} \boldsymbol{k}$ where $\boldsymbol{k} = \boldsymbol{\Phi} \varphi({\boldsymbol{z}}) = [ k(\boldsymbol{x^{(1)}}, \boldsymbol{z}), ..., k(\boldsymbol{x^{(n)}}, \boldsymbol{z}) ]^\intercal = [ \varphi(\boldsymbol{x^{(1)}}) \cdot \varphi(\boldsymbol{z}), ..., \varphi(\boldsymbol{x^{(n)}}) \cdot \varphi(\boldsymbol{z}) ]^\intercal$ is a kernel vector, consisting of kernel values between training instances and new instance
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Algorithm} --- 
Training:
\begin{enumerate}
    \item Compute kernel matrix given RBF kernel\\
    Time complexity: $\mathcal{O}(n^2 \times m)$ for $n^2$ kernel matrix values and $m$ number of features in each instance vector
    \item Perform training by solving $\boldsymbol{\alpha} = \boldsymbol{K}^{-1} \boldsymbol{y}$ for $\boldsymbol{\alpha}$\\
    Time complexity: $\mathcal{O}(n^3)$ 
    \item Store $\boldsymbol{\alpha}$\\
    Space complexity: $\mathcal{O}(n^2)$ 
\end{enumerate}
Prediction:
\begin{enumerate}
    \item Compute kernel vector\\
    Time complexity: $\mathcal{O}(n \times m \times d)$ for $d$ new instances, given $n$ instances in training data and $m$ features in each instance vector
    \item Store $\boldsymbol{k}$\\
    Space complexity: $\mathcal{O}(n \times d)$ for $d$ new instances, given $n$ as length of kernel vector
    \item Predict response using stored kernel vector\\
    Time complexity: $\mathcal{O}(n \times d)$ for $d$ new instances, given $n$ as length of $\boldsymbol{\alpha}$
\end{enumerate}
Value:
\begin{itemize}
    \item Primal solution training is of time complexity $\mathcal{O}(k^3)$ and prediction is of time complexity $\mathcal{O}(k)$
    \item Dual solution speeds this up as seen above in the algorithm
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically 
\end{itemize}

\section{Gaussian Processes}
\subsection*{Description}
\emph{Task} --- Models a distribution over functions

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Non-parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} + \epsilon$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \epsilon$
    \item $\boldsymbol{\beta} \sim \mathcal{N} (0,\boldsymbol{\Lambda}^{-1})$
    \item $\epsilon \sim \mathcal{N} (0,\sigma \boldsymbol{I}_m)$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Optimization} --- 
\begin{itemize}
    \item If we compute the moment of the Gaussian:
    \begin{itemize}
        \item $\mathbb{E}[\boldsymbol{y}] = \boldsymbol{X}^\intercal \mathbb{E}(\boldsymbol{\beta}) =  \boldsymbol{X}^\intercal 0 = 0$
        \item $\textrm{Cov}(\boldsymbol{y}) = \mathbb{E} [ (\boldsymbol{X}^\intercal \boldsymbol{\beta} + \epsilon) ( 
        \boldsymbol{X}^\intercal \boldsymbol{\beta} + \epsilon )^\intercal ] = \boldsymbol{X} \mathbb{E}(\boldsymbol{\beta} \boldsymbol{\beta}^\intercal) =  \boldsymbol{X}^\intercal + \boldsymbol{X} \mathbb{E}(\boldsymbol{\beta}) \mathbb{E}(\epsilon^\intercal) + \mathbb{E}(\epsilon) \mathbb{E}(\boldsymbol{\beta}^\intercal) \boldsymbol{X}^\intercal + \mathbb{E}(\epsilon \epsilon^\intercal) = 0$ where
        \begin{itemize}
            \item $\mathbb{V}(\boldsymbol{\beta}) = \mathbb{E}(\boldsymbol{\beta} \boldsymbol{\beta}^\intercal)$ and $\mathbb{V}(\epsilon) = \mathbb{E}(\epsilon \epsilon^\intercal)$ because $\mathbb{V}(x) = \mathbb{E}[(x-\mathbb{E}(x))^2] = \mathbb{E}[x^2]$ if $\mathbb{E}(x) = 0$, which is the case here due to the defined distributions
            \item $\mathbb{E}(\epsilon) = 0$ 
        \end{itemize}
        \item Plugging in the variance for $\boldsymbol{\beta}$ and $\epsilon$, we have $\textrm{Cov}(\boldsymbol{y}) = \boldsymbol{X} \boldsymbol{\Lambda}^{-1} \boldsymbol{X}^\intercal + \sigma^2 \boldsymbol{I}_m $
        \item This can be written as a Kernel matrix $\boldsymbol{K}$:
        $\begin{bmatrix}
        K_{1,1} + \sigma^2 & ... & ... & K_{1,n} \\
        ... & K_{2,2} + \sigma^2 & ... & ... \\
        ... & ... & ... & ... \\
        K_{n,1} & ... & ... & K_{n,n} + \sigma^2
        \end{bmatrix}$ with $K_{ij} = \boldsymbol{x^{(i)}}^\intercal \boldsymbol{\Lambda}^{-1} \boldsymbol{x^{(j)}}$
        \item In this kernel matrix, the kernel function can take any shape
    \end{itemize}
    \item On this basis, Gaussian process is defined as collection of random variables such that every finite subset of variables is jointly Gaussian: $f \sim \mathcal{GP}( \boldsymbol{\mu}, \boldsymbol{K})$
    \item A new instance follows the distribution $p(y_{n+1}) = \mathcal{N} ( \boldsymbol{k}^\intercal \boldsymbol{C}_n^{-1} \boldsymbol{y}, c - \boldsymbol{k}^\intercal \boldsymbol{C}_n^{-1} \boldsymbol{k} )$ where 
    \begin{itemize}
        \item $\boldsymbol{k} = k(\boldsymbol{x^{(1)}}, \boldsymbol{x^{(n+1)}}), ..., k(\boldsymbol{x^{(n)}}, \boldsymbol{x^{(n+1)}}) ]^\intercal = [ \varphi(\boldsymbol{x^{(1)}}) \cdot \varphi(\boldsymbol{x^{(n+1)}}), ..., \varphi(\boldsymbol{x^{(n)}}) \cdot \varphi(\boldsymbol{x^{(n+1)}}) ]^\intercal$ is the kernel vector
        \item $\boldsymbol{C}_n = k(\boldsymbol{x^{(i)}},\boldsymbol{x^{(j)}}) + \sigma^2 \boldsymbol{I}_m$
        \item $c = k(\boldsymbol{x^{(n+1)}},\boldsymbol{x^{(n+1)}}) + \sigma^2 \boldsymbol{I}_m$
    \end{itemize}\\
    Proof:
    \begin{itemize}
        \item We derive the joint distribution $p(
        \begin{bmatrix}
        \boldsymbol{y} \\
        y_{n+1}
        \end{bmatrix} 
        ) \sim \mathcal{N} [0, 
        \begin{bmatrix}
        \boldsymbol{C}_n | \boldsymbol{k} \\
        \boldsymbol{k}^\intercal | c
        \end{bmatrix} 
        ]$
        \item To obtain a closed-form solution for this, we can make use of the following theorem:
        \begin{itemize}
            \item Given a joint Gaussian distribution: $\begin{bmatrix}
            a_1 \\
            a_2
            \end{bmatrix} 
            ) \sim \mathcal{N} [
            \begin{bmatrix}
            u_1 \\
            u_2
            \end{bmatrix},
            \begin{bmatrix}
            \Sigma_{11} | \Sigma_{12} \\
            \Sigma_{21} | \Sigma_{22}
            \end{bmatrix} 
            ]$
            \item The conditional Gaussian distribution is given by:
            $p(a_2 | a_1 = z) = \mathcal{N} (u_2 + \Sigma_{21} \Sigma_{11}^{-1} (z-u_1), \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{12})$
        \end{itemize}
        \item Then, we get $p(y_{n+1}) = \mathcal{N} ( \boldsymbol{k}^\intercal \boldsymbol{C}_n^{-1} \boldsymbol{y}, c - \boldsymbol{k}^\intercal \boldsymbol{C}_n^{-1} \boldsymbol{k} )$
    \end{itemize}
    \item A new instance vector follows the distribution $p(\boldsymbol{y}_{*}) = \mathcal{N} ( \boldsymbol{K}_{*n} \boldsymbol{K}_{nn}^{-1} \boldsymbol{y}_{n}, \boldsymbol{K}_{**} - \boldsymbol{K}_{*n} \boldsymbol{K}_{nn}^{-1} \boldsymbol{K}_{n*} )$ where 
    \begin{itemize}
        \item Subscript $*$ indicates new instances, subscript $n$ indicates old instances
        \item $\boldsymbol{K} $is the kernel matrix, e.g. $\boldsymbol{K}_{*n}$ has new instances on rows and old instances on columns 
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Algorithm} --- 
\begin{enumerate}
    \item Compute kernel matrix based on observed data
    \item Compute kernel vector based on observed data and new instance
    \item Calculate mean and variance of predicted distribution
    \item Return predicted distribution
\end{enumerate}

\section{Kernel SVM}
\subsection*{Description}
\emph{Task} --- Classification

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Optimization} --- 
\begin{itemize}
    \item Cost function $argmin_\beta \frac{1}{2} \| \boldsymbol{\beta} \|^2 + C \sum_{i=1}^n \xi^{(i)}$ resp. dual Lagrangian objective function $argmax_\alpha \sum_{i=1}^n \alpha^{(i)} - \frac{1}{2}  \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \varphi(\boldsymbol{x}^{(i)}) \cdot \varphi(\boldsymbol{x}^{(j)})$ of soft-margin SVM is eligible for kernel trick
    \item Thus, $\boldsymbol{\beta}^{*} = \sum_{i=1}^n \tilde{\alpha}^{(i)} \varphi(\boldsymbol{x}^{(i)})$ where $\tilde{\alpha}$ refers to Kernel theorem
    \item Given general Lagrangian solution, $\boldsymbol{\beta}^{*} = \sum_{i=1}^n \alpha^{(i)} y^{(i)} \varphi(\boldsymbol{x}^{(i)}) $ where $\alpha$ refers to Lagrange multiplier, we know that: $\tilde{\alpha}^{(i)} = \alpha^{(i)} y^{(i)}$
    \item Outcome of novel instance can be predicted as: $\boldsymbol{\beta}^{*} \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} y^{(i)} \varphi(\boldsymbol{x}^{(i)}) \cdot \varphi({\boldsymbol{z}}) = \sum_{i=1}^n \alpha^{(i)} y^{(i)} k(\boldsymbol{x^{(i)}}, \boldsymbol{z})$
\end{itemize}