\section{Part of Speech (POS) Tagging with Conditional Random Field (CRF)}
\subsection*{Description}
\emph{Task} --- 
\begin{itemize}
    \item Tag parts of speech, e.g., adverb, verb, noun, etc.
    \item \emph{Structured prediction task:} Predict over exponentially (or even infinitely) large set of classes
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\begin{itemize}
    \item Input: Sequence of words $w$
    \item Output: Sequence of tags $t \in T$
    \item Can be considered a shortest or highest-scoring path search problem on a \emph{trellis}:
    \begin{itemize}
        \item Directed Acyclic Graph (DAG), where nodes are divided into vertical slices and each slice is associated with a timestamp
        \item Each node corresponds to a distinct state
        \item Each arrow represents a transition to a new state in the next slice
    \end{itemize}
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Point of departure} --- 
\begin{itemize}
    \item Log-linear model:
    $
    p(t \mid w) = \frac{\exp(\textrm{score}(t, w))}{\sum_{t'} \exp(\textrm{score}(t', w))}
    $
    \item Challenge: Naively computing the normalizer $\sum_{t'} \exp(\textrm{score}(t', w))$ would take exponential time $\mathcal{O}(|T|^N)$, where $N = |w|$ is the length of the sentence
    \item Solution: Consider a scoring function that is additively decomposable over tag bigrams:
    $
    \textrm{score}(t, w) = \sum_{n=1}^N \textrm{score}(\langle t_{n-1}, t_n \rangle, w)
    $
    where $t_0 = \textrm{BOS}$ tag and $t_{N} = \textrm{EOS}$ tag
    \item Then, we have:
    $
    p(t \mid w) = \frac{\exp\left( \sum_{n=1}^N \textrm{score}(\langle t_{n-1}, t_n \rangle, w) \right)}{\sum_{t'} \exp\left( \sum_{n=1}^N \textrm{score}(\langle t_{n-1}', t_n' \rangle, w) \right)}
    $
    \item If we take the denominator:
    \begin{itemize}
        \item $
        \sum_{t_1:t_N} \exp\left( \sum_{n=1}^N \dots \right) = \sum_{t_1:t_N} \prod_{n=1}^N \exp(\dots) = \sum_{t_1:t_{N-1}} \sum_{t_N} \prod_{n=1}^N \exp(\dots)
        $ since last tag does not depend on predecessors
        \item $ = \sum_{t_1:t_{N-1}} \prod_{n=1}^{N-1} \exp(\dots) \sum_{t_N} \prod_{n=N}^{N} \exp\left(\textrm{score}(\langle t_{n-1}, t_n \rangle, w) \right) 
        $ based on distributivity of $\otimes$ over $\oplus$
        \item $ = \sum_{t_1:t_{N-1}} \prod_{n=1}^{N-1} \exp(\dots) \sum_{t_N} \exp\left(\textrm{score}(\langle t_{N-1}, t_N \rangle, w) \right) 
        $
        \item $ = \sum_{t_1} [ \exp\left(\textrm{score}(\langle t_0, t_1 \rangle, w) \right) \times \sum_{t_2} [ \exp(\dots) \times \dots \times \sum_{t_N} \exp\left(\textrm{score}(\langle t_{N-1}, t_N \rangle, w) \right)]]
        $
    \end{itemize}
    \item We've moved from an exponential number of terms to a linear number of terms
    \item We go from having a score for each path in the trellis to a score for each edge
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Backward algorithm} --- 
\begin{itemize}
    \item Algorithm for computing the denominator
    \begin{enumerate}
        \item For $t_{N-1}$:
        $
        \beta(w, t_{N-1}, N-1) \gets \exp(\textrm{score}(\langle t_{N-1}, \textrm{EOS} \rangle, w))
        $
        \begin{itemize}
            \item Handles scores of edges incoming to EOS
            \item $\beta$ is memoized
        \end{itemize}
        \item For $n \in N-2, \dots, 1$:\\
        For $t_n \in T$:
        $
        \beta(w, t_n, n) \gets \sum_{t_{n+1}} \exp(\textrm{score}(\langle t_n, t_{n+1} \rangle, w)) \times \beta(w, t_{n+1}, n+1)
        $
        \begin{itemize}
            \item Handles scores over normal edges
            \item $\beta$ is memoized
        \end{itemize}
        \item For $t_1$:
        $
        \beta(w, t_0, 0) \gets \beta(w, t_0, 0) + \exp(\textrm{score}(\langle t_0, t_1 \rangle, w)) \times \beta(w, t_1, 1)
        $
        \begin{itemize}
            \item Handles scores of edges outgoing from BOS
            \item $\beta$ is returned
        \end{itemize}
    \end{enumerate}
    \item $\beta(w, t_n, n)$ are \emph{backward variables} that contain the sum of the scores of all paths starting at EOS and ending at tag $t_n$ in position $n$.
    \item Therefore:
    $
    \textrm{denominator} = \beta(w, t_0, 0) = \beta(w, \textrm{BOS}, 0)
    $
    \item Complexity:
    \begin{itemize}
        \item Time complexity: $\mathcal{O}(N|T|^2)$, given that we compute $|T| \cdot N$ backward variables and for each of them, compute the sum over $|T|$ successor tags
        \item Space complexity: $\mathcal{O}(N|T|)$, since we have to keep $|T| \cdot N$ backward variables in memory
    \end{itemize}
    \item Alternatively, the same can be achieved with a \emph{forward algorithm}, starting from BOS $t_0$ and going forward towards EOS $t_N$
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Viterbi algorithm} --- 
\begin{itemize}
    \item Motivation:
    \begin{itemize}
        \item Let us now assume we are not interested in the exact probability $p(t \mid w)$, but only in which sequence of tags $t$ has the highest probability
        \item Then we can ignore the denominator and have:
        $
        p(t \mid w) \propto \exp(\textrm{score}(t, w))
        $
        \item We need to solve:
        $
        t^* = argmax_t \exp(\textrm{score}(t, w)) = argmax_t \prod_{n=1}^N \exp(\textrm{score}(\langle t_{n-1}, t_n \rangle, w))
        $
    \end{itemize}
    \item Algorithm for computing the score of the best tag sequence $t^*$ and recovering the sequence itself
    \begin{enumerate}
        \item For $t_{N-1}$:
        $
        v(w, t_{N-1}, N-1) \gets \exp(\textrm{score}(\langle t_{N-1}, \textrm{EOS} \rangle, w))
        $
        \item For $n \in N-2, \dots, 1$:\\
        For $t_n \in T$:
        \begin{itemize}
            \item
            $
            v(w, t_n, n) \gets \max_{t_{n+1} \in T} \left[ \exp(\textrm{score}(\langle t_n, t_{n+1} \rangle, w)) \times v(w, t_{n+1}, n+1) \right]
            $ for each tag $t_n$, stores the score of the next best tag $t_{n+1}$ 
            \item $
            b(t_n, n) \gets argmax_{t_{n+1} \in T} \left[ \exp(\textrm{score}(\langle t_n, t_{n+1} \rangle, w)) \times v(w, t_{n+1}, n+1) \right]
            $ for each tag $t_n$, stores the tag $t_{n+1}$ that gave the best score
        \end{itemize}
        \item For $t_1$:
        \begin{itemize}
            \item $
            v(w, t_0, 0) \gets \max_{t_1} \left[ v(w, t_0, 0), \exp(\textrm{score}(\langle t_0, t_1 \rangle, w)) \right] \times v(w, t_1, 1)
            $
            \item $
            b(t_0, 0) \gets argmax_{t_1} \left[ v(w, t_0, 0), \exp(\textrm{score}(\langle t_0, t_1 \rangle, w)) \right] \times v(w, t_1, 1)
            $
        \end{itemize}
        \item For $n = 1, \dots, N$: Recover the best sequence using backpointers, by always plugging in last $t$:
        $
        t_n \gets b(t_{n-1}, n-1)
        $
        \item Return $t_{1:N}$ and $v(w, t_0, 0)$
    \end{enumerate}
    \item $b(t_n, n)$ are \emph{backpointers} that point to the $t_{n+1}$ tag in $t^*$
    \item $v(w, t_n, n)$ are \emph{Viterbi variables} that contain the score of $t^*$ starting at EOS and ending with tag $t_n$ in position $n$
    \item Complexity:
    \begin{itemize}
        \item For scoring function over tag bigrams: $\mathcal{O}(N |T|^2)$
        \item For scoring function over tag trigrams: $\mathcal{O}(N |T|^3)$
        \item For scoring function over $k$-order dependencies: $\mathcal{O}(N |T|^{k+1})$
    \end{itemize}
    \item Alternatively, the same can be achieved with a \emph{forward Viterbi algorithm}, starting from BOS $t_0$ and going forward towards EOS $t_N$
    \item Then the backpointers point to the $t_{n-1}$ tag in $t^*$
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Dijkstraâ€™s algorithm} --- 
\begin{itemize}
    \item Alternative to the Viterbi algorithm 
    \begin{enumerate}
        \item Initialize:
        \begin{itemize}
            \item Popped $\gets \{ \}$.
            \item Queue $\gets$ PriorityQueue() (returns best scores first)
            \item $\gamma \gets -\infty$
        \end{itemize}

        \item Push $\langle\langle 0, \textrm{BOS} \rangle, 0\rangle$ to queue
    
        \item While $|\textrm{queue}| > 0$:
        \begin{enumerate}
            \item Pop $\langle \langle n, t \rangle, \textrm{score} \rangle$ from queue, add $\langle \langle n, t \rangle, \textrm{score} \rangle$ to popped, 
            $
            \gamma[n, t] \gets \textrm{score}
            $
            \item If $n = |w|$ or stop early: Return score    
            \item If $n < |w|$:\\
            For $t' \in T$:\\
            If $\langle n+1, t' \rangle$ is not in popped:
            Push:
            $
            \langle \langle n+1, t' \rangle, \textrm{score}(t, t', w) + \gamma[n, t] \rangle
            $
            to queue
        \end{enumerate}
        \item Return $\gamma$
    \end{enumerate}
    \item Complexity:
    \begin{itemize}
        \item For scoring function over tag bigrams: $\mathcal{O}(N |T|^2 \log(N |T|))$, where $\log(N |T|)$ stems from push and update operations on the queue
        \item Dijkstra is generally slower than Viterbi, unless we leverage early stopping conditions
    \end{itemize}
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Generalized algorithm} --- 
\begin{itemize}
    \item A more general formulation of backward, Viterbi, and Dijkstra algorithms using semirings
    \item Backward algorithm: Uses inside semiring:
    $
    \langle \mathbb{R}_0^+ \cup \{+\infty\}, +, \times, 0, 1 \rangle
    $
    \item Viterbi algorithm: 
    \begin{itemize}
        \item Uses Viterbi semiring:
        $
        \langle [0, 1], \max, \times, 0, 1 \rangle
        $
        \item Challenge: Multiplying probabilities for long sequences may cause numbers to go to $0$, leading to numerical underflow
        \item Solution: Revert to log probabilities with the semiring:
        $
        \langle \mathbb{R} \cup \{-\infty\}, \max, +, -\infty, 0 \rangle.
        $
    \end{itemize}
    \item Dijkstra's algorithm: Uses arctic semiring:
    $
    \langle \mathbb{R} \cup \{-\infty\}, \max, +, -\infty, 0 \rangle
    $
    \item In addition to probabilities (as we calculate in the inside/Viterbi/log/arctic semirings), we can compute entropy using the expectation semiring:
    \begin{itemize}
        \item Expectation semiring given by $
        \langle \mathbb{R} \times \mathbb{R}, \oplus, \otimes, \underline{0}, \underline{1} \rangle
        $ where:
        \begin{itemize}
            \item
            $
            \langle x, y \rangle \oplus \langle x', y' \rangle = \langle x + x', y + y' \rangle
            $
            \item $
            \langle x, y \rangle \otimes \langle x', y' \rangle = \langle x \cdot x', x \cdot y' + x' \cdot y \rangle$
            \item $\underline{0} = \langle 0,0 \rangle$
            \item $\underline{1} = \langle 1,0 \rangle$
        \end{itemize}
        \item Instead of $\omega = \exp(\textrm{score}(\langle t_{n-1}, t_n \rangle, w))$, we do:
        $
        \omega = \langle w, -\omega \log(w) \rangle
        $
        as the weight for each arc in the CRF
        \item Running the backward algorithm with these weights and in the expectation semiring, we can additionally compute unnormalized entropy:
        $
        H_u = -\sum_{t \in T^N} \exp(\textrm{score}(t, w)) \cdot \textrm{score}(t, w)
        $
        \item We can then compute the normalized entropy as: 
        $
        H = Z^{-1} H_u + \log(Z)
        $
    \end{itemize}
\end{itemize}

{\color{lightgrey}\hrule height 0.001mm}

\emph{Scoring functions} --- 
\begin{itemize}
    \item So far, we only imposed the requirement that the scoring function is additively decomposable, but we have not specified it further
    \item \emph{Hidden Markov Model}:
    $
    \textrm{score} = \textrm{transition}(t_{n-1}, t_n) + \textrm{emission}(t_n, w_n) =
    $ transition probability (tag-tag pairs) $+$ emission probability (word-tag pairs, excl. BOS and EOS)
    \item A more complex example of scoring functions is a neural network with inputs $\langle t_{n-1}, t_n \rangle$ and $w$
    \item E.g. for the sentence 'The girl drinks':
    \begin{itemize}
        \item $\textrm{score}(w = \textrm{The girl drinks}, t = \langle D, N, V \rangle) = \sum_{n=1}^{n+1} \textrm{score}(\langle t_{n-1}, t_n \rangle, w)$
        \item $= \textrm{score}(\langle \textrm{BOS}, D \rangle, w) + \textrm{score}(\langle D, N \rangle, w) + \textrm{score}(\langle N, V \rangle, w) + \textrm{score}(\langle V, \textrm{EOS} \rangle, w)$
        \item Combination of emission and transition features: $(w_1 = \textrm{The}, y_1 = D) + (y_1 = D, y_0 = \textrm{BOS}) + (w_2 = \textrm{girl}, y_2 = N) + (y_2 = N, y_1 = D) + (w_3 = \textrm{drinks}, y_3 = V) + (y_3 = V, y_2 = N) + (y_4 = \textrm{EOS}, y_3 = V)$
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Objective function} --- 
\begin{itemize}
    \item Assume we have a dataset of $K$ iid data points $( w^{(i)}, t^{(i)} )$
    \item Log Likelihood:
    $
    \sum_{k=1}^K \log(p(t^{(k)} \mid w^{(k)})) = \sum_{k=1}^K \log \left( \frac{\exp(\textrm{score}(t^{(k)}, w^{(k)}))}{\sum_{t'} \exp(\textrm{score}(t', w^{(k)}))} \right) = \sum_{k=1}^K \left[ \textrm{score}(t^{(k)}, w^{(k)}) - \log \sum_{t'} \exp(\textrm{score}(t', w^{(k)})) \right]
    $
    \item Risk of overflow since sum makes things big and log makes things small
    \item If we want to use a temperature parameter $T$, we have:
    $
    \sum_{k=1}^K \log(p(t^{(k)} \mid w^{(k)})) = \sum_{k=1}^K \log \left( \frac{\exp(\textrm{score}(t^{(k)}, w^{(k)})) / T}{\sum_{t'} \exp(\textrm{score}(t', w^{(k)})) / T} \right) = \sum_{k=1}^K \left[ \textrm{score}(t^{(k)}, w^{(k)}) - T \log \sum_{t'} \exp\left(\frac{\textrm{score}(t', w^{(k)})}{T}\right) \right]
    $
    \item As $T \to 0$:
    $
    \sum_{k=1}^K \left[ \textrm{score}(t^{(k)}, w^{(k)}) - \max_{t'} \textrm{score}(t', w^{(k)}) \right]
    $
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Gradient can be computed using backpropagation linear in $N$, given the Viterbi algorithm for forward propagation linear in $N$
    \item Can be optimized using subgradient descent, since $\max$ is not differentiable everywhere
    \item Because the linear objective is convex, subgradient descent will converge
\end{itemize}
