\section{GMM}
\subsection*{Description}
\emph{Task} --- Clustering

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Unsupervised
    \item Non-parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item Instances $\{\boldsymbol{x^{(i)}}\}_{i=1}^n$
    \item Each instance has a latent cluster assignment given by: $z^{(i)} \in \{1,...,k\}$
    \item Probability that cluster assigned to instance i is cluster j is given by: $\pi^{[j]} = p(z^{(i)} = j)$ 
    \item Contingent on cluster assignment, each instance is the outcome of a random variable associated with a given cluster: $\boldsymbol{x^{(i)}} | z^{(i)} = j \sim \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}})$ where $\boldsymbol{\mu^{[j]}}$ is the mean and $\boldsymbol{\Sigma^{[j]}}$ is the covariance associated with cluster $j$
    \item Then, marginal distribution of each instance is given by: $p(\boldsymbol{x^{(i)}}) = \sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]}$
    \item This is the GMM, characterized by parameters $\{ \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}, \pi^{[j]} \}_{j=1}^k$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\{\boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}, \pi^{[j]}\}_{j=1}^k$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Maximize likelihood $L = \sum_{i=1}^n log( \sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} ) $ subject to $\sum_{j=1}^k \pi^{[j]} = 1$ and $\boldsymbol{\Sigma^{[j]}} \succ 0$
    \item This is a constrained, not concave, not analytically solvable optimization problem
    \item Temporarily assume we know which cluster each instance is associated with
    \item Let us define a distribution $q$ over ${1,...,k}$: $q(z^{(i)}) = p( z^{(i)} = j | \boldsymbol{x^{(i)}}, \theta^{(t)} ) = \frac{ \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j](t)}}, \boldsymbol{\Sigma^{[j](t)}}) \times \pi^{[j](t)} }{\sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j](t)}}, \boldsymbol{\Sigma^{[j](t)}}) \times \pi^{[j](t)} }$
    \item Then, we can rewrite log likelihood as: $L = \sum_{i=1}^n log( \sum_{j=1}^k q(z^{(i)}) \frac{ \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} }{ q(z^{(i)}) } )$
    \item According to Jensen's inequality: $L = \sum_{i=1}^n log( \sum_{j=1}^k q(z^{(i)}) \frac{ \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} }{ q(z^{(i)}) } ) \geq \sum_{i=1}^n \sum_{j=1}^k q(z^{(i)}) log( \frac{ \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} }{ q(z^{(i)}) } )$
    \item RHS can be rewritten: $\mathbb{E}_q[log(p_{\theta}(\boldsymbol{x^{(i)}}))] = \mathbb{E}_q[log(\frac{ p_{\theta}(\boldsymbol{x^{(i)}}, \boldsymbol{z^{(i)}}) }{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) } \frac{ q(z^{(i)}) }{ q(z^{(i)}) } )] = \mathbb{E}_q[ log( \frac{ p_{\theta}(\boldsymbol{x^{(i)}}, \boldsymbol{z^{(i)}}) } { q(z^{(i)}) } ) ] + \mathbb{E}_q[ log(\frac{ q(z^{(i)}) }{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) }) ] = M + E$
    \item $E$ corresponds to the KL divergence between $q(z^{(i)})$ and $p( z^{(i)} = j | \boldsymbol{x^{(i)}} )$
    \item $L \geq M \Leftrightarrow E \geq 0$, which we can show to be the case: 
    \begin{itemize}
        \item $E = \mathbb{E}_q[ log(\frac{ q(z^{(i)}) }{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) }) ] = \mathbb{E}_q[ -log(\frac{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }) ]$
        \item According to Jensen's inequality: $E \geq -log(\mathbb{E}_q[\frac{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }]) = -log(\sum_{i=1}^k q(z^{(i)} \frac{ p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }) = -log(\sum_{i=1}^k p_{\theta}(\boldsymbol{z^{(i)}} | \boldsymbol{x^{(i)}})) = -log(1) = 0$
    \end{itemize}
    \item $L = M \Leftrightarrow E = 0$, i.e. when $q(z^{(i)}) = p( z^{(i)} = j | \boldsymbol{x^{(i)}}, \theta^{(t)} )$
    \item Then, we have a lower bound on L, provided by M, with equality to M, if we set q correspondingly
    \item $M$ is tractable to optimize, since the logarithm now only contains a product, not a sum, and can be decomposed: $log(p_{\theta}(\boldsymbol{x^{(i)}}, \boldsymbol{z^{(i)}})) = log( \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} ) = log(\mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}})) + log(\pi^{[j]})$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} --- \emph{Expectation maximization algorithm}
\begin{enumerate}
    \item Randomly initialize $\theta^{(t)} = \{ \boldsymbol{\mu^{[j](t)}}, \boldsymbol{\Sigma^{[j](t)}}, \pi^{[j](t)} \}_{j=1}^k$
    \item \emph{E-step}: Minimize $E$, by computing $q(z^{(i)})$ given $\boldsymbol{x^{(i)}}$ and $\theta^{(t)}$
    \item \emph{M-step}: Maximize $M$, by updating $\theta^{(t)}$ based on MLE for Gaussians, while keeping $q(z^{(i)})$ fixed:
    \begin{itemize}
        \item $\boldsymbol{\mu^{[j](t+1)}} = \frac{ \sum_{i=1}^n q(z^{(i}) \boldsymbol{x^{(i)}} }{ \sum_{i=1}^n q(z^{(i}) }$
        \item $\boldsymbol{\Sigma^{[j](t+1)}} = \frac{ \sum_{i=1}^n q(z^{(i}) ( \boldsymbol{x^{(i)}} - \boldsymbol{\mu^{[j](t+1)}} )( \boldsymbol{x^{(i)}} - \boldsymbol{\mu^{[j](t+1)}} )^\intercal }{ \sum_{i=1}^n q(z^{(i}) }$
        \item $\pi^{[j](t+1)} = \frac{1}{n} \sum_{i=1}^n q(z^{(i})$
    \end{itemize}
    \item Repeat 2 and 3 until convergence
\end{enumerate}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Not convex
    \item May converge to local minimum
    \item Not analytically solvable
    \item Always converges, since $L \geq M$ and $M^{(t+1)} \geq M^{(t)}$ due to maximizing over M at each step
\end{itemize}