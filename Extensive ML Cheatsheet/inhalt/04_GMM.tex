\section{Gaussian Mixture Models (GMM)}
\subsection*{Description}
\emph{Task} --- Clustering

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Unsupervised
    \item Non-parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item Instances $\{\boldsymbol{x^{(i)}}\}_{i=1}^n$
    \item Each instance has a latent cluster assignment given by: $z^{(i)} \in \{1,...,k\}$
    \item Probability that cluster assigned to instance i is cluster j is given by: $\pi^{[j]} = p(z^{(i)} = j)$ 
    \item $z^{(i)} \sim \textrm{Categorical}(\pi^{[1]}, ..., \pi^{[k]})$
    \item $\pi^{[j]} \sim \textrm{Dir}(\alpha_1, ..., \alpha_k)$
    \item Contingent on cluster assignment, each instance is the outcome of a random variable associated with a given cluster: $\boldsymbol{x^{(i)}} | z^{(i)} = j \sim \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}})$ where $\boldsymbol{\mu^{[j]}}$ is the mean and $\boldsymbol{\Sigma^{[j]}}$ is the covariance associated with cluster $j$
    \item Then, marginal distribution of each instance is given by: $p(\boldsymbol{x^{(i)}}) = \sum_{j=1}^k p(\boldsymbol{x^{(i)}} | z^{(i)}) p(z^{(i)}) = \sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]}$
    \item This is the GMM, characterized by parameters $\theta = \{ \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}, \pi^{[j]} \}_{j=1}^k$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\theta = \{\boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}, \pi^{[j]}\}_{j=1}^k$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Maximize likelihood $L = \sum_{i=1}^n \log p( \boldsymbol{x^{(i)}}) = \sum_{i=1}^n \log (\sum_{j=1}^k p(\boldsymbol{x^{(i)}} | z^{(i)}) p(z^{(i)})) = \sum_{i=1}^n \log( \sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} ) $ subject to $\sum_{j=1}^k \pi^{[j]} = 1$ and $\boldsymbol{\Sigma^{[j]}} \succ 0$
    \item Challenge: This is a constrained, not concave, not analytically solvable optimization problem, due to the sum within the log
    \item Solution: Expectation maximization algorithm, which we now motivate 
    \item Let's temporarily assume we know which cluster each instance is associated with, i.e. we know $z^{(i)}$
    \item We can rewrite likelihood from $L = \sum_{i=1}^n \log p_\theta( \boldsymbol{x^{(i)}})$ to $L = \sum_{i=1}^n \log \sum_{z^{(i)}} p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})$
    \item For each instance, let $q_{z^{(i)}}$ be some probability distribution over $z^{(i)}$
    \item Then, we can further expand likelihood: $L = \sum_{i=1}^n \log \sum_{z^{(i)}} q(z^{(i)}) \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{q(z^{(i)})}$
    \item According to Jensen's inequality: $L = \sum_{i=1}^n \log \sum_{z^{(i)}} q(z^{(i)}) \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{q(z^{(i)})} \geq \sum_{i=1}^n \sum_{z^{(i)}} q(z^{(i)}) \log \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{q(z^{(i)})}$
    \item We have thus derived a lower bound for $L$
    \item We can tighten this bound and achieve equality by selecting $q_{z^{(i)}}$ accordingly: $q_{z^{(i)}} = $\\
    Proof 1:
    \begin{itemize}
        \item $\sum_{i=1}^n \sum_{z^{(i)}} q(z^{(i)}) \log \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{q(z^{(i)})} = \sum_{i=1}^n \sum_{z^{(i)}} p_\theta( z^{(i)} |\boldsymbol{x^{(i)}}) \log \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{p_\theta( z^{(i)} |\boldsymbol{x^{(i)}})} = \sum_{i=1}^n \sum_{z^{(i)}} p_\theta( z^{(i)} |\boldsymbol{x^{(i)}}) \log \frac{p_\theta( \boldsymbol{x^{(i)}}) p_\theta( z^{(i)} |\boldsymbol{x^{(i)}})}{p_\theta( z^{(i)} |\boldsymbol{x^{(i)}})} = \sum_{i=1}^n \sum_{z^{(i)}} p_\theta( z^{(i)} |\boldsymbol{x^{(i)}}) \log p_\theta( \boldsymbol{x^{(i)}}) = \sum_{i=1}^n \times 1 \times \log p_\theta( \boldsymbol{x^{(i)}}) = L$
    \end{itemize}
    Proof 2:
    \begin{itemize}
        \item $L = \mathbb{E}_q[log(p_{\theta}(\boldsymbol{x^{(i)}}))] = \mathbb{E}_q[log(\frac{ p_{\theta}(\boldsymbol{x^{(i)}}, z^{(i)}) }{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) } \frac{ q(z^{(i)}) }{ q(z^{(i)}) } )] = \mathbb{E}_q[ log( \frac{ p_{\theta}(\boldsymbol{x^{(i)}}, z^{(i)}) } { q(z^{(i)}) } ) ] + \mathbb{E}_q[ log(\frac{ q(z^{(i)}) }{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) }) ] = M + E$
    \end{itemize}
    \item $E$ corresponds to the KL divergence between $q(z^{(i)})$ and $p( z^{(i)} | \boldsymbol{x^{(i)}} )$
    \item $L \geq M \Leftrightarrow E \geq 0$, which we can show to be the case: 
    \begin{itemize}
        \item $E = \mathbb{E}_q[ log(\frac{ q(z^{(i)}) }{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) }) ] = \mathbb{E}_q[ -log(\frac{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }) ]$
        \item According to Jensen's inequality: $E \geq -log(\mathbb{E}_q[\frac{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }]) = -log(\sum_{i=1}^k q(z^{(i)}) \frac{ p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}}) }{ q(z^{(i)}) }) = -log(\sum_{i=1}^k p_{\theta}(z^{(i)} | \boldsymbol{x^{(i)}})) = -log(1) = 0$
    \end{itemize}
    \item $L = M \Leftrightarrow E = 0$, i.e. when $q(z^{(i)}) = p_\theta( z^{(i)} | \boldsymbol{x^{(i)}})$
    \item We can further specify $q_{z^{(i)}} = p_\theta( z^{(i)} |\boldsymbol{x^{(i)}})$ to: $q_{z^{(i)}} = p_\theta( z^{(i)} |\boldsymbol{x^{(i)}}) = \frac{ p_\theta( \boldsymbol{x^{(i)}} | z^{(i)}) p( z^{(i)}) }{ \sum_{j=1}^k p_\theta( \boldsymbol{x^{(i)}} | z^{(i)}) p( z^{(i)}) } = \frac{ \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} } { \sum_{j=1}^k \mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}}) \times \pi^{[j]} } = \gamma^{i[j]}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} --- \emph{Expectation maximization algorithm}
\begin{enumerate}
    \item Randomly initialize $\theta^{(t)} = \{ \boldsymbol{\mu^{[j](t)}}, \boldsymbol{\Sigma^{[j](t)}}, \pi^{[j](t)} \}_{j=1}^k$
    \item \emph{E-step}: Minimize $E$, by computing $q(z^{(i)})$ given $\boldsymbol{x^{(i)}}$ and $\theta^{(t)}$
    \item \emph{M-step}: Maximize $M$, by updating $\theta^{(t)}$ based on MLE for Gaussians, while keeping $q(z^{(i)})$ fixed:
    \begin{itemize}
        \item $\boldsymbol{\mu^{[j](t+1)}} = \frac{ \sum_{i=1}^n q(z^{(i}) \boldsymbol{x^{(i)}} }{ \sum_{i=1}^n q(z^{(i}) }$
        \item $\boldsymbol{\Sigma^{[j](t+1)}} = \frac{ \sum_{i=1}^n q(z^{(i}) ( \boldsymbol{x^{(i)}} - \boldsymbol{\mu^{[j](t+1)}} )( \boldsymbol{x^{(i)}} - \boldsymbol{\mu^{[j](t+1)}} )^\intercal }{ \sum_{i=1}^n q(z^{(i}) }$
        \item $\pi^{[j](t+1)} = \frac{1}{n} \sum_{i=1}^n q(z^{(i})$
    \end{itemize}
    \item Repeat 2 and 3 until convergence
\end{enumerate}

{\color{lightgray}\hrule height 0.001mm}

\emph{Further proofs} --- 
Recall: Likelihood is:
$
L = \sum_{i=1}^n \log \left( \sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right) \right)
$\\
Proof of optimal mean in M-step:
\begin{itemize}
    \item $ \nabla_{\boldsymbol{\mu}^{[j]}} L = \sum_{i=1}^n \frac{\delta L}{\delta \mathcal{N}(\ldots)} \times \frac{\delta \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}}
    $

    $
    = \sum_{i=1}^n \frac{\pi^{[j]}}{\sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)} \times \frac{\delta \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}}
    $

    $
    = \sum_{i=1}^n \frac{\pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)}{\sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)} \times \frac{\delta \log \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)}{\delta \boldsymbol{\mu}^{[j]}}
    $ 

    $
    \text{because } \frac{\delta \log \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}} = \frac{1}{\mathcal{N}(\ldots)} \times \frac{\delta \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}}
    $

    $
    \Rightarrow \frac{\delta \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}} = \mathcal{N}(\ldots) \times \frac{\delta \log \mathcal{N}(\ldots)}{\delta \boldsymbol{\mu}^{[j]}}
    $

    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \frac{\delta \log \left(\text{constant} \times \frac{1}{|\boldsymbol{\Sigma}|^{1|2}} \times \exp\left(-\frac{1}{2} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) \right)\right)}{\delta \boldsymbol{\mu}^{[j]}}
    $

    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \frac{\delta}{\delta \boldsymbol{\mu}^{[j]}} \left( \log(1) - \frac{1}{2} \log(|\boldsymbol{\Sigma}|) - \frac{1}{2} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) \right)
    $

    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \left( -\frac{1}{2} \times \frac{\delta}{\delta \boldsymbol{\mu}^{[j]}} \left[ \log(|\boldsymbol{\Sigma}|)+ (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) \right] \right)
    $

    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \left( -\frac{1}{2} \times 2\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) \times -1 \right)
    $ due to matrix calculus

    $
    = \sum_{i=1}^n \gamma^{i[j]} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) = 0
    $
        
    \item $
    = \sum_{i=1}^n \gamma^{i[j]} \boldsymbol{\Sigma}^{-1} \boldsymbol{x}^{(i)} = \sum_{i=1}^n \gamma^{i[j]} \boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}^{[j]}
    $

    \item $
    \boldsymbol{\mu}^{[j]*} = \frac{\sum_{i=1}^n \gamma^{i[j]} \boldsymbol{x}^{(i)}}{\sum_{i=1}^n \gamma^{i[j]}}
    $
\end{itemize}
Proof of optimal variance in M-step:
\begin{itemize}
    \item $ \nabla_{\boldsymbol{\Sigma}^{[j]}} L = ... \text{as above}
    $
    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \left( -\frac{1}{2} \times \frac{\delta}{\delta \boldsymbol{\Sigma}^{[j]}} \left[ \log(|\boldsymbol{\Sigma}|)+ (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]}) \right] \right)
    $
    $
    = \sum_{i=1}^n \gamma^{i[j]} \times \left( -\frac{1}{2} \times (\boldsymbol{\Sigma}^{-1} - \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal \boldsymbol{\Sigma}^{-1}) \right) = 0
    $ due to matrix calculus
    \item $
    \sum_{i=1}^n \gamma^{i[j]} \times \frac{1}{2} \times \boldsymbol{\Sigma}^{-1} =  \sum_{i=1}^n \gamma^{i[j]} \times \frac{1}{2} \times \boldsymbol{\Sigma}^{-2} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal
    $
    $
    \Rightarrow \sum_{i=1}^n \gamma^{i[j]} \boldsymbol{\Sigma} =  \sum_{i=1}^n \gamma^{i[j]} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal
    $
    \item $
    \boldsymbol{\Sigma}^{[j]*} = \frac{\sum_{i=1}^n \gamma^{i[j]} (\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})(\boldsymbol{x}^{(i)} - \boldsymbol{\mu}^{[j]})^\intercal}{\sum_{i=1}^n \gamma^{i[j]}}
    $
\end{itemize}
Proof of optimal $\pi$ in M-step:
\begin{itemize}
    \item \With Lagrangian optimization:
    $
    \arg\min_{\pi^{[j]}} L
    $ subject to $\sum_{j=1}^k \pi^{[j]} = 1$, i.e. $\sum_{j=1}^k \pi^{[j]} -1 = 0$

    \item Lagrangian: $
    \mathcal{L}(\pi^{[j]}) = \sum_{i=1}^n \log \left( \sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right) \right) + \lambda \left( \sum_{j=1}^k \pi^{[j]} - 1 \right)
    $

    \item Derivative:
    $
    \nabla_{\pi^{[j]}} \mathcal{L}(\pi^{[j]}) = \sum_{i=1}^n \frac{\sum_{j=1}^k \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)}{\sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)} + \lambda
    $

    $
    = \sum_{i=1}^n \sum_{j=1}^k \frac{\gamma^{i[j]}}{\pi^{[j]}} + \lambda = 0
    $

    \item $
    \sum_{i=1}^n \sum_{j=1}^k \gamma^{i[j]} = -\lambda \sum_{j=1}^k \pi^{[j]}
    $

    \item Solving for $\lambda$:

    $
    \frac{\sum_{i=1}^n \sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)}{\sum_{i=1}^n \sum_{j=1}^k \pi^{[j]} \mathcal{N}\left( \boldsymbol{x}^{(i)}; \boldsymbol{\mu}^{[j]}, \boldsymbol{\Sigma}^{[j]} \right)} = -\lambda \times 1
    $ due to constraint

    $
    n = -\lambda
    $

    \item Solving for $\pi^{[j]}$:
    $
    \pi^{[j]} = \frac{\sum_{i=1}^n \gamma^{i[j]}}{-\lambda}
    $\\
    Plugging in $\lambda$:
    $
    \pi^{[j]*} = \frac{1}{n} \sum_{i=1}^n \gamma^{i[j]}
    $
\end{itemize}
Proof that M-step objective is concave:
\begin{itemize}
    \item We aim top optimize $\log( p(\boldsymbol{x^{(i)}}, z^{(i)}) = \log( p(\boldsymbol{x^{(i)}} | z^{(i)})p(z^{(i)}) = \log(\mathcal{N}(\boldsymbol{x^{(i)}} | \boldsymbol{\mu^{[j]}}, \boldsymbol{\Sigma^{[j]}})) + \log(\pi^{[j]})$
    \item This is a sum of concave functions
\end{itemize}
Proof that EM-algorithm converges:
\begin{itemize}
    \item According to Jensen's inequality: $L_t \geq \sum_{i=1}^n \sum_{z^{(i)}} q_t(z^{(i)}) \log \frac{p_{\theta_t}( \boldsymbol{x^{(i)}}, z^{(i)})}{q_t(z^{(i)})}$
    \item In previous M-step, $L$ was maximized by setting $\theta_t$: $\theta_t = argmax_\theta (q_t(z^{(i)}) \log \frac{p_\theta( \boldsymbol{x^{(i)}}, z^{(i)})}{q_t(z^{(i)})})$
    \item Thus, $\sum_{i=1}^n \sum_{z^{(i)}} q_t(z^{(i)}) \log \frac{p_{\theta_t}( \boldsymbol{x^{(i)}}, z^{(i)})}{q_t(z^{(i)})} \geq \sum_{i=1}^n \sum_{z^{(i)}} q_t(z^{(i)}) \log \frac{p_{\theta_{t-1}}( \boldsymbol{x^{(i)}}, z^{(i)})}{q_t(z^{(i)})}$
    \item RHS of this equation is output of previous E-step, where $q_t$ was set such that: $\sum_{i=1}^n \sum_{z^{(i)}} q_t(z^{(i)}) \log \frac{p_{\theta_{t-1}}( \boldsymbol{x^{(i)}}, z^{(i)})}{q_t(z^{(i)})} = L_{t-1}$
    \item Then $L_t \geq L_{t-1}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Not convex
    \item May converge to local minimum
    \item Not analytically solvable
    \item Always converges, since $L \geq M$ and $M^{(t+1)} \geq M^{(t)}$ due to maximizing over M at each step
\end{itemize}