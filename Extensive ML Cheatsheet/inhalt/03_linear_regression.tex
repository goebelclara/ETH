\section{Linear Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$ where $\boldsymbol{X}$ contains $n$ rows, each of which represents an instance, and $m$ columns, each of which represents a feature
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item $\boldsymbol{\beta}$ lies in the rowspace of $\boldsymbol{X}$ resp. columnspace of $\boldsymbol{X}^\intercal$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
Ordinary least squares estimator (OLSE):
\begin{itemize}
    \item Minimize mean squared error: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )$
\end{itemize}
Orthogonality principle:
\begin{itemize}
    \item Yields same result as OLSE
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item Then, by the orthogonality principle, $\boldsymbol{X} \cdot (\hat{\boldsymbol{y}} - \boldsymbol{y}) = \boldsymbol{X} \cdot (\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
    \item Alternatively, $\boldsymbol{\beta}$ lies in the columnspace of $\boldsymbol{X}^\intercal$
    \item Then, we can express $\boldsymbol{\beta}$ as $\boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$
    \item This yields an equation system $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$ which can be solved for $\alpha_i$
    \item On that basis, $\boldsymbol{\beta}$ can be calculated
\end{itemize}
PCA:
\begin{itemize}
    \item Yields same result as OLSE
    \item Instances $y^{(i)}, \boldsymbol{x}^{(i)} = \boldsymbol{\xi}^{(i)}$ can be projected onto hyperplane given by $\boldsymbol{X}\boldsymbol{\beta}$
    \item Projections are given by $\hat{\boldsymbol{\xi}}^{(i)}$
    \item Residuals are given by $e^{(i)} = \boldsymbol{\xi}^{(i)} - \hat{\boldsymbol{\xi}}^{(i)}$
    \item Since $e^{(i)}$ is orthogonal to $\hat{\boldsymbol{\xi}}^{(i)}$, we can write using Pythagorean theorem: $\| e^{(i)} \|^2 = \| \boldsymbol{\xi}^{(i)} \|^2 - \| \hat{\boldsymbol{\xi}}^{(i)} \|^2$
    \item This is a PCA via SVD problem
\end{itemize}
MLE:
\begin{itemize}
    \item Yields same result as OLSE
\end{itemize}
Gradient descent:
\begin{itemize}
    \item Minimum-norm solution
    \item Yields same result as OLSE
\end{itemize}
Pseudo Inverse:
\begin{itemize}
    \item Yields same result as OLSE
    \item Minimum-norm solution
    \item $\boldsymbol{\beta}$ minimizes MSE if $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item Given matrix projection via SVD, $\boldsymbol{X} \boldsymbol{X}^{\#} \boldsymbol{y}$ is that projection
    \item $\Rightarrow \boldsymbol{\beta} = \boldsymbol{X}^{\#} \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( y - \boldsymbol{\beta} \cdot \boldsymbol{x})^2 = (y - \boldsymbol{\beta} \cdot \boldsymbol{x})\boldsymbol{x} = 0$
    resp.     
    $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )) = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (\boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - 2 \boldsymbol{y}^\intercal \boldsymbol{X} \boldsymbol{\beta}) = \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^\intercal (\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Hypothesis Testing of Found Parameters} --- 
\begin{itemize}
    \item Let $\boldsymbol{y} | \boldsymbol{X} \sim \mathcal{N} (\boldsymbol{y}, \sigma^2 \boldsymbol{I}) = \mathcal{N} (\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I})$
    \item Let $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^+ \boldsymbol{y}$ be the OLSE where $\boldsymbol{X}^+$ is a scalar
    \item Then, $\boldsymbol{\hat{\beta}} \sim \mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{\beta}, (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \sigma^2 )$\\Proof:
    \begin{itemize}
        \item $\mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X}^{+ \intercal}  )$ since $\boldsymbol{X}^+$ is a scalar
        \item Further, we have $\mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ ((\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal)^\intercal  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1 \intercal}  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  )$ since $(\boldsymbol{X}^\intercal \boldsymbol{X})$ is symmetric
    \end{itemize}
    \item We can estimate $\sigma^2$ unbiasedly as: $\hat{\sigma}^2 = \frac{1}{n-m} \sum_{i \leq n} (\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y})^2$
    \item Then, confidence interval for $\hat{\beta_j}$ given by: $\hat{\beta_j} \pm z_{\alpha/2} \hat{se}(\hat{\beta_j})$ where 
    \begin{itemize}
        \item $z_{\alpha/2} = \Phi^{-1}(\alpha/2)$ is Gaussian CDF
        \item $\hat{se}(\hat{\beta_j})$ is the $j^{th}$ diagonal element of the covariance matrix $\sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$
    \end{itemize}
    \item We can perform a hypothesis test on $\hat{\beta}$ with the \emph{Wald test}:
    \begin{itemize}
        \item $H_0 : \beta = \beta_0$ (typically 0)\\
        $H_1 : \beta \neq \beta_0$
        \item Wald statistic: $W = \frac{\hat{\beta} - \beta_0}{\hat{se}}$
        \item If p-value associated with $W$ is smaller than $\alpha$ resp. if $|W|$ is greater than or equal to the critical value $z_{\alpha/2}$, we reject $H_0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Evaluation} ---
\begin{itemize}
    \item OLSE is unbiased if noise $\epsilon$ has zero mean:
    \begin{itemize}
        \item Given $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \epsilon$, we can substitute $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal (\boldsymbol{X}\boldsymbol{\beta} + \epsilon) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \epsilon$
        \item Taking the expected value on both sides, we have: $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \mathbb{E}(\epsilon)$
        \item Then, $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta}$ if the noise has zero mean
    \end{itemize}
    \item \emph{Gauss Markov theorem}: OLSE is best (lowest variance, lowest MSE) unbiased estimator, if assumptions ($\boldsymbol{X}$ is full rank and there is no multicollinearity, heteroskedasticity, and exogeneity) are met\\
    Proof:
    \begin{itemize}
        \item Let $\boldsymbol{A}^\intercal \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$ be the OLSE
        \item Let $\boldsymbol{C}^\intercal \boldsymbol{y}$ be another unbiased estimator
        \item $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \boldsymbol{A}^\intercal \mathbb{V}(\boldsymbol{y}) \boldsymbol{A}$ since $\boldsymbol{A}$ is constant
        \item We can further develop to: $\boldsymbol{A}^\intercal \sigma^2 \boldsymbol{I}_m \boldsymbol{A} = \sigma^2 \boldsymbol{A}^\intercal \boldsymbol{A}$ since variance is given by error term
        \item Similarly, $\mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y}) = \sigma^2 \boldsymbol{C}^\intercal \boldsymbol{C} $
        \item For the OLSE, we can plug in $(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal$ for $\boldsymbol{A}$ which yields: $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$ 
        \item Then, we have shown that $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) \leq \mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y})$
    \end{itemize}
    \item Nonetheless, there may be biased estimators that generate a lower variance and MSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically, if $(\boldsymbol{X}^\intercal \boldsymbol{X})$ is invertible
    \item If it has infinitely many solutions, the preferred solution is the \emph{minimum-norm solution}, which minimizes $\| \boldsymbol{\beta} \|$
\end{itemize}

\section{Linear Minimum Mean Squared Error Estimation (LMMSE)}
\subsection*{Description}
\begin{itemize}
    \item Minimizes mean squared error of two random variables, leveraging information about their mean and covariance 
    \item Linear regression with large samples approximates LMMSE 
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $\boldsymbol{y}$ is observed
    \item $\boldsymbol{x}$ is a row vector and quantity of interest
    \item We estimate $\boldsymbol{x}$ as $\hat{\boldsymbol{x}} = \boldsymbol{h}^\intercal \boldsymbol{Y} = \sum_i h_i \boldsymbol{y}_i$ where $\boldsymbol{X}$ contains $m$ rows, each of which represents the n-sized vector for a random variable
    \item This can be considered as a projection of $\boldsymbol{x}$ to the rowspace of $\boldsymbol{Y}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{h}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize expected squared error: $LO = \mathbb{E}[ | \hat{\boldsymbol{x}} - \boldsymbol{x} | ]$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item By the orthogonality principle, $\mathbb{E} [ ( \hat{\boldsymbol{x}} - \boldsymbol{x} ) \cdot \boldsymbol{y}_i ] = \mathbb{E} [ ( \sum_{l=1}^n h_l \boldsymbol{y}_l - \boldsymbol{x} ) \cdot \boldsymbol{y}_i ] = 0$ for $i = 1, ..., n$
    \item Then, $\sum_{l=1}^n \mathbb{E}[ \boldsymbol{y}_l \cdot \boldsymbol{y}_i ] h_l = \mathbb{E}[ \boldsymbol{x} \cdot \boldsymbol{y}_i ]$ for $i = 1, ..., n$ which in matrix notation corresponds to
    $\begin{bmatrix}
    \mathbb{E}[\boldsymbol{y}_1 \cdot \boldsymbol{y}_1] & ... & \mathbb{E}[\boldsymbol{y}_1 \cdot \boldsymbol{y}_n]\\
    ... & ... & ...\\
    \mathbb{E}[\boldsymbol{y}_n \cdot \boldsymbol{y}_1] & ... & \mathbb{E}[\boldsymbol{y}_n \cdot \boldsymbol{y}_n]\\
    \end{bmatrix} \cdot 
    \begin{bmatrix}
    h_1 \\
    ... \\
    h_n
    \end{bmatrix} = 
    \begin{bmatrix}
    \mathbb{E}[\boldsymbol{x} \cdot \boldsymbol{y}_1]\\
    ...\\
    \mathbb{E}[\boldsymbol{x} \cdot \boldsymbol{y}_n]
    \end{bmatrix}$
    resp. concisely
    $\boldsymbol{h}^\intercal \mathbb{E}[ \boldsymbol{Y} \boldsymbol{Y}^\intercal ] = \mathbb{E}[\boldsymbol{x}  \boldsymbol{Y}^\intercal]$
\end{itemize}