\section{Natural Language Processing (NLP) Basics}
\subsection*{Formulation}
\emph{Training data} --- 
\begin{itemize}
    \item $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{\beta}$
    \item For example: 
    $\begin{bmatrix}
    \textrm{score for doc 1} \\
    ...\\
    \textrm{score for doc n}
    \end{bmatrix}
    $ 
    $=
    \begin{bmatrix}
    \textrm{feature 1 for doc 1} ... \textrm{feature m for doc 1} \\
    ...\\
    \textrm{feature 1 for doc n} ... \textrm{feature m for doc n}
    \end{bmatrix}
    $
    $
    \times 
    \begin{bmatrix}
    \textrm{coefficient for feature 1} \\
    ...\\
    \textrm{coefficient for feature m}
    \end{bmatrix}
    $
    \item Where $\textrm{attribute j for doc i} = \boldsymbol{w}^{(i)} \cdot \boldsymbol{a}^{(i)} = 
    \begin{bmatrix}
    \textrm{weight of word 1 in doc i...weight of word n in doc i}
    \end{bmatrix}
    \times 
    \begin{bmatrix}
    \textrm{attribute j of vocab word 1} \\
    ...\\
    \textrm{attribute j of vocab word n}
    \end{bmatrix}
    $
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Terminology} --- 
\begin{itemize}
    \item Corpus
    \item Document:Contained in corpus, constitutes one of $n$ instances for model
    \item Tokens: Document split into preprocessed words, which are the tokens
    \item Vocabulary: Contains unique tokens in corpus
    \item Token-level features: 
    \begin{itemize}
        \item One-hot encoding: Vector of length of vocabulary, with $1$ at index of token and $0$ everywhere else
        \item Embeddings: Measure semantic and syntactic word similarities in higher dimensional space, e.g. Word2Vec, GloVE
    \end{itemize}
    \item Document-level features: Generated by pooling token-level features
    \item Pooling methods:
    \begin{itemize}
        \item Sum pooling: Sum up token-level features $e(\boldsymbol{d}) = \sum_{t \in d} e(\boldsymbol{t})$
        \item Mean pooling: Average token-level features $e(\boldsymbol{d}) = \frac{1}{|d|} \sum_{t \in d} e(\boldsymbol{t})$ with token weights:
        \begin{itemize}
            \item One-hot encoding: e.g., $[0, 1, 0]$
            \item Bag-of-words: e.g., $[1, 2, 0]$
            \item TF-IDF: Bag-of-words counts given by: $\frac{\textrm{vocab word frequency in document}}{\textrm{vocab word frequency in corpus}}$
        \end{itemize}
        \item Max pooling: Maximum of token-level features $e(\boldsymbol{d}) = \max_{t \in d} e(\boldsymbol{t})$
    \end{itemize}
\end{itemize}
