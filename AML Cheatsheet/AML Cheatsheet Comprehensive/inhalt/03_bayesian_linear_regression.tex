\section{Bayesian Linear Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} + \epsilon$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \epsilon$
    \item $\boldsymbol{\beta} \sim \mathcal{N} (0,T^2 \boldsymbol{I}_m)$
    \item $p(\boldsymbol{\beta}) \propto -\frac{1}{2T^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta}$
    \item $\epsilon \sim \mathcal{N} (0,\sigma^2)$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find distribution of parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} --- 
\begin{itemize}
    \item Prior $p( \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item $\boldsymbol{\beta} \sim \mathcal{N}(0,  T^2 \boldsymbol{I}_m)$
        \item $p( \boldsymbol{\beta} ) = \frac{ 1 }{ (2\pi T^2)^{m/2}} exp ( -\frac{1}{2 T^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta} ) $
    \end{itemize}
    \item Likelihood $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} )$: 
    \begin{itemize}
        \item Conditional on $\boldsymbol{\beta}$, $\boldsymbol{y} \sim \mathcal{N}(\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I}_n)$
        \item $p( \boldsymbol{y} | \boldsymbol{X}, \boldsymbol{\beta} ) = \frac{ 1 }{ (2\pi\sigma^2)^{n/2}} exp ( -\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) ) $
    \end{itemize}
    \item Posterior $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} )$: 
    \begin{itemize}
        \item $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto p ( \boldsymbol{y} |\boldsymbol{X}, \boldsymbol{\beta} ) \times p( \boldsymbol{\beta} ) \propto exp ( -\frac{1}{2\sigma^2} (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta})^\intercal (\boldsymbol{y} - \boldsymbol{X} \boldsymbol{\beta}) ) \times exp( -\frac{1}{2T^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta} ) = exp ( -\frac{1}{2} (\frac{1}{\sigma^2} \boldsymbol{y}^\intercal \boldsymbol{y} - 2 \boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{y} + \boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta}) + \frac{1}{2T^2} \boldsymbol{\beta}^\intercal \boldsymbol{\beta}) \propto exp ( -\frac{1}{2} (\boldsymbol{\beta}^\intercal (\frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{2T^2} \boldsymbol{I}_m) \boldsymbol{\beta} - \frac{2}{\sigma^2} \boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{y} )$
        \item We now apply a symmetric matrix property $\boldsymbol{x}^\intercal \boldsymbol{A} \boldsymbol{x} + 2 \boldsymbol{x}^\intercal \boldsymbol{b} = ( \boldsymbol{x} + \boldsymbol{A}^{-1} \boldsymbol{b} )^\intercal \boldsymbol{A} ( \boldsymbol{x} + \boldsymbol{A}^{-1} \boldsymbol{b} ) - \boldsymbol{b}^\intercal \boldsymbol{A}^{-1} \boldsymbol{b}$, with $\boldsymbol{\beta} = \boldsymbol{x}$, $(\frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{2T^2} \boldsymbol{I}_m) = \boldsymbol{A}$ and $(\frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{y}) = \boldsymbol{b}$
        \item Through this, we get $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \propto \exp (  \frac{1}{2}  ( \boldsymbol{\beta} + ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{T^2} \boldsymbol{I}_m )^{-1} ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{y} ) )^\intercal ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{T^2} \boldsymbol{I}_m ) ( \boldsymbol{\beta} + ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{T^2} \boldsymbol{I}_m )^{-1} ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{y} ) ) )$
        \item Thus, $p( \boldsymbol{\beta} | \boldsymbol{X}, \boldsymbol{y} ) \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ with
        \begin{itemize}
            \item $\boldsymbol{\mu} = \boldsymbol{\Sigma} \times \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{y}$
            \item $\boldsymbol{\Sigma} = ( \frac{1}{\sigma^2} \boldsymbol{X}^\intercal \boldsymbol{X} + \frac{1}{T^2} \boldsymbol{I}_m )^{-1}$
        \end{itemize}
        \item If we set an infinitely broad prior $T^2$ then the Bayesian estimate converges to the MLE estimate â€“ if we have n = 0 training instances, the Bayesian estimate reverts to the prior
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Can be solved analytically 
\end{itemize}