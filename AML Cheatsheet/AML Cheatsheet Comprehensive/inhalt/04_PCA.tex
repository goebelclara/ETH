\section{Principal Component Analysis (PCA)}
\subsection*{Maximize Variance Approach}
\subsection*{Description}
\emph{Task} --- Dimensionality reduction via projection, create uncorrelated features

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Unsupervised
    \item Non-parametric
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Overview} --- 
Identifies lower-dimensional subspace and projects data onto it such that the maximum amount of variance in the data is preserved. In lower-dimensional subspace:
\begin{itemize}
    \item Axes are called \emph{principal components}, where the first principal component is the axis accounting for the largest variance
    \item Each axis is given by an eigenvector with \emph{loadings}, indicating how much each variable in the original data contributes to this eigenvector
    \item Variance captured along each axis is given by the corresponding eigenvalue
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\begin{itemize}
    \item Project data $\{\boldsymbol{x^{(i)}}\}_{i=1}^n \in \mathbb{R}^m$ onto space $\mathbb{R}^d$ spanned by orthonormal basis $\{\boldsymbol{u^{[j]}}\}_{j=1}^d \in \mathbb{R}^m$ where $d << m$
    \item Each instance $\boldsymbol{x^{(i)}}$ is projected onto each basis vectors $\boldsymbol{u^{[j]}} \cdot \boldsymbol{x^{(i)}}$:  $\boldsymbol{x^{(i)}} \to [\boldsymbol{u^{[1]}} \cdot \boldsymbol{x^{(i)}} , ..., \boldsymbol{u^{[d]}} \cdot \boldsymbol{x^{(i)}}]^\intercal$
    \item Each basis vector $\boldsymbol{u^{[j]}}$ contains $m$ loadings $[u_i^{[j]}, ..., u_m^{[j]}]$, whose value indicates how important each feature $m$ is for the $j^{th}$ principal component
    \item Mean of projected data for a given basis vector: $\boldsymbol{u^{[j]}} \cdot \overline{\boldsymbol{x}} = \boldsymbol{u^{[j]}} \cdot \frac{1}{n} \sum_{i=1}^n \boldsymbol{x^{(i)}}$
    \item Variance of projected data for a given basis vector: $\frac{1}{n} \sum_{i=1}^n (\boldsymbol{u^{[j]}} \cdot \boldsymbol{x^{(i)}} - \boldsymbol{u^{[j]}} \cdot \overline{\boldsymbol{x}})^2 = \boldsymbol{u^{[j]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[j]}}$ where $\boldsymbol{S} = \frac{1}{n} \sum_{i=1}^n (\boldsymbol{x^{(i)}} - \overline{\boldsymbol{x}})(\boldsymbol{x^{(i)}} - \overline{\boldsymbol{x}})^\intercal = \frac{1}{n} \boldsymbol{X}^\intercal \boldsymbol{X}$ is the covariance matrix of the data in the orthogonal complement to the subspace spanned by the first $j-1$ principal components, i.e. $X_{j-1} = \{ \boldsymbol{x^{(i)}} - \textrm{projection}_{u_{\leq j-1}} \} = \{ \boldsymbol{x^{(i)}} - \sum_{l=1}^{j-1} (\boldsymbol{u^{[j]}} \cdot \boldsymbol{x^{(i)}} ) \cdot  \boldsymbol{u^{[j]}} \}$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find principal components $\{\boldsymbol{u^{[j]}}\}_{j=1}^d$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} ---
\begin{itemize}
    \item For the $j^{th}$ principal component, maximize variance $\sum_{j=1}^d \boldsymbol{u^{[j]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[j]}}$ subject to orthonormal $\{\boldsymbol{u^{[j]}}\}_{j=1}^d$
    \item Gives rise to Lagrangian formulation
    \item Lagrangian formulation for $\boldsymbol{u^{[1]}}$ capturing the most variance: $\mathcal{L} = \boldsymbol{u^{[1]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[1]}} - \lambda^{[1]} (\boldsymbol{u^{[1]}} \cdot \boldsymbol{u^{[1]}} - 1)$ where $\lambda^{[1]}$ captures the orthonormality constraint that $\boldsymbol{u^{[1]}} \cdot \boldsymbol{u^{[1]}} = 1$
    \item Lagrangian formulation for $\boldsymbol{u^{[2]}}$ capturing the secondmost variance: $\mathcal{L} = \boldsymbol{u^{[2]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[2]}} - \lambda^{[2]} (\boldsymbol{u^{[2]}} \cdot \boldsymbol{u^{[2]}} - 1) - \lambda^{[1][2]} (\boldsymbol{u^{[1]}} \cdot \boldsymbol{u^{[2]}} - 0)$ where $\lambda^{[1][2]}$ captures the orthogonality constraint that $\boldsymbol{u^{[1]}} \cdot \boldsymbol{u^{[2]}} = 0$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
For $\boldsymbol{u^{[1]}}$, based on $X = X_{0}$: 
\begin{itemize}
    \item $\nabla_{\boldsymbol{u^{[1]}}} \mathcal{L} = 2 \boldsymbol{S} \boldsymbol{u^{[1]}} - 2 \lambda^{[1]} \boldsymbol{u^{[1]}} = 0$
    \item $\Rightarrow \boldsymbol{S} \boldsymbol{u^{[1]}} = \lambda^{[1]} \boldsymbol{u^{[1]}}$
    \item This is the eigenvector/eigenvalue equation, so $\boldsymbol{u^{[1]}}$ is the eigenvector of $\boldsymbol{S}$ and $\lambda^{[1]}$ is the associated eigenvalue
    \item We see that the variance of the projected data is equal to $\lambda^{[1]}$:
    $\boldsymbol{u^{[1]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[1]}} = \boldsymbol{u^{[1]}}^\intercal \lambda^{[1]} \boldsymbol{u^{[1]}} = \lambda^{[1]} \boldsymbol{u^{[1]}}^\intercal \boldsymbol{u^{[1]}} = \lambda^{[1]} \times 1$
\end{itemize}
For $\boldsymbol{u^{[2]}}$, based on $X = X_{1}$: 
\begin{itemize}
    \item $\nabla_{\boldsymbol{u^{[2]}}} \mathcal{L} = 2 \boldsymbol{S} \boldsymbol{u^{[2]}} - 2 \lambda^{[2]} \boldsymbol{u^{[2]}} - \lambda^{[1][2]} \boldsymbol{u^{[1]}} = 0$
    \item $\Rightarrow \boldsymbol{S} \boldsymbol{u^{[2]}} = \lambda^{[2]} \boldsymbol{u^{[2]}}$ \\
    Proof: 
    \begin{itemize}
        \item Multiplying with $\boldsymbol{u^{[1]}}^\intercal$: $2 \boldsymbol{u^{[1]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[2]}} - 2 \lambda^{[2]} \boldsymbol{u^{[1]}}^\intercal \boldsymbol{u^{[2]}} - \lambda^{[1][2]} \boldsymbol{u^{[1]}}^\intercal \boldsymbol{u^{[1]}} = 0$
        \item $ = 2 \boldsymbol{u^{[1]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[2]}} - 0 - \lambda^{[1][2]} \times 1 = 0$ because of orthogonality resp. orthonormality
        \item $ = 2 \boldsymbol{u^{[2]}}^\intercal \boldsymbol{S} \boldsymbol{u^{[1]}} - \lambda^{[1][2]} = 0$ because the variance is a scalar and can be transposed and because the covariance matrix is symmetric
        \item $ = 2 \boldsymbol{u^{[2]}}^\intercal \lambda^{[1]} \boldsymbol{u^{[1]}} - \lambda^{[1][2]} = 0$ after plugging in the first found basis vector
        \item $ = 2 \lambda^{[1]} \times 0 - \lambda^{[1][2]} = 0$
        \item $ = \lambda^{[1][2]} = 0$
    \end{itemize}
    \item [...] continue as for previous vector
\end{itemize}
In the end, we have a total projected variance of $\sum_{j=1}^d \lambda^{[j]}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex 
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically
\end{itemize}

{\color{black}\hrule height 0.001mm}
_
{\color{black}\hrule height 0.001mm}

\subsection*{SVD Approach}
\subsection*{Formulation}
\begin{itemize}
    \item Project data $\{\boldsymbol{x^{(i)}}\}_{i=1}^n \in \mathbb{R}^m$ onto space $\mathbb{R}^d$ spanned by orthonormal basis $\{\boldsymbol{u^{[j]}}\}_{j=1}^d \in \mathbb{R}^m$ (subset of $\{\boldsymbol{u^{[j]}}\}_{j=1}^m \in \mathbb{R}^m$) where $d \ll m$
    \item Given basis, we can represent original datapoint as: $\boldsymbol{x}^{(i)} = \sum_{j=1}^m (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[j]})\boldsymbol{u}^{[j]}$
    \item We can represent projected datapoint as: $\tilde{\boldsymbol{x}}^{(i)} = \boldsymbol{B}\boldsymbol{B}^T \boldsymbol{x^{(i)}} = \boldsymbol{B}^T \boldsymbol{x^{(i)}}$ since $\boldsymbol{B}$ is orthogonal
    or, equivalently\\
    $\tilde{\boldsymbol{x}}^{(i)} = \sum_{j=1}^d \alpha_{ij} \boldsymbol{u^{[j]}} + \sum_{j=d+1}^m \gamma_j \boldsymbol{u^{[j]}}$ where $\alpha_{ij}$ is specific to the instance and $\gamma_j$ is generic and maps up to the subspace
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Objective function} ---
\begin{itemize}
    \item Reconstruction error: $J = \frac{1}{n} \sum_{i=1}^n \| \boldsymbol{x^{(i)}} - \tilde{\boldsymbol{x}}^{(i)} \|^2 = \frac{1}{n} \sum_{i=1}^n \| \boldsymbol{x^{(i)}}\|^2 - \| \tilde{\boldsymbol{x}}^{(i)} \|^2 = \frac{1}{n} \sum_{i=1}^n \| \boldsymbol{x^{(i)}} \|^2 - \| \boldsymbol{B}^T \boldsymbol{x^{(i)}} \|^2$    
    \item We can show that $\alpha_{ij} = \boldsymbol{x}^{(i)} \cdot \boldsymbol{u^{[j]}}$ and $\gamma_j = \bar{\boldsymbol{x}} \cdot \boldsymbol{u^{[j]}}$\\
    Proof:
    \begin{itemize}
        \item Take derivative of reconstruction error $J$ and set it to $0$
    \end{itemize}
    \item If $m = d-1$, $J = \boldsymbol{u^{[d]\intercal}} \boldsymbol{S}\boldsymbol{u^{[d]}}$\\
    Proof:
    \begin{itemize}
        \item For $ d = m - 1 $,  
        $
        \tilde{\boldsymbol{x}}^{(i)} = \sum_{j=1}^d \alpha_{ij} \boldsymbol{u}^{[j]}
        $
        \item Substituting $\tilde{\boldsymbol{x}}^{(i)}$ and $\boldsymbol{x}^{(i)}$ in $ J $, we get:
        $
        J = \frac{1}{n} \sum_{i=1}^n \| \boldsymbol{x}^{(i)} - \tilde{\boldsymbol{x}}^{(i)} \|^2 = \frac{1}{n} \sum_{i=1}^n \| \sum_{j=1}^m (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[j]}) \boldsymbol{u}^{[j]} - \sum_{j=1}^{m-1} \alpha_{ij} \boldsymbol{u}^{[j]} \|^2 = \frac{1}{n} \sum_{i=1}^n \| \sum_{j=1}^m (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[j]}) \boldsymbol{u}^{[j]} - \sum_{j=1}^{m-1} (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[j]}) \boldsymbol{u}^{[j]} \|^2 = \frac{1}{n} \sum_{i=1}^n \| (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[D]}) \boldsymbol{u}^{[D]} \|^2
        $
        \item Using the orthonormality of $ \boldsymbol{u}^{[D]} $, we simplify:
        $
        \| (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[D]}) \boldsymbol{u}^{[D]} \|^2 = (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[D]})^2
        $
        \item Thus:
        $
        J = \frac{1}{n} \sum_{i=1}^n (\boldsymbol{x}^{(i)} \cdot \boldsymbol{u}^{[D]})^2
        $
        \item To relate this to the covariance matrix 
        $
        \boldsymbol{S} = \frac{1}{n} \sum_{i=1}^n \big(\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}\big) \big(\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}\big)^T,
        $, we transform the projection to use centered data:
        $
        J = \frac{1}{n} \sum_{i=1}^n ((\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}) \cdot \boldsymbol{u}^{[D]})^2
        $
        \item Expanding the squared projection:
        $
        J = \frac{1}{n} \sum_{i=1}^n ((\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}) \cdot \boldsymbol{u}^{[D]})^\intercal ((\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}) \cdot \boldsymbol{u}^{[D]}) = \boldsymbol{u}^{[D]}^\intercal (\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}}) (\boldsymbol{x}^{(i)} - \bar{\boldsymbol{x}})^\intercal \boldsymbol{u}^{[D]} = \boldsymbol{u}^{[D]}^\intercal \boldsymbol{S} \boldsymbol{u}^{[D]}
        $
        \item Here, $\boldsymbol{u}^{[D]}$ is the eigenvector of $\boldsymbol{S}$ with the smallest eigenvalue
    \end{itemize}
    \item $\boldsymbol{x^{(i)}}$ is a column of $\boldsymbol{X}^\intercal$
    \item If $\boldsymbol{A} = \boldsymbol{X}^\intercal$ and SVD of $\boldsymbol{A}$ is $\boldsymbol{U}\boldsymbol{S}\boldsymbol{V}^\intercal$, then $\boldsymbol{B}$ is given by $\boldsymbol{U}^{(j \leq d)}$, i.e. the first $d$ columns of $\boldsymbol{U}$
    \item Then, reconstruction error is given by: $J = \frac{1}{n} \sum_{i=1}^n \| \boldsymbol{x^{(i)}} \|^2 - \| \boldsymbol{B}^T \boldsymbol{x^{(i)}} \|^2 =\sum_{i=1}^n \| \boldsymbol{x^{(i)}} \|^2 - \frac{1}{d} \sum_{i=1}^d \sigma_d^2$ given SVD Projection Energy
\end{itemize}