\section{Non-Parametric Bayesian Clustering}
\subsection*{Motivation}
\begin{itemize}
    \item In GMM, $\pi^{[i]} \sim \textrm{Dir}(\alpha_1, ..., \alpha_k)$
    \item $\textrm{Dir}$ is finite, i.e., all $k$ clusters will be realized with probability 1
    \item Challenge: selecting $k$ in advance
    \item Potential solution: Select $k \to \infty$, then only when $n \to \infty$ all clusters will be realized with probability 1
    \item Better solution: select $k = \infty$, this leads to non-parametric Bayesian models
    \item Question: How do we get infinite probabilities that sum to 1? We need:
    \begin{enumerate}
        \item A suitable distribution
        \item A way to sample from it
    \end{enumerate}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Dirichlet process (DP)}
\begin{itemize}
    \item Distribution over probability distributions of a space $\Theta$
    \item $\textrm{DP}(\alpha, H)$ where $\alpha$ is the concentration parameter and $H$ is the base measure on $\Theta$
    \item Sample $G \sim \textrm{DP}(\alpha, H)$ is a probability distribution
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Stick Breaking Process}
\begin{itemize}
    \item Observation: Sampling $\pi^{[i]} \sim \textrm{Dir}(\alpha_1, ..., \alpha_k)$ is equivalent to sampling:
    $\pi^{[1]} \sim \textrm{Beta}(\alpha_1, ..., \alpha_k)$ and $(\pi^{[2]}, ..., \pi^{[k]}) \sim \textrm{Dir}(\alpha_2, ..., \alpha_k)$
    \item For finite length $k$, we have:
    \begin{itemize}
        \item $\pi^{[1]} = \beta_1$ with $\beta_1 \sim \textrm{Beta}(\alpha_1, ..., \alpha_k)$
        \item $\pi^{[2]} = (1 - \beta_1)\beta_2$ with $\beta_2 \sim \textrm{Beta}(\alpha_2, ..., \alpha_k)$
        \item $\pi^{[3]} = (1 - \beta_1)(1 - \beta_2)\beta_3$ with $\beta_3 \sim \textrm{Beta}(\alpha_3, ..., \alpha_k)$
    \end{itemize}
    \item If we fix $\alpha$, we have a revised form:
    \begin{itemize}
        \item $\pi^{[1]} = \beta_1$ with $\beta_1 \sim \textrm{Beta}(\alpha)$
        \item $\pi^{[2]} = (1 - \beta_1)\beta_2$ with $\beta_2 \sim \textrm{Beta}(\alpha)$
        \item $\pi^{[k]} = (1 - \sum_{j=1}^{k-1} \pi^{[j]})\beta_k$ with $\beta_k \sim \textrm{Beta}(\alpha)$
    \end{itemize}
    \item This is the GEM distribution:
    $\pi \sim \textrm{GEM}(\alpha)$ with $\pi = \{ \pi^{[k]} \}_{k=1}^\infty$
    \item Connection to DP:
    \begin{itemize}
        \item If $\pi \sim \textrm{GEM}(\alpha)$ and $\theta_k \sim H$, then
        $G(\theta) \sim \sum_{k=1}^\infty \pi^{[k]} \delta_{\theta_k}(\theta)$
        is a sample from $\textrm{DP}(\alpha, H)$ and is a distribution over $\Theta$
        \item If we repeatedly sample $\theta^{[1]}, \theta^{[2]}, \dots$ from $G$, we have $\theta^{[i]} = \theta_{k_i}$ where value sometimes has not been observed before ($k_i \neq k_j$ for all $j < i$), sometimes value has been observed before ($k_i = k_j$ for some $j < i$)
        \item $\theta^{[i]}, \theta^{[j]}$ with $k_i = k_j$ belong to the same cluster
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Chinese restaurant process (CRP)}
Connection to clustering:
\begin{itemize}
    \item Customers are observations $\theta^{[i]}$
    \item Tables are clusters $\theta_k$
    \item When a new customer arrives, he either:
    \begin{itemize}
        \item Joins an existing table with probability proportional to the number of customers already sitting there
        \item Starts a new table with probability proportional to $\alpha$
    \end{itemize}
    \item Let $\mathcal{P}$ be the current seating arrangement
    \item The probability that customer $n+1$ joins table $T$ is:
    $P(\textrm{customer } n+1 \textrm{ joins table } T \, | \, \mathcal{P}) = 
    \begin{cases} 
    \frac{|T|}{\alpha + n} & \textrm{if table } T \textrm{ is in } \mathcal{P}, \\
    \frac{\alpha}{\alpha + n} & \textrm{otherwise}
    \end{cases}$
    \item The joint probability for the seating arrangement $\mathcal{P}$ is:
    $P(\mathcal{P}) = P(\textrm{customer 1 joins table T}) \times P(\textrm{what customer 2 does (join table or start new table)}), ... = \frac{\alpha^{|\mathcal{P}|}}{\alpha^{(n)}} \prod_{T \in \mathcal{P}} (|T| - 1)!$
    \item The expected number of tables is:
    $\mathbb{E}(|\mathcal{P}|) = \mathcal{O}(\alpha \log(N))$
\end{itemize}
Connection to DP:
\begin{itemize}
    \item Note: CRP is order and labeling independent
    \item A sequence $\boldsymbol{x}$ is exchangeable if random vectors $(x_1, x_2, ...)$ and $(x_{\tau(1)}, x_{\tau(2)}, ...)$ have the same distribution, where $\tau(\cdot)$ is a random permutation
    \item According to \emph{De Finetti's Theorem}: For an exchangeable sequence:
    $P(x_1, x_2, ...) = \int \prod_i P(x_i | G) dP(G)$
    where $G \sim \textrm{DP}(\alpha, H)$
    \item We can apply this theorem to CRP
\end{itemize}
CRP for continuous distributions:
\begin{itemize}
    \item For continuous distributions, the probability of drawing an $x_k$ that matches exactly one of the previous samples drawn is 0
    \item Thus: $p(x_k) = \frac{\alpha}{\alpha + k - 1}$ and $\sum_{k=1}^n p(x_k) = \sum_{k=1}^n \frac{\alpha}{\alpha + k - 1} = S(n)$
    \item We can approximate the sum $S(n)$ with the integral $I(n)$:
    $
    I(n) = \int_1^{n+1} \frac{\alpha}{\alpha + x - 1} dx
    $
    \item Integral is bounded above by the sum:
    $
    I(n) \leq S(n)
    $
    \item Integral is bounded below by:
    $
    I(n) \geq \sum_{k=2}^{n+1} \frac{\alpha}{\alpha + k - 1}
    $
    since the sum grows smaller as $k$ grows larger. Therefore:
    $
    I(n) \geq S(n) - \frac{\alpha}{\alpha} + \frac{\alpha}{\alpha + n} = S'(n)
    $
    \item So we have:
    $
    S(n) - 1 + \frac{\alpha}{\alpha + n} \leq I(n) \leq S(n)
    $
    which we can manipulate to:
    $
    I(n) + 1 - \frac{\alpha}{\alpha + n} \geq S(n) \geq I(n)
    $
    \item Computing the integral $I(n)$:
    $
    I(n) = \int_1^{n+1} \frac{\alpha}{\alpha + x - 1} \, dx = F(n+1) - F(1) = \alpha (\ln(\alpha + n) - \ln(\alpha))
    $
    \item As $n \to \infty$, $I(n) \to \alpha (\ln(n))$\\
    Thus, as $n \to \infty$, $S(n) \to \alpha (\ln(n))$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{DP mixture model}
\begin{itemize}
    \item Let:
    \begin{itemize}
        \item $\Theta = \mathbb{R}$, space of parameters that defines family of probability distributions
        \item $H = \mathcal{N}(\mu_0, \sigma_0)$, base measure on $\Theta$, representing prior belief over parameters
    \end{itemize}
    \item On that basis, we define the DP mixture model:
    \begin{itemize}
        \item Probabilities of clusters:
        $
        \pi = (\pi^{[1]}, \dots) \sim \textrm{GEM}(\alpha)
        $
        \item Cluster centers:
        $
        \mu_k \sim \mathcal{N}(\mu_0, \sigma_0)
        $
        \item Assignments of datapoints to clusters:
        $
        z^{(i)} \sim \textrm{Categorical}(\pi)
        $
        \item Coordinates of datapoints:
        $
        x^{(i)} \sim \mathcal{N}(\mu^{[z^{(i)}]}, \sigma^{[z^{(i)}]})
        $
    \end{itemize}
    \item Fitting the model:
    \begin{itemize}
        \item Prior: Probability of cluster assignment, based on cluster size
        \item Likelihood: Probability of datapoint, given cluster center
        \item \emph{Gibbs sampling}:
        \begin{itemize}
            \item Idea: Sample each variable in turn, conditioned on values of all other variables in the distribution
            \item 
            $
            P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}, \boldsymbol{x}, \alpha, \boldsymbol{\mu}) 
            \propto P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}) \, P(\boldsymbol{x} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)})
            $ due to Baye's rule
            \item 
            $
            \propto P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}) \, P(x^{(i)} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)}, \boldsymbol{x}^{(-i)}) \, P(\boldsymbol{x}^{(-i)} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)})
            $ due to product rule
            \item 
            $
            \propto P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}) \, P(x^{(i)} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)}, \boldsymbol{x}^{(-i)})
            $ because in last term $\boldsymbol{x}^{(-i)} \perp z^{(i)} \mid \boldsymbol{z}^{(-i)}$
            by d-separation, so this term is constant with respect to $k$ 
            \item We then have:
            $
            P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}, \alpha) \, P(x^{(i)} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)}, \boldsymbol{x}^{(-i)}, \boldsymbol{\mu})
            =$ prior $\times$ likelihood, with interchangeable $z^{(1)},...,z^{(k)}$
            \item Prior: Comes from CRP, given by:
            $
            P(z^{(i)}) = k \mid \boldsymbol{z}^{(-i)}, \alpha) =
            \begin{cases}
                \frac{N_{k, -i}}{\alpha + N - 1} & \textrm{if $k$ is an existing cluster} \\
                \frac{\alpha}{\alpha + N - 1} & \textrm{otherwise}
            \end{cases}
            $
            \item Likelihood: We only have to consider points in $\boldsymbol{x}$ that are assigned to cluster $k$, given by:
            $
            P(z^{(i)} \mid z^{(i)} = k, \boldsymbol{z}^{(-i)}, \boldsymbol{x}^{(-i)}, \boldsymbol{\mu}) =
            \begin{cases}
                P(x^{(i)} \mid \boldsymbol{x}^{(-i,k)}, \boldsymbol{\mu}) =
                \frac{P(x^{(i)}, \boldsymbol{x}^{(-i,k)} \mid \boldsymbol{\mu})}{P(\boldsymbol{x}^{(-i,k)} \mid \boldsymbol{\mu})} & \textrm{for existing } k\\
                P(x^{(i)} \mid \boldsymbol{\mu}) & \textrm{otherwise}
            \end{cases}
            $
            \item Thus, Gibbs sampler: 
            $
            P(z^{(i)} = k \mid \boldsymbol{z}^{(-i)}, \boldsymbol{x}, \alpha, \boldsymbol{\mu}) =
            \begin{cases}
                \frac{N_{k, -i}}{\alpha + N - 1} \times P(x^{(i)} \mid \boldsymbol{x}^{(-i,k)}, \boldsymbol{\mu}) & \textrm{for existing } k\\
                \frac{\alpha}{\alpha + N - 1} \times P(x^{(i)} \mid \boldsymbol{\mu}) & \textrm{otherwise}
            \end{cases}
            $
        \end{itemize}
    \end{itemize}
\end{itemize}


{\color{black}\hrule height 0.001mm}

\subsection*{Latent dirichlet allocation (LDA)}
\begin{itemize}
    \item Extension of DP Mixture Model where each $x^{(i)}$ can be assigned multiple clusters (multivariate), but we don’t know in advance how many (non-parametric, as in DP Mixture Model)
    \item Distribution of topics in document $d$:
    $
    \boldsymbol{\theta}_d \sim \textrm{Dir}(\alpha)
    $
    \item What topic word $w$ belongs to in document $d$:
    $
    z^{(d,w)} \sim \textrm{Categorical}(\boldsymbol{\theta}_d)
    $
    \item Distribution of words in topic $k$:
    $
    \varphi_k \sim \textrm{Dir}(\beta)
    $
    \item What is word $w$ in document $d$:
    $
    w^{(d,w)} \sim \textrm{Categorical}(\varphi_{z^{(d,w)}})
    $
    \item $\alpha$ controls prior weights of topics in documents
    \item $\beta$ controls prior weights of words in topics
\end{itemize}