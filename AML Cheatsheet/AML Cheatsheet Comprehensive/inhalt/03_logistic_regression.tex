\section{Logistic Regression}
\subsection*{Description}
\emph{Task} --- Binary classification

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item Probability of each class is estimated via sigmoid function: $\sigma(z) = \frac{1}{1+e^{-z}} = \frac{e^{z}}{1+e^{z}}$
    \item $P(y=1|\boldsymbol{x}) = \frac{1}{1+e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{1+e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}$
    \item $P(y=0|\boldsymbol{x}) = \frac{1}{1+e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}} = \frac{e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}}{1+e^{-\boldsymbol{\beta} \cdot \boldsymbol{x}}}$
    \item Odds: $\frac{P(y=1|\boldsymbol{x})}{P(y=0|\boldsymbol{x})} = e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}$
    \item Log-odds: $ln(\frac{P(y=1|\boldsymbol{x})}{P(y=0|\boldsymbol{x})}) = \boldsymbol{\beta} \cdot \boldsymbol{x} = ln(\frac{P(y=1)}{P(y=0)}) + ln(\frac{P(\boldsymbol{x}|y=1)}{P(\boldsymbol{x}|y=0)})$, i.e. can be decomposed into prior and likelihood
    \item We can influence the prior by introducing an offset to the log-odds: $\boldsymbol{\beta} \cdot \boldsymbol{x} + \eta$
    \item Geometrically, $z = \boldsymbol{\beta} \cdot \boldsymbol{x}$ defines a linear separating hyperplane:
    \begin{itemize}
        \item When $z > 0$ resp. log-odds $> 0$, then odds $> 1$ resp. $P(y=1|\boldsymbol{x}) > P(y=0|\boldsymbol{x})$ resp. $\sigma(z) > 0.5$, then predict $y = 1$
        \item When $z < 0$ resp. log-odds $< 0$, then odds $< 1$ resp. $P(y=1|\boldsymbol{x}) < P(y=0|\boldsymbol{x})$ resp. $\sigma(z) < 0.5$, predict $y = 0$
        \item When $z = 0$ resp. log-odds $= 0$, then odds $= 1$ resp. $P(y=1|\boldsymbol{x}) = P(y=0|\boldsymbol{x})$ resp. $\sigma(z) = 0.5$, then we are on the decision boundary
        \item Decision boundary $z$ is orthogonal to $\boldsymbol{\beta}$\\
        Proof:
        \begin{itemize}
            \item For any $2$ points $\boldsymbol{x}_1,\boldsymbol{x}_2$ on the decision boundary $z = 0$, i.e. $\boldsymbol{\beta} \cdot \boldsymbol{x}_1 =0, \quad \boldsymbol{\beta} \cdot \boldsymbol{x}_2 =0$ 
            \iem Since $\boldsymbol{x}_1,\boldsymbol{x}_2$ are on the decision boundary, vector $\boldsymbol{z}$ can be considered a linear combination of $\boldsymbol{x}_1 - \boldsymbol{x}_2$
            \item Combining these equations, we get $\boldsymbol{\beta} \cdot (\boldsymbol{x}_1 - \boldsymbol{x}_2) =\boldsymbol{\beta} \cdot u \boldsymbol{z}=0$
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Likelihood: $L(\boldsymbol{\beta}) = \prod_{i=1}^n P(y^{(i)} | \boldsymbol{x}^{(i)}; \boldsymbol{\beta}) = \prod_{i=1}^n \sigma(z^{(i)})^{y^{(i)}} (1 - \sigma(z^{(i)}))^{1 - y^{(i)}}$
    \item Maximize log-likelihood: 
    $\log L(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right] = \sum_{i=1}^n \left[ y^{(i)} \log \frac{1}{1 + e^{-z^{(i)}}} + (1 - y^{(i)}) \log \frac{e^{-z^{(i)}}}{1 + e^{-z^{(i)}}} \right] = \sum_{i=1}^n \left[ y^{(i)} z^{(i)} - \log (1 + e^{z^{(i)}}) \right]$
    \item Minimize log-loss: $ -\log L(\boldsymbol{\beta})$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\frac{\partial -\log L(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} = -\sum_{i=1}^n \frac{\partial}{\partial \boldsymbol{\beta}} [ y^{(i)} \log \sigma(z^{(i)}) + (1 - y^{(i)}) \\ \log (1 - \sigma(z^{(i)})) ] = \sum_{i=1}^n [ \sigma(z^{(i)}) - y^{(i)} ] \boldsymbol{x}^{(i)}$
    \\
    Proof:
    \begin{itemize}
        \item Derivative of the sigmoid: $\frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} = \sigma(z^{(i)}) (1 - \sigma(z^{(i)}))$
        \item Derivative of the first term: $
        \frac{\partial}{\partial \boldsymbol{\beta}} \left[ y^{(i)} \log \sigma(z^{(i)}) \right] = y^{(i)} \frac{1}{\sigma(z^{(i)})} \frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial \boldsymbol{\beta}} = y^{(i)} \frac{1}{\sigma(z^{(i)})} \sigma(z^{(i)}) (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = y^{(i)}  (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = y^{(i)}\boldsymbol{x}^{(i)}  - y^{(i)}\sigma(z^{(i)}) \boldsymbol{x}^{(i)}$
        \item Derivative of the second term: $\frac{\partial}{\partial \boldsymbol{\beta}} \left[ (1 - y^{(i)}) \log (1 - \sigma(z^{(i)})) \right] = (1 - y^{(i)}) \frac{1}{1 - \sigma(z^{(i)})} (-1) \frac{\partial \sigma(z^{(i)})}{\partial z^{(i)}} \frac{\partial z^{(i)}}{\partial \boldsymbol{\beta}} = (1 - y^{(i)}) \frac{1}{1 - \sigma(z^{(i)})} (-1) \sigma(z^{(i)}) (1 - \sigma(z^{(i)})) \boldsymbol{x}^{(i)} = -(1 - y^{(i)}) \sigma(z^{(i)}) \boldsymbol{x}^{(i)}= y^{(i)} \sigma(z^{(i)}) \boldsymbol{x}^{(i)} - \sigma(z^{(i)}) \boldsymbol{x}^{(i)}$
    \end{itemize}
    \item If we set gradient to 0, we have expectation matching: $\sum_{i=1}^n \sigma(z^{(i)}) \boldsymbol{x}^{(i)} = \sum_{i=1}^n y^{(i)} \boldsymbol{x}^{(i)}$ resp. optimum is where expected feature counts, weighted by predicted probability $=$ observed feature counts, weighted by true labels
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex (since sum of convex functions in log loss is convex) with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions 
    \item Can be solved numerically
\end{itemize}
Proof that log loss is convex:
\begin{itemize}
    \item Sum of convex functions is convex
    \item Thus, we need to prove convexity of $ \ln(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}}) $ and $ -y^{(i)} \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} $
    \item For second term:
    \begin{itemize}
        \item $\mathcal{H}(\boldsymbol{\beta}) = \nabla^2_{\boldsymbol{\beta}} \left(-y^{(i)} \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}\right) = 0$
        \item $ \mathcal{H} \succcurlyeq 0 $
    \end{itemize}
    \item For first term:
    \begin{itemize}
        \item $ \mathbb{H}(\boldsymbol{\beta}) = \nabla^2_{\boldsymbol{\beta}} \ln(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}}) $
        \item $ = v(x) \times u'(x) - v'(x) \times u(x) = \frac{1}{1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}} \times e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \times \boldsymbol{x} \boldsymbol{x}^\intercal - \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \times \boldsymbol{x}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \times \boldsymbol{x}^\intercal \times e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}$
        \item $= \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^\intercal (1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}) - e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^\intercal e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2}$
        \item $ = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}} \boldsymbol{x} \boldsymbol{x}^T}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} $
        \item $ \mathcal{H} \succcurlyeq 0 $\\
        Proof:
        $ \boldsymbol{a}^T \mathcal{H} \boldsymbol{a} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \boldsymbol{a}^T \boldsymbol{x} \boldsymbol{x}^T \boldsymbol{a} = \frac{e^{\boldsymbol{\beta} \cdot \boldsymbol{x}}}{(1 + e^{\boldsymbol{\beta} \cdot \boldsymbol{x}})^2} \lVert \boldsymbol{a}^T \boldsymbol{x} \rVert^2 \geq 0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Regularization} --- 
\begin{itemize}
    \item Perfectly separable data requires regularization
    \item Let weights for each class $k$ be scaled by $c$ as $c \boldsymbol{\tilde{\beta}}_k$
    \item Gradient of log-loss with respect to $c$ is always negative, causing gradient descent to grow $c$ without bound\\
    Proof:
    \begin{itemize}
        \item $\textrm{Log loss} = \sum_{i=1}^n \ln\left(1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}\right) - y^{(i)} c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}$
        \item $\nabla_c \textrm{ log loss} = \sum_{i=1}^n \frac{1}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} \times e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}} \times \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} - y^{(i)} \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}$
        \item $= \sum_{i=1}^n \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} \left(\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)}\right)$
        \item Given perfect separation:
        \begin{itemize}
            \item If $y^{(i)} = 1$, $\boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} > 0$, $\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)} < 0$
            \item If $y^{(i)} = 0$, $\boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)} < 0$, $\frac{e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}}{1 + e^{c \boldsymbol{\tilde{\beta}} \cdot \boldsymbol{x}^{(i)}}} - y^{(i)} > 0$
        \end{itemize}
        \item Thus, for all $i$, $\nabla_c \textrm{ log loss} < 0$
        \item Thus gradient descent will cause $c$ to grow without bound
    \end{itemize}
\end{itemize}