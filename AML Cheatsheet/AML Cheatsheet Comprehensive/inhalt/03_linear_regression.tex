\section{Linear Regression}
\subsection*{Description}
\emph{Task} --- Regression

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised 
    \item Parametric
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}

\begin{itemize}
    \item $y^{(i)} = \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}}$ resp. $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta}$ where $\boldsymbol{X}$ contains $n$ rows, each of which represents an instance, and $m$ columns, each of which represents a feature
    \item To incorporate offset, first column of $\boldsymbol{X}$ (i.e. first feature) is set to $1$ and first element of $\boldsymbol{\beta}$ is set to $\beta_0$
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item $\boldsymbol{\beta}$ lies in the rowspace of $\boldsymbol{X}$ resp. columnspace of $\boldsymbol{X}^\intercal$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\emph{Ordinary least squares estimator (OLSE)}:
\begin{itemize}
    \item Minimize mean squared error: $LO = \frac{1}{n} \sum_{i=1}^n ( y^{(i)} - \boldsymbol{\beta} \cdot \boldsymbol{x^{(i)}} )^2$ resp. $LO = ( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( y - \boldsymbol{\beta} \cdot \boldsymbol{x})^2 = (y - \boldsymbol{\beta} \cdot \boldsymbol{x})\boldsymbol{x} = 0$
    resp.     
    $\nabla_{\boldsymbol{\beta}} LO = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^\intercal( \boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta} )) = \frac{1}{2} \nabla_{\boldsymbol{\beta}} (\boldsymbol{\beta}^\intercal \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - 2 \boldsymbol{y}^\intercal \boldsymbol{X} \boldsymbol{\beta}) = \boldsymbol{X}^\intercal \boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^\intercal (\boldsymbol{X} \boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Alternatives to OLSE} --- 
MLE:
\begin{itemize}
    \item Yields same result as OLSE
    \item The likelihood is:
    $
    p(\boldsymbol{y} \mid \boldsymbol{\beta}, \sigma^2) = \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2 \right)
    $
    \item The log likelihood is:
    $
    \mathcal{L} = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2
    $
    \item We minimize: $ \|\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta}\|^2$
    \item This is equivlent to OLSE
\end{itemize}
Orthogonality principle:
\begin{itemize}
    \item $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item We wish to minimize $\| \hat{\boldsymbol{y}} - \boldsymbol{y} \| = \| \boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y} \|$ by selecting $\boldsymbol{\beta}$ appropriately
    \item By the orthogonality principle, $\boldsymbol{x}^{[j]} \cdot (\hat{\boldsymbol{y}} - \boldsymbol{y}) = \boldsymbol{x}^{[j]} \cdot (\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) = 0$ where $\boldsymbol{x}^{[j]}$ is the $j^{th}$ column of $\boldsymbol{X}$\\
    resp.\\
    $\boldsymbol{X}^\intercal (\hat{\boldsymbol{y}} - \boldsymbol{y}) = \boldsymbol{X}^\intercal(\boldsymbol{X}\boldsymbol{\beta} - \boldsymbol{y}) = 0$
    \item $\Rightarrow \boldsymbol{\beta} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
    \item Alternatively, $\boldsymbol{\beta}$ lies in the columnspace of $\boldsymbol{X}^\intercal$
    \item Then, we can express $\boldsymbol{\beta}$ as $\boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$
    \item This yields an equation system $\boldsymbol{y} = \boldsymbol{X} \boldsymbol{X}^\intercal [\alpha_1, ..., \alpha_n]^\intercal$ which can be solved for $\alpha_i$
    \item On that basis, $\boldsymbol{\beta}$ can be calculated
\end{itemize}
Pseudo Inverse:
\begin{itemize}
    \item Yields same result as OLSE
    \item Minimum-norm solution
    \item $\boldsymbol{\beta}$ minimizes MSE if $\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{\beta}$ is a projection of $\boldsymbol{y}$ to the columnspace of $\boldsymbol{X}$
    \item Given matrix projection via SVD, $\boldsymbol{X} \boldsymbol{X}^{\#} \boldsymbol{y}$ is that projection
    \item $\Rightarrow \boldsymbol{\beta} = \boldsymbol{X}^{\#} \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$
    \item Shows that $\boldsymbol{\beta}$ is largely determined by $\boldsymbol{X}^{\#}$ and, thus, singular values of $\boldsymbol{X}$ based on SVD
\end{itemize}
PCA:
\begin{itemize}
    \item Instances $y^{(i)}, \boldsymbol{x}^{(i)} = \boldsymbol{\xi}^{(i)}$ can be projected onto hyperplane given by $\boldsymbol{X}\boldsymbol{\beta}$
    \item Projections are given by $\hat{\boldsymbol{\xi}}^{(i)}$
    \item Residuals are given by $e^{(i)} = \boldsymbol{\xi}^{(i)} - \hat{\boldsymbol{\xi}}^{(i)}$
    \item Since $e^{(i)}$ is orthogonal to $\hat{\boldsymbol{\xi}}^{(i)}$, we can write using Pythagorean theorem: $\| e^{(i)} \|^2 = \| \boldsymbol{\xi}^{(i)} \|^2 - \| \hat{\boldsymbol{\xi}}^{(i)} \|^2$
    \item This is a PCA via SVD problem
\end{itemize}
Gradient descent:
\begin{itemize}
    \item Minimum-norm solution
    \item Yields same result as OLSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Hypothesis Testing of Found Parameters} --- 
\begin{itemize}
    \item Let $\boldsymbol{y} | \boldsymbol{X} \sim \mathcal{N} (\boldsymbol{y}, \sigma^2 \boldsymbol{I}) = \mathcal{N} (\boldsymbol{X} \boldsymbol{\beta}, \sigma^2 \boldsymbol{I})$
    \item Let $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y} = \boldsymbol{X}^+ \boldsymbol{y}$ be the OLSE where $\boldsymbol{X}^+$ is a scalar
    \item Then, $\boldsymbol{\hat{\beta}} \sim \mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{\beta}, (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} \sigma^2 )$\\Proof:
    \begin{itemize}
        \item $\mathcal{N}( \boldsymbol{X}^+ \boldsymbol{X} \boldsymbol{\beta}, \boldsymbol{X}^{+ \intercal} \sigma^2 \boldsymbol{X}^+ ) = \mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X}^{+ \intercal}  )$ since $\boldsymbol{X}^+$ is a scalar
        \item Further, we have $\mathcal{N}( \boldsymbol{I} \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ ((\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal)^\intercal  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 \boldsymbol{X}^+ \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1 \intercal}  ) = \mathcal{N}( \boldsymbol{\beta}, \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  )$ since $(\boldsymbol{X}^\intercal \boldsymbol{X})$ is symmetric
    \end{itemize}
    \item We can estimate $\sigma^2$ unbiasedly as: $\hat{\sigma}^2 = \frac{1}{n-m} \sum_{i \leq n} (\boldsymbol{X} \boldsymbol{\hat{\beta}} - \boldsymbol{y})^2$
    \item Then, confidence interval for $\hat{\beta_j}$ given by: $\hat{\beta_j} \pm z_{\alpha/2} \hat{se}(\hat{\beta_j})$ where 
    \begin{itemize}
        \item $z_{\alpha/2} = \Phi^{-1}(\alpha/2)$ is Gaussian CDF
        \item $\hat{se}(\hat{\beta_j})$ is the $j^{th}$ diagonal element of the covariance matrix $\sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$
    \end{itemize}
    \item We can perform a hypothesis test on $\hat{\beta}$ with the \emph{Wald test}:
    \begin{itemize}
        \item $H_0 : \beta = \beta_0$ (typically 0)\\
        $H_1 : \beta \neq \beta_0$
        \item Wald statistic: $W = \frac{\hat{\beta} - \beta_0}{\hat{se}}$
        \item If p-value associated with $W$ is smaller than $\alpha$ resp. if $|W|$ is greater than or equal to the critical value $z_{\alpha/2}$, we reject $H_0$
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Evaluation} ---
\begin{itemize}
    \item OLSE is unbiased if noise $\epsilon$ has zero mean:
    \begin{itemize}
        \item Given $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \epsilon$, we can substitute $\boldsymbol{\hat{\beta}} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal (\boldsymbol{X}\boldsymbol{\beta} + \epsilon) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \epsilon$
        \item Taking the expected value on both sides, we have: $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta} + (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \mathbb{E}(\epsilon)$
        \item Then, $\mathbb{E}(\boldsymbol{\hat{\beta}}) = \boldsymbol{\beta}$ if the noise has zero mean
    \end{itemize}
    \item \emph{Gauss Markov theorem}: OLSE is best (lowest variance, lowest MSE) unbiased estimator, if assumptions ($\boldsymbol{X}$ is full rank and there is no multicollinearity, heteroskedasticity, and exogeneity) are met\\
    Proof:
    \begin{itemize}
        \item Let $\hat{\boldsymbol{\beta}} = \boldsymbol{A}^\intercal \boldsymbol{y} = (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{y}$ be the OLSE
        \item Let $\boldsymbol{C}^\intercal \boldsymbol{y}$ be another unbiased estimator
        \item $\mathbb{V}(\hat{\boldsymbol{\beta}}) = \mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \boldsymbol{A}^\intercal \mathbb{V}(\boldsymbol{y}) \boldsymbol{A}$ since $\boldsymbol{A}$ is constant
        \item We can further develop to: $\boldsymbol{A}^\intercal \sigma^2 \boldsymbol{I}_m \boldsymbol{A} = \sigma^2 \boldsymbol{A}^\intercal \boldsymbol{A}$ since variance is given by error term
        \item Similarly, $\mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y}) = \sigma^2 \boldsymbol{C}^\intercal \boldsymbol{C} $
        \item For the OLSE, we can plug in $(\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal$ for $\boldsymbol{A}$ which yields: $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}  \boldsymbol{X}^\intercal \boldsymbol{X} (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1} = \sigma^2 (\boldsymbol{X}^\intercal \boldsymbol{X})^{-1}$ 
        \item Then, we have shown that $\mathbb{V}(\boldsymbol{A}^\intercal \boldsymbol{y}) \leq \mathbb{V}(\boldsymbol{C}^\intercal \boldsymbol{y})$
    \end{itemize}
    \item Nonetheless, there may be biased estimators that generate a lower variance and MSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Convex with psd Hessian
    \item Has global minimum
    \item Has unique or infinitely many solutions
    \item Can be solved analytically, if $\boldsymbol{X}^\intercal \boldsymbol{X}$ is invertible
    \item In case of \emph{multicollinearity}:
    \begin{itemize}
        \item The rank of $X$ is less than full, i.e. there are multiple columns (predictor variables) that are linearly dependent
        \item $\boldsymbol{X}^\intercal \boldsymbol{X}$ is singular, i.e. non-invertible
        \item There are multiple solutions for $\boldsymbol{\beta}$
    \end{itemize}
    \item If it has infinitely many solutions, the preferred solution is the \emph{minimum-norm solution}, which minimizes $\| \boldsymbol{\beta} \|$ and lies in the column space of $\boldsymbol{X}^\intercal$ resp. is a solution to $\boldsymbol{X}\boldsymbol{u}$
\end{itemize}