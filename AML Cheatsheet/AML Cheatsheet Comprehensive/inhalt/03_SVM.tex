\section{SVM Classifier}
\subsection*{Description}
\emph{Task} --- Classification

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- 
\begin{itemize}
    \item Supervised
    \item Parametric
\end{itemize}

\section{Hard-Margin SVM Classifier}
\subsection*{Formulation}
\begin{itemize}
    \item Assume $y \in \{-1,1\}$
    \item Discriminant:
    \begin{itemize}
        \item $f = sgn(\boldsymbol{\beta} \cdot \boldsymbol{x} + b)$
        \item If sign is positive, $f$ outputs $1$, else $-1$
        \item Separating hyperplane given by $\boldsymbol{z} = \boldsymbol{\beta} \cdot \boldsymbol{x} + b = 0$
        \item Is a linear discriminant 
        \item $\boldsymbol{z} \bot \boldsymbol{\beta}$
    \end{itemize}
    \item For some point $\tilde{\boldsymbol{x}}$ closest to the origin:
    \begin{itemize}
        \item The perpendicular distance to the origin is given by: $\boldsymbol{\beta} \cdot\tilde{\boldsymbol{x}} + b = 0$ since $\tilde{\boldsymbol{x}}$ lies on the separating hyperplane $\boldsymbol{z}$
        \item Then, $\| \boldsymbol{\beta} \| \textrm{ } \| \tilde{\boldsymbol{x}} \| cos(\varphi) + b = \| \boldsymbol{\beta} \| \textrm{ } \| \tilde{\boldsymbol{x}} \| (-1) + b = 0$ because $\varphi = 180$ degrees
        \item Then, $\| \tilde{\boldsymbol{x}} \| = \frac{b}{\| \boldsymbol{\beta} \|}$
    \end{itemize}
    \item For some point $\boldsymbol{x}^{(i)}$ above $\boldsymbol{z}$:
    \begin{itemize}
        \item Projection of instance onto direction of $\boldsymbol{\beta}$: $\boldsymbol{x}^{(i)'} = \frac{ \boldsymbol{x}^{(i)} \cdot \boldsymbol{\beta} }{ \| \boldsymbol{\beta} \|^2 } \boldsymbol{\beta}$
        \item Distance of projection to the origin is given by $\| \boldsymbol{x}^{(i)'} \| = cos(\varphi^{(i)}) \| \boldsymbol{x}^{(i)} \| = \frac{ cos(\varphi^{(i)}) \| \boldsymbol{x}^{(i)} \| \textrm{ } \| \boldsymbol{\beta} \| }{ \| \boldsymbol{\beta} \| } = \frac{\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)}}{ \| \boldsymbol{\beta} \| }$
        \item Margin $\gamma^{(i)}$ of instance given by: $\gamma^{(i)} = \| \boldsymbol{x}^{(i)'} \| + \| \tilde{\boldsymbol{x}} \| = \frac{ \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b }{\| \boldsymbol{\beta} \|}$
    \end{itemize}
    \item For some point $\boldsymbol{x}^{(i)}$ below $\boldsymbol{z}$:
    \begin{itemize}
        \item Margin $\gamma^{(i)}$ of instance given by: $\gamma^{(i)} = - \frac{ \boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b }{\| \boldsymbol{\beta} \|}$
    \end{itemize}
    \item For well-classified points, $\gamma^{(i)} > 0$, for mis-classified points, $\gamma^{(i)} < 0$
    \item Given that $y \in \{-1,1\}$ and thus $y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x} + b) > 0$:
    $\gamma^{(i)} = \frac{ y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) }{\| \boldsymbol{\beta} \|}$
    \item Margin of system defined by smallest margin for instance: $\gamma = min_i \gamma^{(i)} = \frac{1}{\| \boldsymbol{\beta} \|} min_i y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)$
    \item Margin is invariate to scaling of $\boldsymbol{\beta}$ and $b$
    \item Thus, we can write:
    \begin{itemize}
        \item $min_i \gamma^{(i)} = min_i y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) = 1$
        \item Then, $y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \geq 1$ (resp. $\geq m$ without scaling) for all $\boldsymbol{x}^{(i)}$
        \item Moreover, since margin for system is defined by smallest margin for instance, $\gamma = \frac{1}{\| \boldsymbol{\beta} \|} $
        \item Since margin is defined in both directions (below and above the separating hyperplane), $\gamma = \frac{2}{\| \boldsymbol{\beta} \|} $
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\boldsymbol{\beta}$ and $b$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Objective function: $\gamma = \frac{2}{\| \boldsymbol{\beta} \|} $ (resp. $2m$) subject to $y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \geq 1$ (resp. $\geq m$)
    \item Equivalent cost function: $\gamma = \frac{1}{2} \| \boldsymbol{\beta} \|^2 $ subject to $1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \leq 0$
    item Cost function in Lagrangian formulation: $\mathcal{L} = \frac{1}{2} \| \boldsymbol{\beta} \|^2 + \sum_{i=1}^n \alpha^{(i)} (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b))$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item General solution:
    \begin{itemize}
        \item $\nabla_{\boldsymbol{\beta}} \mathcal{L} =  \boldsymbol{\beta} - \sum_{i=1}^n \alpha^{(i)} y^{(i)} \boldsymbol{x}^{(i)} = 0$
        \item $\Rightarrow \boldsymbol{\beta}^{*} = \sum_{i=1}^n \alpha^{(i)} y^{(i)}\boldsymbol{x}^{(i)}$
        \item $\nabla_b \mathcal{L} = - \sum_{i=1}^n \alpha^{(i)} y^{(i)} = 0$
        \item Subject to:
        \begin{itemize}
            \item $1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \leq 0$
            \item $\alpha^{(i)} \geq 0$
            \item $\alpha^{(i)} (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) = 0$
        \end{itemize}
    \end{itemize}
    \item By Slater's condition (linear separability), strong duality holds
    \item Objective function in dual Lagrangian, after plugging in found $\boldsymbol{\beta}$: $\mathcal{D} = \frac{1}{2} \| \boldsymbol{\beta} \|^2  \textrm{ (provided barrier function) }= \frac{1}{2}  \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} + \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)} - \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} = \sum_{i=1}^n \alpha^{(i)} - \frac{1}{2}  \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)}$
    \item Dual optimization: Maximize $\alpha$ subject to 
    \begin{itemize}
        \item $\alpha^{(i)} \geq 0$ 
        \item $\sum_{i=1}^n \alpha^{(i)} y^{(i)} = 0$ due to $\nabla_b \mathcal{L}$
    \end{itemize}
    \item Note that only \emph{support vectors} ($\alpha^{(i)} > 0$, sit on the hyperplane $ 1 = y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) = 1$) matter in establishing $\boldsymbol{\beta}^{*}$ and $b^*$:
    \begin{itemize}
        \item Based on complementary slackness condition $\alpha^{(i)} (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) = 0$: We either have 
        \begin{itemize}
            \item $\alpha^{(i)} = 0$ and $ (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) > 0$ resp. $ y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) > 1$ or
            \item $\alpha^{(i)} > 0$ and $ (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) = 0$ resp. $ y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) = 1$ 
        \end{itemize}
        \item Intercept is calculated as average between optimal intercept for support vector on positive and negative side of margin $b = \frac{1}{2} (\boldsymbol{\beta} \cdot \boldsymbol{x}^+ + \boldsymbol{\beta} \cdot \boldsymbol{x}^-)$ since:
        \begin{itemize}
            \item $\boldsymbol{\beta} \cdot \boldsymbol{x}^+ + b = 1$
            \item $\boldsymbol{\beta} \cdot \boldsymbol{x}^- + b = -1$
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Strictly convex with psd Hessian
    \item Has global minimum
    \item Has unique solution
\end{itemize}

\section{Soft-Margin SVM Classifier}
\subsection*{Optimization}
\emph{Objective function} --- 
\begin{itemize}
    \item Cost function: Hinge loss: $max(0,1-\gamma^{(i)})$
    \item Mis-classified instances incur a loss
    \item Well-classified instances incur a loss, if their margin $\gamma^{(i)} < 1$
    \item Always is equal to or dominates the plain misclassification error
    \item To translate hinge loss into inequality constraint, we introduce slack variables $\xi^{(i)}$:
    \begin{itemize}
        \item $y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \geq 1 - \xi^{(i)}$
        \item By setting $\xi^{(i)} = 0$, we get pulled down towards hinge loss
        \item For
        \begin{itemize}
            \item Well-classified points outside of margin $\xi^{(i)} < 0$
            \item Well-classified points within of margin $0 < \xi^{(i)} < 1$
            \item Points on decision boundary $\xi^{(i)} = 1$
            \item Mis-classified points $\xi^{(i)} > 1$
        \end{itemize}
    \end{itemize}
    \item We can then write cost function as slack variables penalized by $\ell_1$ norm: $\frac{1}{2} \| \boldsymbol{\beta} \|^2 + C \sum_{i=1}^n \xi^{(i)}$ where 
    \begin{itemize}
        \item $\| \boldsymbol{\beta} \|^2$ maximizes margin and $C \sum_{i=1}^n \xi^{(i)}$ minimizes hinge loss
        \item $C$ is a hyperparameter that determines how tolerant we are of margin errors: If C is large, we are less tolerant, margin will decrease, and the soft-margin will become a hard-margin
    \end{itemize}
    subject to:
    \begin{itemize}
        \item $1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \leq \xi^{(i)}$ resp. $1-\xi^{(i)} - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \leq 0$ 
        \item $\xi^{(i)} \geq 0$ resp. $-\xi^{(i)} \leq 0$
    \end{itemize}
    \item Cost function in Lagrangian formulation: $\mathcal{L} = \frac{1}{2} \| \boldsymbol{\beta} \|^2 + C \sum_{i=1}^n \xi^{(i)} + \sum_{i=1}^n \alpha^{(i)} (1 - \xi^{(i)} - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) - \sum_{i=1}^n \zeta^{(i)} \xi^{(i)}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item General solution:
    \begin{itemize}
        \item $\nabla_{\boldsymbol{\beta}} \mathcal{L} =  \boldsymbol{\beta} - \sum_{i=1}^n \alpha^{(i)} y^{(i)} \boldsymbol{x}^{(i)} = 0$
        \item $\Rightarrow \boldsymbol{\beta}^{*} = \sum_{i=1}^n \alpha^{(i)} y^{(i)} \boldsymbol{x}^{(i)}$
        \item $\nabla_b \mathcal{L} = - \sum_{i=1}^n \alpha^{(i)} y^{(i)} = 0$
        \item $\nabla_{\xi^{(i)}} \mathcal{L} = C - \alpha^{(i)} - \zeta^{(i)} = 0$
        \item Subject to:
        \begin{itemize}
            \item $- \xi^{(i)} - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) + 1 \leq 0$
            \item $\xi^{(i)} \leq 0$
            \item $\alpha^{(i)}, \zeta^{(i)} \geq 0$
            \item $\alpha^{(i)} (- \xi^{(i)} - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) + 1) = 0$
            \item $\zeta^{(i)} (- \xi^{(i)}) = 0$
        \end{itemize}
    \end{itemize}
    \item By Slater's condition (linear separability), strong duality holds
    \item Objective function in dual Lagrangian, after plugging in found $\boldsymbol{\beta}$: $\mathcal{D} = \frac{1}{2} \| \boldsymbol{\beta} \|^2  \textrm{ (provided barrier function) }= \frac{1}{2}  \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} + \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)} - \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)} = \sum_{i=1}^n \alpha^{(i)} - \frac{1}{2}  \sum_{i=1}^n  \sum_{j=1}^n \alpha^{(i)}\alpha^{(j)} y^{(i)}y^{(j)} \boldsymbol{x}^{(i)} \cdot \boldsymbol{x}^{(j)}$
    \item Dual optimization: Maximize $\alpha$ subject to 
    \begin{itemize}
        \item $\sum_{i=1}^n \alpha^{(i)} y^{(i)} = 0$ due to $\nabla_b \mathcal{L}$
        \item $0 \leq \alpha^{(i)} \leq C$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$ 
    \end{itemize}
    \item Note that only \emph{support vectors} ($\alpha^{(i)} > 0$, sit in or on the hyperplane $ 1 \geq y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) = 1$) matter in establishing $\boldsymbol{\beta}^{*}$ and $b^*$:
    \begin{itemize}
        \item Based on complementary slackness condition $\alpha^{(i)} (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) = 0$: We either have 
        \begin{itemize}
            \item $\alpha^{(i)} = 0$ and $ (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) > 0$ resp.$ y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) > 1$ or
            \item $\alpha^{(i)} > 0$ and $ (1 - y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)) = 0$ resp.$ y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) = 1$ 
        \end{itemize}
        \item Similarly, we either have
        \begin{itemize}
            \item $\zeta^{(i)} = 0$ and $ -\xi^{(i)} < 0$ resp.$ \xi^{(i)} > 0$ or
            \item $\zeta^{(i)} > 0$ and $ -\xi^{(i)} = 0$ resp.$ \xi^{(i)} = 0$
        \end{itemize}
        \item Then, each instance lies in one of three areas:
        \begin{itemize}
            \item Beyond $\gamma$:
            \begin{itemize}
                \item $\alpha^{(i)} = 0$
                \item $C = \zeta^{(i)}$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$
                \item $\xi^{(i)} = 0$
                \item $1 < y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)$
            \end{itemize}
            \item On $\gamma$:
            \begin{itemize}
                \item $\alpha^{(i)}, \zeta^{(i)} > 0$
                \item $0 < \alpha^{(i)} < C$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$
                \item $\xi^{(i)} = 0$
                \item $1 = y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)$
            \end{itemize}
            \item Within $\gamma$:
            \begin{itemize}
                \item $\alpha^{(i)} > 0$
                \item $\zeta^{(i)} = 0$
                \item $\alpha^{(i)} = C$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$
                \item $\xi^{(i)} > 0$
                \item $1 > y^{(i)} (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b)$
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Characteristics} --- 
\begin{itemize}
    \item Strictly convex with psd Hessian
    \item Has global minimum
    \item Has unique solution
\end{itemize}

\section{Extensions to the SVM}
\subsection*{Multiclass SVMs}
\emph{Description} --- 
\begin{itemize}
    \item Instead of binary classification of $y \in \{-1,1\}$, we have multiclass classification, where each $y$ is assigned to one of $k$ classes $y \in \{1,...,k\}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Formulation} --- 
\begin{itemize}
    \item Let $z^{(i)}$ define the class associated with $ \boldsymbol{x}^{(i)}$
    \item We define a weight vector for each class
    \item Then, we have:
    $(\boldsymbol{\beta}_{z^{(i)}} \cdot \boldsymbol{x}^{(i)} + b_{z^{(i)}}) - \max_{z \neq z^{(i)}} (\boldsymbol{\beta}_{z} \cdot \boldsymbol{x}^{(i)} + b_{z}) \geq 1$ (resp. $\geq m$ without scaling) for all $\boldsymbol{x}^{(i)}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
Hard margin:
\begin{itemize}
    \item Cost function: $\frac{1}{2} \| \boldsymbol{B} \|^2 = \frac{1}{2} \sum_{z=1}^k \| \boldsymbol{\beta}_z \|^2 $ subject to $(\boldsymbol{\beta}_{z^{(i)}} \cdot \boldsymbol{x}^{(i)} + b_{z^{(i)}}) - \max_{z \neq z^{(i)}} (\boldsymbol{\beta}_{z} \cdot \boldsymbol{x}^{(i)} + b_{z}) \geq 1$ (resp. $\geq m$) 
\end{itemize}
Soft margin:
\begin{itemize}
    \item Cost function: $\frac{1}{2} \| \boldsymbol{B} \|^2 + C \sum_{i=1}^n \xi^{(i)} = \frac{1}{2} \sum_{z=1}^k \| \boldsymbol{\beta}_z \|^2 + C \sum_{i=1}^n \xi^{(i)}$ subject to:
    \begin{itemize}
        \item $1-\xi^{(i)} - (\boldsymbol{\beta}_{z^{(i)}} \cdot \boldsymbol{x}^{(i)} + b_{z^{(i)}}) + \max_{z \neq z^{(i)}} (\boldsymbol{\beta}_{z} \cdot \boldsymbol{x}^{(i)} + b_{z}) \leq 0$ 
        \item $-\xi^{(i)} \leq 0$
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Structured SVMs}
\emph{Description} --- 
\begin{itemize}
    \item Instead of binary classification of $y \in \{-1,1\}$, we have structured prediction, where each $y$ is assigned to one structured output (e.g. tree, partition, etc.)
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Formulation} --- 
\begin{itemize}
    \item Challenge: Could be formulated as multi-class SVMs, but that would blow up the number of parameters, since we have one weight for each class
    \item Solution: 
    \begin{itemize}
        \item Formulate a feature function $\Psi(\boldsymbol{y}, \boldsymbol{x})$
        \item Define a scoring function $f(\boldsymbol{y}, \boldsymbol{x}) = \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x})$, where the number of parameters depends on the dimensionality of the feature function, but is independent of the number of classes
        \item Perform classification via $\hat{y} = \arg\max_y f(\boldsymbol{y}, \boldsymbol{x})$
    \end{itemize}
    \item Then, we have:
    $f(\boldsymbol{y}, \boldsymbol{x}) - \max_{y'} f(\boldsymbol{y}', \boldsymbol{x}) = \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x}) - \max_{y'} \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}', \boldsymbol{x})  \geq 1$ (resp. $\geq m$ without scaling) for all $\boldsymbol{x}^{(i)}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
Hard margin:
\begin{itemize}
    \item Cost function: $\frac{1}{2} \| \boldsymbol{B} \|^2 = \frac{1}{2} \sum_{z=1}^k \| \boldsymbol{\beta}_z \|^2 $ subject to $\boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x}) - \max_{y'} \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}', \boldsymbol{x})  \geq 1$ (resp. $\geq m$) 
\end{itemize}
Soft margin:
\begin{itemize}
    \item Cost function: $\frac{1}{2} \| \boldsymbol{B} \|^2 + C \sum_{i=1}^n \xi^{(i)} = \frac{1}{2} \sum_{z=1}^k \| \boldsymbol{\beta}_z \|^2 + C \sum_{i=1}^n \xi^{(i)}$ subject to:
    \begin{itemize}
        \item $-\xi^{(i)} - \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x}) + \max_{y'} [ \Delta(\boldsymbol{y},\boldsymbol{y}') + \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}', \boldsymbol{x}) ] \leq 0$ resp. $\Delta(\boldsymbol{y},\boldsymbol{y}') -\xi^{(i)} - \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x}) +  \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}', \boldsymbol{x})  \leq 0$ 
        \item Here, $\Delta$ is a loss function, quantifying the loss of predicting $\boldsymbol{y}'$ when the correct output is $\boldsymbol{y}$
        \item This means, we're performing \emph{margin rescaling} to account for varying levels of misclassification severity, based on how far off the prediction is from the correct output
        \item $-\xi^{(i)} \leq 0$
    \end{itemize}
    \item Cost function in Lagrangian formulation: $\mathcal{L} = \frac{1}{2} \| \boldsymbol{\beta} \|^2 + C \sum_{i=1}^n \xi^{(i)} + \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} (\Delta(\boldsymbol{y}^{(i)},\boldsymbol{y}^{(j)}) -\xi^{(i)} - \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}^{(i)}, \boldsymbol{x}^{(i)}) +  \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}^{(j)}, \boldsymbol{x}^{(i)})) - \sum_{i=1}^n \zeta^{(i)} \xi^{(i)}$
    \item We can abbreviate $\Psi_i(\boldsymbol{y}^{(j)}) = (-\Psi(\boldsymbol{y}^{(i)}, \boldsymbol{x}^{(i)}) + \Psi(\boldsymbol{y}^{(j)}, \boldsymbol{x}^{(i)}))$ and $\Delta_i(\boldsymbol{y}^{(j)}) = \Delta(\boldsymbol{y}^{(i)},\boldsymbol{y}^{(j)})$
    \item General solution:
    \begin{itemize}
        \item $\nabla_{\boldsymbol{\beta}} \mathcal{L} = \boldsymbol{\beta} - \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Psi_i(\boldsymbol{y}^{(j)}) = 0$
        \item $\Rightarrow \boldsymbol{\beta}^{*} = \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Psi_i(\boldsymbol{y}^{(j)})$
        \item $\nabla_b \mathcal{L} = - \sum_{i=1}^n \alpha^{(i)} y^{(i)} = 0$
        \item $\nabla_{\xi^{(i)}} \mathcal{L} = \sum_{i=1}^n C - \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} - \sum_{i=1}^n \zeta^{(i)} = 0$
    \end{itemize}
    \item By Slater's condition (linear separability), strong duality holds
    \item Objective function in dual Lagrangian, after plugging in found $\boldsymbol{\beta}$: $\mathcal{D} = -\frac{1}{2} \| \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Psi_i(\boldsymbol{y}^{(j)}) \|^2 + \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Delta_i(\boldsymbol{y}^{(j)})$\\
    Proof:
    \begin{itemize}
        \item $\mathcal{D} = \frac{1}{2} \| \boldsymbol{\beta}^* \|^2 + C \sum_{i=1}^n \xi^{(i)} + \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Delta(\boldsymbol{y}^{(i)},\boldsymbol{y}^{(j)}) - \alpha^{(ij)}\xi^{(i)} - \alpha^{(ij)} \boldsymbol{\beta} \cdot \Psi_i(\boldsymbol{y}^{(j)}) - \sum_{i=1}^n \zeta^{(i)} \xi^{(i)}$
        \item $= \frac{1}{2} \| \boldsymbol{\beta}^* \|^2 +  \sum_{i=1}^n \xi^{(i)} ( C - \sum_{y^{(j)}} \alpha^{(ij)} - \zeta^{(i)}) + \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Delta(\boldsymbol{y}^{(i)},\boldsymbol{y}^{(j)}) - \alpha^{(ij)} \boldsymbol{\beta} \cdot \Psi_i(\boldsymbol{y}^{(j)})$
        \item $ C - \sum_{y^{(j)}} \alpha^{(ij)} - \zeta^{(i)}) = 0$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$
        \item After plugging in found $\boldsymbol{\beta}$ and contracting the two first terms, we have $= -\frac{1}{2} \| \boldsymbol{\beta}^* \|^2 +  \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Delta(\boldsymbol{y}^{(i)},\boldsymbol{y}^{(j)}) = -\frac{1}{2} \| \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Psi_i(\boldsymbol{y}^{(j)}) \|^2 + \sum_{i=1}^n \sum_{y^{(j)}} \alpha^{(ij)} \Delta_i(\boldsymbol{y}^{(j)})$
    \end{itemize}
    \item Dual optimization: Maximize $\alpha$ subject to 
    \begin{itemize}
        \item $0 \leq \sum_{y^{(j)}} \alpha^{(ij)} \leq C$ due to $\nabla_{\xi^{(i)}} \mathcal{L}$ 
    \end{itemize}
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{SVMs in Practice} ---
For a structured SVM, we need to define $4$ functions: 
\begin{itemize}
    \item Feature function $\Psi(\boldsymbol{y}, \boldsymbol{x})$
    \item Loss function $\Delta(\boldsymbol{y}', \boldsymbol{y})$
    \item Prediction rule $\arg\max_{\boldsymbol{y}} \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}, \boldsymbol{x})$
    \item Loss-augmented inference $\arg\max_{\boldsymbol{y}'} \left( \Delta(\boldsymbol{y}', \boldsymbol{y}^{(i)}) + \boldsymbol{\beta} \cdot \Psi(\boldsymbol{y}', \boldsymbol{x}^{(i)}) \right)$
\end{itemize}
For a structured SVM, we need to overcome $4$ problems:
\begin{itemize}
    \item Number of parameters needs to be sub-linear with respect to the number of classes
    \item Enumerating all possible classes may be infeasible, hence, making a single prediction might require a problem-specific algorithm
    \item A problem-specific loss function must be defined that provides a rank-ordering of solutions with regard to their correctness
    \item Efficient training algorithms with a run-time complexity sub-linear in the number of classes are needed
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{SVM Regressor}
\emph{Description} --- 
\begin{itemize}
    \item Instead of classification, we do regression
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Formulation} --- 
\begin{itemize}
    \item Let $y^{(i)}$ define the output associated with $ \boldsymbol{x}^{(i)}$
    \item Let $\epsilon$ be the width of the region around the regression line in which points can lie at no extra cost
    \item Let $\hat{\xi}^{(i)}$ and $\xi^{(i)}$ be slack variables
    \item Then, we have:
    $(\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) - y^{(i)} \leq \epsilon + \hat{\xi}^{(i)}$ and $y^{(i)} - (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) \leq \epsilon + \xi^{(i)}$ for all $\boldsymbol{x}^{(i)}$
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Cost function: $\frac{1}{2} \| \boldsymbol{B} \|^2 + C \sum_{i=1}^n (\xi^{(i)} + \hat{\xi}^{(i)})$ subject to:
    \begin{itemize}
        \item $(\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) - y^{(i)} - \epsilon - \hat{\xi}^{(i)} \leq 0$ 
        \item $y^{(i)} - (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) - \epsilon - \xi^{(i)} \leq 0$
        \item $-\hat{\xi}^{(i)} \leq 0$
        \item $-\xi^{(i)} \leq 0$
    \end{itemize}
    \item Cost function in Lagrangian formulation: $\mathcal{L} = \frac{1}{2} \| \boldsymbol{\beta} \|^2 + C \sum_{i=1}^n (\xi^{(i)} + \hat{\xi}^{(i)}) - \sum_{i=1}^n \hat{\mu}^{(i)} \hat{\xi}^{(i)} - \sum_{i=1}^n \mu^{(i)} \xi^{(i)} + \sum_{i=1}^n \hat{\alpha}^{(i)}((\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) - y^{(i)} - \epsilon - \hat{\xi}^{(i)}) + \sum_{i=1}^n \alpha^{(i)} (y^{(i)} - (\boldsymbol{\beta} \cdot \boldsymbol{x}^{(i)} + b) - \epsilon - \xi^{(i)})$
    \item General solution:
    \begin{itemize}
        \item $\nabla_{\boldsymbol{\beta}} \mathcal{L} = \boldsymbol{\beta} + \sum_{i=1}^n \hat{\alpha}^{(i)} \boldsymbol{x}^{(i)} - \sum_{i=1}^n \alpha^{(i)} \boldsymbol{x}^{(i)} = \boldsymbol{\beta} + \sum_{i=1}^n (\hat{\alpha}^{(i)} - \alpha^{(i)}) \boldsymbol{x}^{(i)} = 0$
        \item $\Rightarrow \boldsymbol{\beta}^{*} = \sum_{i=1}^n \sum_{i=1}^n (\alpha^{(i)} - \hat{\alpha}^{(i)} ) \boldsymbol{x}^{(i)}$
        \item $\nabla_b \mathcal{L} = \sum_{i=1}^n \hat{\alpha}^{(i)} - \sum_{i=1}^n \alpha^{(i)} = \sum_{i=1}^n (\hat{\alpha}^{(i)} - \alpha^{(i)}) = 0$
        \item $\nabla_{\xi^{(i)}} \mathcal{L} = \sum_{i=1}^n C - \sum_{i=1}^n \mu^{(i)} - \sum_{i=1}^n \alpha^{(i)} = 0$, analog for $\hat{\xi}^{(i)}$
    \end{itemize}
    \item By Slater's condition (linear separability), strong duality holds
    \item Objective function in dual Lagrangian, after plugging in found $\boldsymbol{\beta}$: $\mathcal{D} = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\alpha^{(i)} - \hat{\alpha}^{(i)})(\alpha^{(j)} - \hat{\alpha}^{(j)}) \boldsymbol{x}^{(j)\intercal} \boldsymbol{x}^{(i)} + \sum_{i=1}^n (\alpha^{(i)} - \hat{\alpha}^{(i)}) y^{(i)} - \epsilon \sum_{i=1}^n (\alpha^{(i)} + \hat{\alpha}^{(i)})$\\
    Proof:
    \begin{itemize}
        \item 1) $
        \mathcal{D} = C \sum_{i=1}^N (\xi^{(i)} + \hat{\xi}^{(i)})$
        2) $
        + \frac{1}{2} \sum_{i=1}^N \Big((\alpha^{(i)} - \hat{\alpha}^{(i)}) \boldsymbol{x}^{(i)}\Big)^T \Big(\sum_{i=1}^N (\alpha^{(i)} - \hat{\alpha}^{(i)}) \boldsymbol{x}^{(i)}\Big)
        $
        3) $
        - \sum_{i=1}^N \mu^{(i)} \xi^{(i)} - \hat{\mu}^{(i)} \hat{\xi}^{(i)}
        $ 
        4) $ 
        + \sum_{i=1}^N (\alpha^{(i)} - \hat{\alpha}^{(i)}) y^{(i)}
        $ 
        5) $
        + \sum_{i=1}^N (\hat{\alpha}^{(i)} - \alpha^{(i)}) b
        $
        6) $
        + \sum_{i=1}^N (-\hat{\alpha}^{(i)} - \alpha^{(i)}) \epsilon
        $
        7) $
        - \sum_{i=1}^N \hat{\alpha}^{(i)}\hat{\xi}^{(i)} - \alpha^{(i)}\xi^{(i)}
        $
        8) $
        - \sum_{i=1}^N \alpha^{(i)} \sum_{j=1}^N ((\alpha^{(j)} - \hat{\alpha}^{(j)}) \boldsymbol{x}^{(j)})^T \boldsymbol{x}^{(i)}
        $
        $
        + \sum_{i=1}^N \hat{\alpha}^{(i)} \sum_{j=1}^N ((\alpha^{(j)} - \hat{\alpha}^{(j)}) \boldsymbol{x}^{(j)})^T \boldsymbol{x}^{(i)}
        $
        \item We can simplify:
        1, 3, 7) $
        \mathcal{D} = \sum_{i=1}^N \Big[C - \mu^{(i)} - \alpha^{(i)}\Big] \xi^{(i)} + \sum_{i=1}^N \Big[C - \hat{\mu}^{(i)} - \hat{\alpha}^{(i)}\Big] \hat{\xi}^{(i)}
        $
        8) $
        - \sum_{i=1}^N \sum_{j=1}^N (\alpha^{(i)} - \hat{\alpha}^{(i)})(\alpha^{(j)} - \hat{\alpha}^{(j)}) \boldsymbol{x}^{(j)\intercal} \boldsymbol{x}^{(i)}
        $
        2) $
        + \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N (\alpha^{(i)} - \hat{\alpha}^{(i)})(\alpha^{(j)} - \hat{\alpha}^{(j)}) \boldsymbol{x}^{(j)\intercal} \boldsymbol{x}^{(i)}
        $
        4) $
        + \sum_{i=1}^N (\alpha^{(i)} - \hat{\alpha}^{(i)}) y^{(i)}
        $
        5) $
        + \sum_{i=1}^N (\hat{\alpha}^{(i)} - \alpha^{(i)} ) b
        $
        6) $
        + \sum_{i=1}^N (- \hat{\alpha}^{(i)} - \alpha^{(i)}) \epsilon 
        $
        \item Given the derivatives and that certain terms ($(\sum_{i=1}^n \alpha^{(i)} - \hat{\alpha}^{(i)}), (C-\mu^{(i)}-\alpha^{(i)}), (C-\hat{\mu}^{(i)}-\hat{\alpha}^{(i)})$ in 1,3,5,7,8) simplify to $0$, we end up with the Dual formulation
    \end{itemize}
\end{itemize}