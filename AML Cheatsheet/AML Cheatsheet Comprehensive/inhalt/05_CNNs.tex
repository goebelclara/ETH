\section{Convolutional Neural Networks (CNNs)}
\subsection*{Formulation}
\emph{Formulation} --- 
\begin{itemize}
    \item Model architecture: 
    \begin{itemize}
        \item Input: Composed of channels (e.g. R with 3 channels)
        \item Convolutional layer: Composed of feature maps
        \item Channel: Sublayer in input and output, composed of pixels
        \item Feature map: Sublayer in convolutional layer, composed of neurons, each neuron is generated by applying filter to all receptive fields across all sublayers in lower layer, weights and biases shared across all neurons in feature map
        \item Receptive field: Group of neurons in lower layer, that single neuron in higher layer is connected to, size $f_h \times f_w$
        \item Filter resp. convolutional kernel: Weights applied to all receptive fields across all sublayers in lower layer, size $K \times K$
        \item Zero padding: Padding applied to retain same dimensions in each layer, size $\frac{f_h - 1}{2}$ resp. $\frac{f_w - 1}{2}$
        \item Stride: By how many neurons receptive field shifts, size $s_h \times s_w$, if stride $> 1$, spatial dimensions in subsequent layer decrease (convolution), if stride $< 1$, spatial dimensions increase (deconvolution)
    \end{itemize}
    \item Output of neuron in layer $n$, given previous layer $n-1$: \\
    $z_{i,j,k} = b_k + \sum_{f_n} \sum_{f_w} \sum_{f_n'} x_{i',j',k'} \cdot w_{u,v,k',k}$, i.e. sum of element-wise matrix product over all receptive fields and all feature maps, where
    \begin{itemize}
        \item $z_{i,j,k}$ is the output of neuron in row $i$ and column $j$ on feature map $k$ in layer $n$
        \item $f_n$ and $f_w$ are dimensions of the receptive field in layer $n-1$
        \item $f_n'$ is the number of feature maps in layer $n-1$
        \item $x_{i',j',k'}$ is the output of neuron in row $i'$ and column $j'$ on feature map $k'$ in layer $n-1$
        \item $i' = i \times \textrm{stride}_h + u - \textrm{padding}_h$
        and $j' = j \times \textrm{stride}_w + v - \textrm{padding}_w$
        \item $w_{u,v,k',k}$ is the connection weight between any neuron on feature map $k$ in layer $n$ and its input at $u,v$ on feature map $k'$
        \item $u,v \in \Delta_K$ are possible shifts allowed by kernel
    \end{itemize}
    \item Output of neurons in layer $n$, given previous layer $n-1$: \\
    $\boldsymbol{z}_k = b_k + \sum_{f_n} \sum_{f_w} \sum_{f_n'} \boldsymbol{W}_{k',k} \boldsymbol{X}_{k'}$
    \item Output size in layer $n$, given previous layer $n-1$: $H' = \frac{H + 2p - K}{\textrm{stride}_h} + 1$ and
    $W' = \frac{W + 2p - K}{\textrm{stride}_w} + 1$
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\theta = \boldsymbol{W}$

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Minimize standard objectives, e.g. MSE
\end{itemize}

{\color{lightgray}\hrule height 0.001mm}

\emph{Optimization} ---
\begin{itemize}
    \item Perform \emph{forward pass} with randomly initialized parameters, to calculate loss
    \item Perform \emph{backpropagation}, to calculate gradient
    \item Perform gradient descent to find best weights 
\end{itemize}