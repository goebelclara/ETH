\section{Variational Autoencoders (VAE)}
\subsection*{Description}
\emph{Task} --- Autoencoders as representation learning (learn to copy inputs to outputs)

{\color{lightgray}\hrule height 0.001mm}

\emph{Description} --- Variational autoencoders as probabilistic autoencoders

{\color{black}\hrule height 0.001mm}

\subsection*{Formulation}
\emph{Formulation} --- 
\begin{itemize}
    \item Model architecture: 
    \begin{itemize}
        \item Encoder / recognition network: Converts input to latent representation
        \item Decoder / generative network: Converts latent representation to output
    \end{itemize}
    \item Assumes data $\boldsymbol{x}$ is generated by latent variable $\boldsymbol{h}$: $p(\boldsymbol{x}, \boldsymbol{h}) = p_\theta(\boldsymbol{x} | \boldsymbol{h}) \times p_{\theta'}(\boldsymbol{h})$
    \item Prior $p_{\theta'}(\boldsymbol{h}) \sim \mathcal{N}(0,\sigma^2 \boldsymbol{I})$
    \item Encoder produces a probability distribution $q_\Phi(\boldsymbol{h}|\boldsymbol{x})$ which approximates the true posterior $p_{\theta, \theta'}(\boldsymbol{h}|\boldsymbol{x})$ since the true posterior is intractable:
    \begin{itemize}
        \item Produces $\boldsymbol{\mu}_{\boldsymbol{h|x}}$ and $\boldsymbol{\Sigma}_{\boldsymbol{h|x}}$ according to which $\boldsymbol{h}$ is approximately distributed in latent space
    \end{itemize}
    \item Decoder reconstructs input $p_\theta(\boldsymbol{x} | \boldsymbol{h})$ by sampling from latent space $\boldsymbol{h} \sim q_\Phi(\boldsymbol{h}|\boldsymbol{x})$:
    \begin{itemize}
        \item Produces $\boldsymbol{\mu}_{\boldsymbol{x|h}}$ and $\boldsymbol{\Sigma}_{\boldsymbol{x|h}}$ according to which $\boldsymbol{\hat{x}}$ is approximately distributed in output space
    \end{itemize}
\end{itemize}

{\color{black}\hrule height 0.001mm}

\subsection*{Optimization}
\emph{Parameters} --- Find parameters $\theta'$ (prior), $\theta$ (likelihood), $\Phi$ (approximate posterior) 

{\color{lightgray}\hrule height 0.001mm}

\emph{Objective function} --- 
\begin{itemize}
    \item Maximize log likelihood $\sum_{i=1}^n \log p_{\theta,\theta'}(\boldsymbol{x}^{(i)})$
    \item Challenge: This does not involve the encoder
    \item Solution: $\arg\max_{\theta,\theta',\Phi} \sum_{i=1}^n \mathbb{E}_{ q(\boldsymbol{h|x})} \log [ \frac{p_{\theta,\theta'}(\boldsymbol{x}^{(i)}, \boldsymbol{h})}{p_{\theta,\theta'}(\boldsymbol{h} | \boldsymbol{x}^{(i)})} \times \frac{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})} ] = 
    \sum_{i=1}^n \mathbb{E} \log [ \frac{p_{\theta,\theta'}(\boldsymbol{x}^{(i)}, \boldsymbol{h})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}] + \mathbb{E} \log [\frac{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}{p_{\theta,\theta'}(\boldsymbol{h} | \boldsymbol{x}^{(i)})}]
    = \sum_{i=1}^n \textrm{ELBO}_{\theta,\theta',\Phi}(\boldsymbol{x}^{(i)}) + \textrm{KL divergence}(q_\Phi(\cdot | \boldsymbol{x}^{(i)}) | p_{\theta,\theta'}(\cdot |\boldsymbol{x}^{(i)}))
    $
    \item ELBO provides lower bound of log likelihood: $\log p_{\theta,\theta'}(\boldsymbol{x}^{(i)}) \geq \textrm{ELBO}_{\theta,\theta',\Phi}(\boldsymbol{x}^{(i)}) 
    =
    \mathbb{E} \log [ \frac{p_{\theta,\theta'}(\boldsymbol{x}^{(i)}, \boldsymbol{h})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}]
    = 
    \mathbb{E} \log [ \frac{p_{\theta'}(\boldsymbol{h}) \times p_{\theta}(\boldsymbol{x}^{(i)} | \boldsymbol{h})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}]
    = 
    \mathbb{E} \log [ p_{\theta}(\boldsymbol{x}^{(i)} | \boldsymbol{h}) ] + \mathbb{E} \log [ \frac{p_{\theta'}(\boldsymbol{h})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}]
    =
    \textrm{mutual information resp. cross-entropy from decoder } - 
    \textrm{ KL divergence } (q_\Phi(\cdot | \boldsymbol{x}^{(i)}) | p_{\theta'}(\cdot)) \textrm{ from encoder} 
    $
    \item ELBO can also be formulated as $\textrm{ELBO}_{\theta,\theta',\Phi}(\boldsymbol{x}^{(i)}) 
    = \mathbb{E} \log [ \frac{p_{\theta,\theta'}(\boldsymbol{x}^{(i)}, \boldsymbol{h})}{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}] 
    =
    \mathbb{E} \log [ p_{\theta,\theta'}(\boldsymbol{x}^{(i)}, \boldsymbol{h})] - \mathbb{E} \log [ q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})] 
    = 
    \mathbb{E} \log [ p_{\theta,\theta'}(\boldsymbol{h} |\boldsymbol{x}^{(i)}) p_{\theta,\theta'}(\boldsymbol{x}^{(i)})] - \mathbb{E} \log [ q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})]
    = 
    \mathbb{E} \log [ p_{\theta,\theta'}(\boldsymbol{h} |\boldsymbol{x}^{(i)})] + \mathbb{E} \log [p_{\theta,\theta'}(\boldsymbol{x}^{(i)})] - \mathbb{E} \log [ q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})]
    = 
    - \mathbb{E} \log [ \frac{q_\Phi(\boldsymbol{h}|\boldsymbol{x}^{(i)})}{p_{\theta,\theta'}(\boldsymbol{h} |\boldsymbol{x}^{(i)})} ] + \mathbb{E} \log [p_{\theta,\theta'}(\boldsymbol{x}^{(i)})]
    $
    \item Mutual information enforces infomax principle of ANNs
    \item KL divergence acts as regularization
\end{itemize}